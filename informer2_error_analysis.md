# Informer-2 æ¨¡å‹é”™è¯¯æ·±åº¦åˆ†æ

## ğŸš¨ æ ¸å¿ƒé”™è¯¯

```
ERROR - Informer-2è®­ç»ƒå¤±è´¥: max(): Expected reduction dim 3 to have non-zero size.
```

**é”™è¯¯ä½ç½®**: `ProbSparseSelfAttention._prob_QK()` æ–¹æ³•ä¸­çš„ `M = Q_K.max(dim=-1)[0]`

---

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### é—®é¢˜1: **è¾“å…¥æ•°æ®ç»´åº¦ä¸åŒ¹é…**

#### å½“å‰å®ç°çš„é—®é¢˜ï¼š

**Informer-2æ¨¡å‹æœŸæœ›çš„è¾“å…¥**:
```python
# informer2_model.py line 447
def forward(self, x: torch.Tensor) -> torch.Tensor:
    """
    Args:
        x: (batch, n_features) - å•ä¸ªæ ·æœ¬çš„ç‰¹å¾å‘é‡  âŒ é”™è¯¯çš„æ³¨é‡Š
    """
```

**å®é™…çš„Informeræ¶æ„éœ€æ±‚**:
- Informeræ˜¯ä¸º**æ—¶é—´åºåˆ—**è®¾è®¡çš„ï¼Œéœ€è¦ `(batch, seq_len, features)` ä¸‰ç»´è¾“å…¥
- ä½†å½“å‰å®ç°æ¥æ”¶çš„æ˜¯ `(batch, features)` äºŒç»´è¾“å…¥ï¼ˆå•ä¸ªæ—¶é—´ç‚¹çš„ç‰¹å¾ï¼‰

#### ä»£ç ä¸­çš„"ä¸´æ—¶ä¿®å¤"ï¼š

```python
# line 451-452
x = self.input_projection(x)  # (batch, d_model)
x = x.unsqueeze(1)  # (batch, 1, d_model) âš ï¸ å¼ºè¡Œæ·»åŠ seq_len=1
```

**è¿™å¯¼è‡´äº†è‡´å‘½é—®é¢˜**ï¼š
- `seq_len = 1` æ„å‘³ç€åªæœ‰1ä¸ªæ—¶é—´æ­¥
- åœ¨ProbSparseæ³¨æ„åŠ›ä¸­ï¼Œéœ€è¦è®¡ç®— `sample_k = factor * ceil(log(L_K))`
- å½“ `L_K = 1` æ—¶ï¼Œ`log(1) = 0`ï¼Œå¯¼è‡´ `sample_k = 0`
- ç„¶å `K_sample = K[:, :, torch.randperm(L_K)[:sample_k], :]` å˜æˆç©ºå¼ é‡
- æœ€ç»ˆ `Q_K.max(dim=-1)` åœ¨ç©ºç»´åº¦ä¸Šæ“ä½œï¼Œè§¦å‘é”™è¯¯

---

### é—®é¢˜2: **ProbSparseæ³¨æ„åŠ›çš„é‡‡æ ·é€»è¾‘ç¼ºé™·**

```python
# informer2_model.py line 73-76
def _prob_QK(self, Q, K, sample_k, n_top):
    # ...
    K_sample = K[:, :, torch.randperm(L_K)[:sample_k], :]  # âŒ å½“sample_k=0æ—¶ä¸ºç©º
    Q_K = torch.matmul(Q, K_sample.transpose(-2, -1))  # (B, H, L_Q, 0)
    M = Q_K.max(dim=-1)[0]  # ğŸ’¥ åœ¨ç»´åº¦3(size=0)ä¸Šæ±‚maxï¼ŒæŠ¥é”™ï¼
```

**æ•°å­¦æ¨å¯¼**:
- `L_K = 1` (åºåˆ—é•¿åº¦)
- `sample_k = factor * ceil(log(1)) = 5 * 0 = 0`
- `K_sample.shape = (B, H, 0, d)` â† ç¬¬3ç»´ä¸º0
- `Q_K.shape = (B, H, L_Q, 0)` â† ç¬¬4ç»´ä¸º0
- `max(dim=-1)` åœ¨ç©ºç»´åº¦ä¸Šæ“ä½œ â†’ **ERROR**

---

## ğŸ“Š é…ç½®åˆç†æ€§åˆ†æ

### å½“å‰é…ç½®ï¼š

```python
# ensemble_ml_service.py
self.informer_d_model = 128      # æ¨¡å‹ç»´åº¦
self.informer_n_heads = 8        # æ³¨æ„åŠ›å¤´æ•°
self.informer_n_layers = 3       # Encoderå±‚æ•°
self.informer_epochs = 50        # è®­ç»ƒè½®æ•°
self.informer_batch_size = 256   # æ‰¹æ¬¡å¤§å°
```

### è¶…å‚æ•°æœç´¢ç©ºé—´ï¼ˆ15mï¼‰ï¼š

```python
'd_model': [64, 128, 256]
'n_heads': [4, 8, 16]
'n_layers': [2, 3, 4]
'epochs': [20, 40]
'batch_size': [128, 256, 512]
```

---

## âš–ï¸ é…ç½®åˆç†æ€§è¯„ä¼°

### âœ… åˆç†çš„éƒ¨åˆ†ï¼š

1. **d_model = 128**: é€‚ä¸­ï¼Œå¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æˆæœ¬
2. **n_heads = 8**: æ ‡å‡†é…ç½®ï¼ˆTransformerè®ºæ–‡æ¨èï¼‰
3. **batch_size = 256**: å¯¹äº27,025ä¸ªæ ·æœ¬åˆç†
4. **epochs = 50**: GPUåŠ é€Ÿä¸‹å¯æ¥å—

### âŒ ä¸åˆç†çš„éƒ¨åˆ†ï¼š

#### 1. **æ¶æ„è®¾è®¡æ ¹æœ¬æ€§é”™è¯¯**

**é—®é¢˜**: Informeræ˜¯ä¸º**é•¿åºåˆ—æ—¶é—´åºåˆ—é¢„æµ‹**è®¾è®¡çš„ï¼Œä½†ä½ çš„ä»»åŠ¡æ˜¯**å•æ—¶é—´ç‚¹åˆ†ç±»**

| ç»´åº¦ | InformeråŸå§‹è®¾è®¡ | ä½ çš„å®é™…éœ€æ±‚ |
|------|-----------------|-------------|
| è¾“å…¥ | (batch, seq_len=96, features) | (batch, features) |
| ä»»åŠ¡ | é¢„æµ‹æœªæ¥24æ­¥ | åˆ†ç±»å½“å‰æ—¶åˆ»(LONG/HOLD/SHORT) |
| ä¼˜åŠ¿ | é•¿åºåˆ—å»ºæ¨¡ | ç‰¹å¾æå– |

**ç»“è®º**: **Informerä¸é€‚åˆä½ çš„ä»»åŠ¡ï¼**

#### 2. **seq_len = 1 çš„è‡´å‘½ç¼ºé™·**

```python
x = x.unsqueeze(1)  # (batch, 1, d_model)
```

- ProbSparseæ³¨æ„åŠ›éœ€è¦ `seq_len >= 10` æ‰æœ‰æ„ä¹‰
- `log(1) = 0` å¯¼è‡´é‡‡æ ·å¤±è´¥
- å¤±å»äº†Informerçš„æ ¸å¿ƒä¼˜åŠ¿ï¼ˆé•¿åºåˆ—å»ºæ¨¡ï¼‰

#### 3. **n_layers = 3 è¿‡æ·±**

- å¯¹äº `seq_len = 1`ï¼Œå¤šå±‚Encoderæ¯«æ— æ„ä¹‰
- æ¯å±‚è’¸é¦ä¼šå‡åŠåºåˆ—é•¿åº¦ï¼š`1 â†’ 0.5 â†’ 0.25` (ä¸å¯è¡Œ)
- å¢åŠ è®¡ç®—æˆæœ¬ä½†æ— æ€§èƒ½æå‡

#### 4. **use_distilling = True ä¸é€‚ç”¨**

```python
# informer2_model.py line 177
class DistillingLayer:
    def forward(self, x):
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦åˆ†åˆ«è¿›è¡Œè’¸é¦
        # ä½¿ç”¨MaxPool1d(kernel_size=3, stride=2)
        # è¾“å…¥: (B, L, D) â†’ è¾“å‡º: (B, L//4, D)
```

- è’¸é¦å±‚è®¾è®¡ç”¨äºå‡å°‘åºåˆ—é•¿åº¦
- å½“ `L = 1` æ—¶ï¼Œ`L//4 = 0` â†’ åºåˆ—æ¶ˆå¤±ï¼

---

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: **ä¿®å¤Informer-2ï¼ˆä¸æ¨èï¼‰**

#### æ­¥éª¤ï¼š
1. ç§»é™¤ProbSparseæ³¨æ„åŠ›ï¼Œä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›
2. ç§»é™¤è’¸é¦å±‚
3. è®¾ç½® `n_layers = 1`
4. ç®€åŒ–ä¸ºæ™®é€šTransformer Encoder

#### ä»£ç ä¿®æ”¹ï¼š
```python
class Informer2ForClassification(nn.Module):
    def __init__(self, ...):
        # ä½¿ç”¨æ ‡å‡†MultiHeadAttentionæ›¿ä»£ProbSparse
        self.attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=n_heads,
            dropout=dropout,
            batch_first=True
        )
        # ç§»é™¤è’¸é¦å±‚
        # å•å±‚Encoder
```

**ç¼ºç‚¹**: å¤±å»Informerçš„æ‰€æœ‰ä¼˜åŠ¿ï¼Œå˜æˆæ™®é€šTransformer

---

### æ–¹æ¡ˆ2: **ä½¿ç”¨æ›´é€‚åˆçš„æ¨¡å‹ï¼ˆå¼ºçƒˆæ¨èï¼‰**

#### æ¨èæ¨¡å‹ï¼š

##### A. **TabNet** (Google 2019)
- ä¸“ä¸ºè¡¨æ ¼æ•°æ®è®¾è®¡
- å¯è§£é‡Šæ€§å¼ºï¼ˆç‰¹å¾é‡è¦æ€§ï¼‰
- æ€§èƒ½ä¼˜äºä¼ ç»ŸGBDT

```python
from pytorch_tabnet.tab_model import TabNetClassifier

model = TabNetClassifier(
    n_d=64,  # å†³ç­–å±‚ç»´åº¦
    n_a=64,  # æ³¨æ„åŠ›å±‚ç»´åº¦
    n_steps=5,  # å†³ç­–æ­¥æ•°
    gamma=1.5,  # ç¨€ç–æ€§ç³»æ•°
    n_independent=2,
    n_shared=2
)
```

##### B. **FT-Transformer** (2021)
- Feature Tokenizer + Transformer
- ä¸“ä¸ºè¡¨æ ¼æ•°æ®ä¼˜åŒ–
- SOTAæ€§èƒ½

```python
class FTTransformer(nn.Module):
    def __init__(self, n_features, d_model=128, n_heads=8, n_layers=3):
        # æ¯ä¸ªç‰¹å¾ç‹¬ç«‹åµŒå…¥
        self.feature_embeddings = nn.ModuleList([
            nn.Linear(1, d_model) for _ in range(n_features)
        ])
        # æ ‡å‡†Transformer Encoder
        self.transformer = nn.TransformerEncoder(...)
```

##### C. **ç®€åŒ–ç‰ˆTransformer**
- ç§»é™¤æ—¶é—´åºåˆ—ç›¸å…³ç»„ä»¶
- ä¿ç•™è‡ªæ³¨æ„åŠ›æœºåˆ¶
- è½»é‡çº§

```python
class SimpleTransformerClassifier(nn.Module):
    def __init__(self, n_features, d_model=128, n_heads=8):
        self.embedding = nn.Linear(n_features, d_model)
        self.attention = nn.MultiheadAttention(d_model, n_heads)
        self.classifier = nn.Linear(d_model, 3)
```

---

### æ–¹æ¡ˆ3: **é‡æ–°è®¾è®¡è¾“å…¥ï¼ˆå¦‚æœåšæŒç”¨Informerï¼‰**

#### æ„é€ æ—¶é—´åºåˆ—è¾“å…¥ï¼š

```python
def create_sequence_input(df, seq_len=96):
    """
    å°†å•æ—¶é—´ç‚¹ç‰¹å¾è½¬æ¢ä¸ºæ—¶é—´åºåˆ—
    
    Args:
        df: åŸå§‹æ•°æ® (n_samples, n_features)
        seq_len: åºåˆ—é•¿åº¦ï¼ˆå¦‚96ä¸ª15åˆ†é’Ÿ = 24å°æ—¶ï¼‰
    
    Returns:
        X_seq: (n_samples, seq_len, n_features)
        y: (n_samples,)
    """
    X_seq = []
    y_seq = []
    
    for i in range(seq_len, len(df)):
        # å–è¿‡å»seq_lenä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾
        X_seq.append(df.iloc[i-seq_len:i].values)
        y_seq.append(df.iloc[i]['label'])
    
    return np.array(X_seq), np.array(y_seq)
```

**ä¼˜ç‚¹**: å……åˆ†åˆ©ç”¨Informerçš„é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›
**ç¼ºç‚¹**: 
- éœ€è¦é‡æ–°è®¾è®¡æ•°æ®ç®¡é“
- è®­ç»ƒæ ·æœ¬å‡å°‘ï¼ˆå‰96ä¸ªæ ·æœ¬æ— æ³•ä½¿ç”¨ï¼‰
- æ¨ç†æ—¶éœ€è¦96ä¸ªå†å²æ—¶é—´æ­¥

---

## ğŸ“‹ æ¨èé…ç½®

### å¦‚æœä½¿ç”¨æ–¹æ¡ˆ2A (TabNet):

```python
# ensemble_ml_service.py
self.enable_tabnet = True
self.tabnet_n_d = 64
self.tabnet_n_a = 64
self.tabnet_n_steps = 5
self.tabnet_gamma = 1.5
self.tabnet_epochs = 50
self.tabnet_batch_size = 256
self.tabnet_lr = 0.02
```

### å¦‚æœä½¿ç”¨æ–¹æ¡ˆ2C (ç®€åŒ–Transformer):

```python
self.enable_simple_transformer = True
self.st_d_model = 128
self.st_n_heads = 8
self.st_n_layers = 2  # å‡å°‘å±‚æ•°
self.st_epochs = 50
self.st_batch_size = 256
self.st_lr = 0.001
```

### å¦‚æœä½¿ç”¨æ–¹æ¡ˆ3 (é‡æ–°è®¾è®¡Informer):

```python
self.enable_informer2 = True
self.informer_seq_len = 96  # æ–°å¢ï¼šåºåˆ—é•¿åº¦
self.informer_d_model = 128
self.informer_n_heads = 8
self.informer_n_layers = 2  # å‡å°‘åˆ°2å±‚
self.informer_factor = 5  # ProbSparseé‡‡æ ·å› å­
self.informer_epochs = 30  # å‡å°‘è½®æ•°ï¼ˆåºåˆ—è¾“å…¥è®­ç»ƒæ›´æ…¢ï¼‰
self.informer_batch_size = 64  # å‡å°æ‰¹æ¬¡ï¼ˆåºåˆ—è¾“å…¥å ç”¨æ›´å¤šå†…å­˜ï¼‰
self.informer_lr = 0.0005
```

---

## ğŸ¯ æœ€ç»ˆå»ºè®®

### çŸ­æœŸï¼ˆç«‹å³ä¿®å¤ï¼‰ï¼š
1. **ç¦ç”¨Informer-2**: `self.enable_informer2 = False`
2. ç»§ç»­ä½¿ç”¨3ä¸ªGBDTæ¨¡å‹çš„Stackingé›†æˆ
3. ç³»ç»Ÿå·²ç»æœ‰47%çš„å‡†ç¡®ç‡ï¼Œå¯ä»¥æ­£å¸¸è¿è¡Œ

### ä¸­æœŸï¼ˆ1-2å‘¨ï¼‰ï¼š
1. **å®ç°TabNet**: æœ€é€‚åˆè¡¨æ ¼æ•°æ®çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
2. æ›¿æ¢Informer-2ä¸ºTabNet
3. é‡æ–°è®­ç»ƒ4æ¨¡å‹é›†æˆï¼ˆLGB + XGB + CAT + TabNetï¼‰

### é•¿æœŸï¼ˆ1ä¸ªæœˆ+ï¼‰ï¼š
1. **é‡æ–°è®¾è®¡æ•°æ®ç®¡é“**: æ„é€ æ—¶é—´åºåˆ—è¾“å…¥
2. å®ç°çœŸæ­£çš„Informer-2ï¼ˆç”¨äºé•¿åºåˆ—é¢„æµ‹ï¼‰
3. æ¢ç´¢å¤šä»»åŠ¡å­¦ä¹ ï¼ˆåˆ†ç±» + ä»·æ ¼é¢„æµ‹ï¼‰

---

## ğŸ“Š æ€§èƒ½é¢„æœŸ

| æ¨¡å‹ç»„åˆ | é¢„æœŸå‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´ | æ¨ç†é€Ÿåº¦ |
|---------|-----------|---------|---------|
| å½“å‰(3 GBDT) | 47% | 1.5h | å¿« |
| + TabNet | 49-51% | 2h | ä¸­ |
| + åºåˆ—Informer | 52-55% | 3h | æ…¢ |

---

## ğŸ”§ ç«‹å³å¯æ‰§è¡Œçš„ä¿®å¤

```python
# backend/app/services/ensemble_ml_service.py
# Line 70: ä¸´æ—¶ç¦ç”¨Informer-2
self.enable_informer2 = False  # âŒ æš‚æ—¶ç¦ç”¨ï¼Œç­‰å¾…ä¿®å¤

# æˆ–è€…æ·»åŠ åºåˆ—é•¿åº¦æ£€æŸ¥
def _train_informer2(self, ...):
    # åœ¨è®­ç»ƒå‰æ£€æŸ¥
    if X_train.shape[0] < 96:
        logger.warning("âš ï¸ æ ·æœ¬æ•°ä¸è¶³ï¼Œè·³è¿‡Informer-2è®­ç»ƒ")
        return None
    
    # æ„é€ åºåˆ—è¾“å…¥
    X_seq, y_seq = self._create_sequence_input(X_train, y_train, seq_len=96)
    # ... ç»§ç»­è®­ç»ƒ
```

---

## æ€»ç»“

**æ ¸å¿ƒé—®é¢˜**: Informer-2æ˜¯ä¸ºé•¿åºåˆ—æ—¶é—´åºåˆ—é¢„æµ‹è®¾è®¡çš„ï¼Œä½†ä½ çš„ä»»åŠ¡æ˜¯å•æ—¶é—´ç‚¹ç‰¹å¾åˆ†ç±»ï¼Œæ¶æ„ä¸åŒ¹é…ã€‚

**æœ€ä½³æ–¹æ¡ˆ**: ä½¿ç”¨TabNetæˆ–FT-Transformeræ›¿ä»£Informer-2ã€‚

**ä¸´æ—¶æ–¹æ¡ˆ**: ç¦ç”¨Informer-2ï¼Œä½¿ç”¨3æ¨¡å‹é›†æˆï¼ˆå·²ç»æœ‰47%å‡†ç¡®ç‡ï¼‰ã€‚
