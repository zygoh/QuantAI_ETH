"""
é›†æˆæœºå™¨å­¦ä¹ æœåŠ¡ - Stackingä¸‰æ¨¡å‹èåˆ
"""
# Standard library imports
import logging
import gc
import time
import traceback
import os
import tempfile
import shutil
from typing import Dict, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
import pickle

# Third-party imports
import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
from scipy.special import entr
from scipy.stats import entropy as scipy_entropy
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, log_loss
from sklearn.preprocessing import StandardScaler
from numpy.lib.format import open_memmap

# Local application imports
from app.services.ml_service import MLService
from app.core.config import settings
from app.core.cache import cache_manager
from app.services.hyperparameter_optimizer import HyperparameterOptimizer
from app.services.direction_consistency_checker import TradingDirectionConsistencyChecker, ConsistencyCheck
from app.services.adaptive_frequency_controller import AdaptiveFrequencyController, FrequencyControl
from app.services.model_stability_enhancer import ModelStabilityEnhancer
from app.utils.helpers import format_signal_type

logger = logging.getLogger(__name__)

# ğŸ”¥ å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–ï¼ˆä»…åœ¨å¿…è¦æ—¶ï¼‰
# binance_client åœ¨æ–¹æ³•å†…å¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–

# æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆPyTorchï¼‰
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, Dataset, TensorDataset
    from torch.optim.lr_scheduler import ReduceLROnPlateau
    from app.services.informer2_model import Informer2ForClassification
    from app.services.gmadl_loss import create_trade_loss
    TORCH_AVAILABLE = True
    logger.info("âœ… PyTorchå·²åŠ è½½ï¼ŒInformer-2æ¨¡å‹å¯ç”¨")
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("âš ï¸ PyTorchæœªå®‰è£…ï¼ŒInformer-2æ¨¡å‹å°†ä¸å¯ç”¨")


class InformerWrapper:
    """
    åŒ…è£…Informer-2æ¨¡å‹ï¼Œæä¾›predict_probaæ¥å£ï¼ˆæ”¯æŒåºåˆ—è¾“å…¥ï¼‰
    
    å°†ç±»ç§»åˆ°æ¨¡å—çº§åˆ«ä»¥æ”¯æŒpickleåºåˆ—åŒ–
    """
    
    def __init__(self, model, device):
        """
        åˆå§‹åŒ–åŒ…è£…å™¨
        
        Args:
            model: Informer2ForClassificationæ¨¡å‹å®ä¾‹
            device: PyTorchè®¾å¤‡ï¼ˆ'cuda'æˆ–'cpu'ï¼‰
        """
        self.model = model
        self.device = device
    
    def predict_proba(self, X_seq):
        """
        é¢„æµ‹æ¦‚ç‡ï¼ˆå…¼å®¹scikit-learnï¼Œæ”¯æŒåºåˆ—è¾“å…¥ï¼‰
        
        Args:
            X_seq: NumPyæ•°ç»„ (n_samples, seq_len, n_features)
        
        Returns:
            æ¦‚ç‡æ•°ç»„ (n_samples, n_classes)
        """
        self.model.eval()
        with torch.no_grad():
            # ğŸ”¥ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨from_numpyé¿å…æ•°æ®å¤åˆ¶ï¼Œç¡®ä¿float32
            if not isinstance(X_seq, torch.Tensor):
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®æ˜¯è¿ç»­çš„numpyæ•°ç»„
                if not isinstance(X_seq, np.ndarray):
                    X_seq = np.asarray(X_seq, dtype=np.float32)
                elif X_seq.dtype != np.float32:
                    X_seq = X_seq.astype(np.float32)
                
                if not X_seq.flags['C_CONTIGUOUS']:
                    X_seq = np.ascontiguousarray(X_seq)
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨copy()é¿å…å†…å­˜æ˜ å°„é—®é¢˜
                X_tensor = torch.from_numpy(X_seq.copy()).to(self.device)
            else:
                X_tensor = X_seq.to(self.device)
            
            probs = self.model.predict_proba(X_tensor)
            return probs.cpu().numpy()
    
    def predict(self, X_seq):
        """
        é¢„æµ‹ç±»åˆ«ï¼ˆå…¼å®¹scikit-learnï¼Œæ”¯æŒåºåˆ—è¾“å…¥ï¼‰
        
        Args:
            X_seq: NumPyæ•°ç»„ (n_samples, seq_len, n_features)
        
        Returns:
            é¢„æµ‹ç±»åˆ«æ•°ç»„
        """
        probs = self.predict_proba(X_seq)
        return np.argmax(probs, axis=1)


class EnsembleMLService(MLService):
    """
    é›†æˆæœºå™¨å­¦ä¹ æœåŠ¡ï¼ˆStackingï¼‰
    
    ä½¿ç”¨LightGBM + XGBoost + CatBoost + Informer-2 å››æ¨¡å‹Stackingèåˆ
    ç›®æ ‡ï¼šå‡†ç¡®ç‡ä»37%æå‡åˆ°50%+
    
    Phase 1: æ—¶é—´åºåˆ—CV + å…ƒç‰¹å¾ + HOLDæƒ©ç½š + é˜²è¿‡æ‹Ÿåˆ
    Phase 2A: 82ä¸ªé«˜çº§æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
    Phase 2B: Optunaè¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–
    Phase 3: Informer-2æ·±åº¦å­¦ä¹  + GMADLæŸå¤±å‡½æ•°
    """
    
    def __init__(self):
        super().__init__()
        
        # é›†æˆæ¨¡å‹å­—å…¸ {timeframe: {lgb, xgb, cat, inf, meta}}
        self.ensemble_models = {}
        
        # ğŸ”’ è®­ç»ƒçŠ¶æ€ç®¡ç†ï¼ˆç”Ÿäº§çº§åˆ«ï¼šåå°è®­ç»ƒï¼Œä¸å½±å“é¢„æµ‹ï¼‰
        self.training_in_progress = {}  # {timeframe: bool}
        self.models_ready = {}  # {timeframe: bool}
        self.background_training = False  # ğŸ”¥ åå°è®­ç»ƒæ ‡å¿—ï¼ˆä¸é˜»æ­¢é¢„æµ‹ï¼‰
        for tf in settings.TIMEFRAMES:
            self.training_in_progress[tf] = False
            self.models_ready[tf] = False
        
        # ğŸ”¥ æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ï¼ˆæ”¯æŒçƒ­æ›´æ–°ï¼‰
        self.model_versions = {}  # {timeframe: version_number}
        
        # é›†æˆæƒé‡ï¼ˆStackingè‡ªåŠ¨å­¦ä¹ ï¼Œè¿™é‡Œä½œä¸ºé™çº§æ–¹æ¡ˆï¼‰
        self.fallback_weights = {
            'lgb': 0.4,
            'xgb': 0.3,
            'cat': 0.3
        }
        
        # ğŸ”§ è¶…å‚æ•°ä¼˜åŒ–é…ç½®
        self.enable_hyperparameter_tuning = True  # âœ… å·²å¯ç”¨ï¼ˆPhase 2Bï¼‰
        self.optimize_all_models = True  # âœ… GPUåŠ é€Ÿä¸‹ä¼˜åŒ–æ‰€æœ‰æ¨¡å‹
        self.optimize_informer2 = True  # âœ… ä¼˜åŒ–Informer-2ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰
        self.optuna_n_trials = 100  # Optunaè¯•éªŒæ¬¡æ•°ï¼ˆä¼ ç»Ÿæ¨¡å‹ï¼‰
        self.informer_n_trials = 20  # Informer-2è¯•éªŒæ¬¡æ•°ï¼ˆä¿è¯è‡³å°‘å®Œæˆæ•´è½®æœç´¢ï¼‰
        self.optuna_timeout = 1800  # è¶…æ—¶30åˆ†é’Ÿï¼ˆGPUåŠ é€Ÿä¸‹è¶³å¤Ÿä¼˜åŒ–3ä¸ªæ¨¡å‹ï¼‰
        self.informer_timeout = 3600  # Informer-2è¶…æ—¶60åˆ†é’Ÿï¼ˆé˜²æ­¢ä»…å®Œæˆ1-2æ¬¡è¯•éªŒï¼‰
        
        # ğŸ¤– Informer-2æ·±åº¦å­¦ä¹ é…ç½®
        self.enable_informer2 = True  # âœ… å·²å¯ç”¨ï¼ˆPhase 3 - ç¥ç»ç½‘ç»œï¼‰
        self.informer_d_model = 128  # æ¨¡å‹ç»´åº¦
        self.informer_n_heads = 8  # æ³¨æ„åŠ›å¤´æ•°
        self.informer_n_layers = 3  # Encoderå±‚æ•°
        self.informer_epochs = 50  # è®­ç»ƒè½®æ•°ï¼ˆGPUåŠ é€Ÿï¼‰
        self.informer_batch_size = 256  # æ‰¹æ¬¡å¤§å°
        self.informer_lr = 0.0005  # å­¦ä¹ ç‡ï¼ˆé™ä½ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§ï¼š0.001â†’0.0005ï¼‰
        
        # ğŸ”¥ é«˜çº§å†…å­˜ä¼˜åŒ–é…ç½®ï¼ˆç”Ÿäº§çº§åˆ«ï¼‰
        self.use_gradient_checkpointing = True  # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœ50-70%å†…å­˜ï¼‰
        self.use_8bit_adam = True  # 8-bit Adamä¼˜åŒ–å™¨ï¼ˆèŠ‚çœ75%ä¼˜åŒ–å™¨å†…å­˜ï¼‰
        self.use_aggressive_amp = True  # æ¿€è¿›æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16 + TF32ï¼‰
        
        # ğŸ® GPUé…ç½®ï¼ˆä»configè¯»å–ï¼‰
        self.use_gpu = settings.USE_GPU
        self.gpu_device = settings.GPU_DEVICE
        
        # ğŸ”‘ åºåˆ—é•¿åº¦é…ç½®ï¼ˆç”¨äºInformer-2åºåˆ—è¾“å…¥ï¼‰
        # ğŸ¯ ä¼˜åŒ–ï¼šå‡å°‘åºåˆ—é•¿åº¦ä»¥é™ä½å†…å­˜å ç”¨ï¼ˆå‡å°‘80-90%ï¼‰
        self.seq_len_config = {
            '3m': 96,   # 96 Ã— 3åˆ†é’Ÿ = 4.8å°æ—¶ï¼ˆè¶³å¤ŸçŸ­æœŸæ¨¡å¼è¯†åˆ«ï¼‰
            '5m': 96,   # 96 Ã— 5åˆ†é’Ÿ = 8å°æ—¶ï¼ˆä¸»æ—¶é—´æ¡†æ¶ï¼‰
            '15m': 64   # 64 Ã— 15åˆ†é’Ÿ = 16å°æ—¶ï¼ˆè¶‹åŠ¿ç¡®è®¤ï¼‰
        }
        # ğŸ§  åºåˆ—å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å†…å­˜æ˜ å°„æ–‡ä»¶ï¼Œé¿å…æ•´åº“å¸¸é©»å†…å­˜
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¦ç”¨å†…å­˜æ˜ å°„ï¼ˆåœ¨äº¤å‰éªŒè¯æ—¶ä¼šå¯¼è‡´ç´¢å¼•é—®é¢˜ï¼‰
        self.use_sequence_memmap = False
        
        # ğŸ›¡ï¸ ç³»ç»Ÿä¼˜åŒ–ç»„ä»¶
        self.direction_checker = TradingDirectionConsistencyChecker()
        self.frequency_controller = AdaptiveFrequencyController()
        self.stability_enhancer = ModelStabilityEnhancer()
        
        # ğŸ“Š ä¼˜åŒ–æŒ‡æ ‡è®°å½•
        self.optimization_metrics = {
            'fatal_error_rate': 0.0,
            'fee_impact': 0.0,
            'model_stability': 0.0,
            'consistency_rate': 0.0
        }

        # ğŸ—‚ï¸ æ¨¡å‹ç›®å½•é˜²å¾¡å¼åˆå§‹åŒ–ï¼ˆé¿å…æ—©æœŸè°ƒç”¨å‡ºç° AttributeErrorï¼‰
        if not hasattr(self, 'model_dir') or not self.model_dir:
            self.model_dir = "models"
        try:
            Path(self.model_dir).mkdir(parents=True, exist_ok=True)
        except Exception:
            pass
        
        logger.info("âœ… é›†æˆMLæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆStackingå››æ¨¡å‹èåˆ + æ·±åº¦å­¦ä¹ ï¼‰")
        logger.info(f"   è¶…å‚æ•°ä¼˜åŒ–: {'å¯ç”¨' if self.enable_hyperparameter_tuning else 'å…³é—­'}")
        logger.info(f"   Informer-2ç¥ç»ç½‘ç»œ: {'å¯ç”¨' if self.enable_informer2 else 'å…³é—­'}")
        logger.info(f"   åºåˆ—é•¿åº¦é…ç½®: {self.seq_len_config}")
        logger.info(f"   GPUåŠ é€Ÿ: {'å¯ç”¨' if self.use_gpu else 'å…³é—­'} (è®¾å¤‡: {self.gpu_device if self.use_gpu else 'CPU'})")
        
        # ğŸ”¥ é«˜çº§å†…å­˜ä¼˜åŒ–çŠ¶æ€
        if self.enable_informer2:
            logger.info(f"   ğŸš€ é«˜çº§å†…å­˜ä¼˜åŒ–:")
            logger.info(f"      - æ¢¯åº¦æ£€æŸ¥ç‚¹: {'âœ… å¯ç”¨' if self.use_gradient_checkpointing else 'âŒ å…³é—­'} (èŠ‚çœ50-70%å†…å­˜)")
            logger.info(f"      - 8-bit Adam: {'âœ… å¯ç”¨' if self.use_8bit_adam else 'âŒ å…³é—­'} (èŠ‚çœ75%ä¼˜åŒ–å™¨å†…å­˜)")
            logger.info(f"      - æ¿€è¿›æ··åˆç²¾åº¦: {'âœ… å¯ç”¨' if self.use_aggressive_amp else 'âŒ å…³é—­'} (FP16+TF32)")
            
            # ä¼°ç®—å†…å­˜èŠ‚çœ
            if self.use_gradient_checkpointing and self.use_8bit_adam and self.use_aggressive_amp:
                logger.info(f"      ğŸ’¾ é¢„æœŸGPUå†…å­˜èŠ‚çœ: ~60-70% (6.3GB â†’ 2.0GB)")
            elif self.use_gradient_checkpointing:
                logger.info(f"      ğŸ’¾ é¢„æœŸGPUå†…å­˜èŠ‚çœ: ~40-50% (6.3GB â†’ 3.5GB)")
    
    def clear_gpu_memory(self):
        """
        æ¸…ç†GPUå†…å­˜
        
        åŠŸèƒ½ï¼š
        - æ¸…ç©ºPyTorchç¼“å­˜
        - åŒæ­¥GPUæ“ä½œ
        - å¼ºåˆ¶åƒåœ¾å›æ”¶
        - è®°å½•æ¸…ç†çŠ¶æ€
        """
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()
            
            # è®°å½•GPUå†…å­˜çŠ¶æ€
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            gpu_used = torch.cuda.memory_allocated(0)
            gpu_free = gpu_memory - gpu_used
            logger.info(f"ğŸ§¹ GPUå†…å­˜å·²æ¸…ç† (ä½¿ç”¨: {gpu_used/1024**3:.1f}GB, å¯ç”¨: {gpu_free/1024**3:.1f}GB)")
        else:
            logger.info("ğŸ§¹ CPUæ¨¡å¼ï¼Œæ— éœ€æ¸…ç†GPUå†…å­˜")
    
    def monitor_gpu_memory(self):
        """
        ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        
        Returns:
            Dict: GPUå†…å­˜çŠ¶æ€ä¿¡æ¯
        """
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            gpu_used = torch.cuda.memory_allocated(0)
            gpu_free = gpu_memory - gpu_used
            gpu_reserved = torch.cuda.memory_reserved(0)
            
            return {
                'total': gpu_memory,
                'used': gpu_used,
                'free': gpu_free,
                'reserved': gpu_reserved,
                'usage_percent': (gpu_used / gpu_memory) * 100
            }
        else:
            return {'error': 'GPUä¸å¯ç”¨'}
    
    async def _prepare_diverse_training_data(self, timeframe: str, days_multiplier: float = 1.0) -> pd.DataFrame:
        """
        å‡†å¤‡å·®å¼‚åŒ–è®­ç»ƒæ•°æ®ï¼ˆä¸åŒå¤©æ•°ï¼‰
        
        Args:
            timeframe: æ—¶é—´æ¡†æ¶
            days_multiplier: å¤©æ•°å€æ•°ï¼ˆ1.0=æ ‡å‡†ï¼Œ1.5=+50%ï¼Œ2.0=+100%ï¼‰
        
        Returns:
            Kçº¿æ•°æ®DataFrame
        """
        try:
            # ğŸ”¥ å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            from app.exchange.binance_client import binance_client
            
            symbol = settings.SYMBOL
            
            # ğŸ”‘ åŸºç¡€è®­ç»ƒå¤©æ•°é…ç½®ï¼ˆè¶…çŸ­çº¿ç­–ç•¥ï¼šç¡®ä¿è¶³å¤Ÿæ ·æœ¬ï¼‰
            base_days_config = {
                '3m': 120,   # 3m: 120å¤©=57,600æ¡ï¼ˆé«˜é¢‘æ ·æœ¬ï¼Œæ•æ‰æçŸ­æœŸæ¨¡å¼ï¼‰
                '5m': 120,   # 5m: 120å¤©=34,560æ¡ï¼ˆä¸»æ—¶é—´æ¡†æ¶ï¼Œå……è¶³æ ·æœ¬ï¼‰
                '15m': 120   # 15m: 120å¤©=11,520æ¡ï¼ˆä¸­æœŸè¿‡æ»¤ï¼Œè¶³å¤Ÿè¯†åˆ«è¶‹åŠ¿ï¼‰
            }
            base_days = base_days_config.get(timeframe, 120)
            
            # åº”ç”¨å€æ•°
            training_days = int(base_days * days_multiplier)
            
            # è®¡ç®—éœ€è¦çš„Kçº¿æ•°é‡
            interval_minutes = {
                '3m': 3, '5m': 5, '15m': 15
            }
            minutes = interval_minutes.get(timeframe, 60)
            required_klines = int((training_days * 24 * 60) / minutes)
            
            logger.info(f"ğŸ“¥ è·å–{timeframe}æ•°æ®ï¼ˆÃ—{days_multiplier}å€ï¼‰: {required_klines}æ¡Kçº¿ ({training_days}å¤©)")
            
            # âœ… ç»Ÿä¸€ä½¿ç”¨åˆ†é¡µæ–¹æ³•ï¼ˆè‡ªåŠ¨å¤„ç†è¶…è¿‡1500çš„æƒ…å†µï¼‰
            all_klines = binance_client.get_klines_paginated(
                symbol=symbol,
                interval=timeframe,
                limit=required_klines,
                rate_limit_delay=0.1
            )
            
            # è½¬æ¢ä¸ºDataFrameï¼ˆä¸ä¾èµ–reverseï¼Œç›´æ¥ç”¨æ—¶é—´æˆ³æ’åºï¼‰
            df = pd.DataFrame(all_klines)
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            
            # ğŸ”‘ å…³é”®ï¼šä¾èµ–æ—¶é—´æˆ³æ’åºï¼Œè€Œä¸æ˜¯å‡è®¾APIè¿”å›é¡ºåº
            df = df.sort_values('timestamp', ascending=True)  # æ˜ç¡®æŒ‡å®šå‡åºï¼ˆæ—§â†’æ–°ï¼‰
            df = df.drop_duplicates(subset=['timestamp'], keep='last')
            df = df.set_index('timestamp')
            
            logger.info(f"âœ… è·å–æˆåŠŸ: {len(df)}æ¡ï¼ˆÃ—{days_multiplier}å€æ•°æ®ï¼‰")
            
            return df
            
        except Exception as e:
            logger.error(f"å‡†å¤‡å·®å¼‚åŒ–è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def _prepare_features_labels_reuse(self, df: pd.DataFrame, timeframe: str) -> Tuple[pd.DataFrame, pd.Series]:
        """
        å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå¤ç”¨å·²é€‰æ‹©çš„ç‰¹å¾åˆ—ï¼‰
        
        ç”¨é€”ï¼šä¸ºXGBoostå’ŒCatBoostå‡†å¤‡æ•°æ®æ—¶ï¼Œå¤ç”¨LightGBMå·²é€‰æ‹©çš„ç‰¹å¾åˆ—
        
        Args:
            df: åŒ…å«labelåˆ—çš„DataFrame
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            (X, y): ç‰¹å¾DataFrameå’Œæ ‡ç­¾Series
        """
        try:
            # ä½¿ç”¨å·²é€‰æ‹©çš„ç‰¹å¾åˆ—ï¼ˆLightGBMè®­ç»ƒæ—¶å·²ç¡®å®šï¼‰
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            
            if not feature_columns:
                logger.error(f"{timeframe} ç‰¹å¾åˆ—æœªæ‰¾åˆ°ï¼Œæ— æ³•å¤ç”¨")
                return pd.DataFrame(), pd.Series()
            
            X = df[feature_columns].copy()
            y = df['label'].copy()
            
            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            mask = ~(X.isna().any(axis=1) | y.isna())
            X = X[mask]
            y = y[mask]
            
            
            return X, y
            
        except Exception as e:
            logger.error(f"å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå¤ç”¨ï¼‰å¤±è´¥: {e}")
            return pd.DataFrame(), pd.Series()
    
    def _create_sequence_input(
        self,
        df: pd.DataFrame,
        seq_len: int,
        timeframe: str
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        æ„é€ åºåˆ—è¾“å…¥ï¼ˆç”¨äºInformer-2æ¨¡å‹ï¼‰- å†…å­˜ä¼˜åŒ–ç‰ˆ
        
        ä½¿ç”¨æ»‘åŠ¨çª—å£å°†å•ç‚¹ç‰¹å¾è½¬æ¢ä¸ºåºåˆ—è¾“å…¥ï¼Œå……åˆ†åˆ©ç”¨å†å²æ—¶é—´åºåˆ—ä¿¡æ¯
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ä½¿ç”¨float32ä»£æ›¿float64ï¼ŒèŠ‚çœ50%å†…å­˜
        2. é¢„åˆ†é…NumPyæ•°ç»„ï¼Œé¿å…åŠ¨æ€append
        3. é¢„å…ˆè½¬æ¢DataFrameä¸ºNumPyæ•°ç»„ï¼Œé¿å…é‡å¤ilocåˆ‡ç‰‡
        4. æ˜¾å¼åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾ä¸­é—´æ•°æ®
        
        Args:
            df: ç‰¹å¾å·¥ç¨‹åçš„DataFrameï¼ˆåŒ…å«labelåˆ—ï¼‰
            seq_len: åºåˆ—é•¿åº¦ï¼ˆ3m=480, 5m=288, 15m=96ï¼‰
            timeframe: æ—¶é—´æ¡†æ¶ï¼ˆ3m/5m/15mï¼‰
        
        Returns:
            X_seq: (n_samples, seq_len, n_features) - åºåˆ—ç‰¹å¾ï¼ˆfloat32ï¼‰
            y: (n_samples,) - æ ‡ç­¾ï¼ˆint8ï¼‰
        """
        try:
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            
            if not feature_columns:
                logger.error(f"âŒ {timeframe} ç‰¹å¾åˆ—æœªæ‰¾åˆ°ï¼Œæ— æ³•æ„é€ åºåˆ—è¾“å…¥")
                return np.array([]), np.array([])
            
            # ğŸ”¥ ä¼˜åŒ–1ï¼šé¢„å…ˆè½¬æ¢ä¸ºNumPyæ•°ç»„ï¼ˆé¿å…é‡å¤DataFrameåˆ‡ç‰‡ï¼‰
            logger.debug(f"ğŸ”§ {timeframe} å¼€å§‹æ„é€ åºåˆ—è¾“å…¥ï¼ˆseq_len={seq_len}ï¼‰...")
            X_all = df[feature_columns].values.astype(np.float32)  # float32èŠ‚çœ50%å†…å­˜
            y_all = df['label'].values.astype(np.int8)  # int8èŠ‚çœå†…å­˜
            
            n_total = len(df)
            n_features = len(feature_columns)
            max_samples = n_total - seq_len
            
            if max_samples <= 0:
                logger.warning(f"âš ï¸ {timeframe} æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•æ„é€ åºåˆ—ï¼ˆéœ€è¦>{seq_len}æ¡ï¼‰")
                return np.array([]), np.array([])
            
            # ğŸ”¥ ä¼˜åŒ–2ï¼šé¢„åˆ†é…å†…å­˜ï¼ˆé¿å…åŠ¨æ€appendå’Œå†…å­˜ç¢ç‰‡ï¼‰
            X_seq = np.empty((max_samples, seq_len, n_features), dtype=np.float32)
            y = np.empty(max_samples, dtype=np.int8)
            
            # ğŸ”¥ ä¼˜åŒ–3ï¼šä½¿ç”¨NumPyåˆ‡ç‰‡ï¼ˆæ¯”DataFrame.ilocå¿«5-10å€ï¼‰
            valid_count = 0
            for i in range(seq_len, n_total):
                idx = i - seq_len
                X_window = X_all[idx:i]  # NumPyåˆ‡ç‰‡ï¼ŒO(1)å¤æ‚åº¦
                y_label = y_all[i]
                
                # ä»…æ£€æŸ¥NaNï¼ˆå·²åœ¨ç‰¹å¾å·¥ç¨‹é˜¶æ®µå¤„ç†è¿‡infå’Œå¤§å€¼ï¼‰
                if not np.isnan(X_window).any() and not np.isnan(y_label):
                    X_seq[valid_count] = X_window
                    y[valid_count] = y_label
                    valid_count += 1
            
            # ğŸ”¥ ä¼˜åŒ–4ï¼šæˆªæ–­åˆ°æœ‰æ•ˆé•¿åº¦ï¼ˆé‡Šæ”¾æœªä½¿ç”¨å†…å­˜ï¼‰
            X_seq = X_seq[:valid_count]
            y = y[:valid_count]
            
            # è®¡ç®—å†…å­˜å ç”¨
            memory_mb = (X_seq.nbytes + y.nbytes) / (1024 ** 2)

            # å¯é€‰ï¼šè½ç›˜ä¸ºå†…å­˜æ˜ å°„ï¼Œé¿å…æ•´åº“å¸¸é©»å†…å­˜
            if getattr(self, 'use_sequence_memmap', False):
                try:
                    memmap_dir = self.model_dir if hasattr(self, 'model_dir') and self.model_dir else 'models'
                    os.makedirs(memmap_dir, exist_ok=True)
                    seq_path = os.path.join(memmap_dir, f"{settings.SYMBOL}_{timeframe}_Xseq.npy")
                    y_path = os.path.join(memmap_dir, f"{settings.SYMBOL}_{timeframe}_Yseq.npy")

                    # å†™å…¥ä¸º.npyï¼ˆå†…å«shapeä¸dtypeï¼‰ï¼Œå†ä»¥åªè¯»å†…å­˜æ˜ å°„æ–¹å¼æ‰“å¼€
                    mm_x = open_memmap(seq_path, mode='w+', dtype=np.float32, shape=X_seq.shape)
                    mm_x[:] = X_seq
                    del mm_x
                    mm_y = open_memmap(y_path, mode='w+', dtype=np.int8, shape=y.shape)
                    mm_y[:] = y
                    del mm_y

                    # é‡Šæ”¾å†…å­˜ä¸­æ•°ç»„ï¼Œä½¿ç”¨å†…å­˜æ˜ å°„è¯»å–
                    del X_seq, y
                    gc.collect()

                    X_seq = np.load(seq_path, mmap_mode='r')
                    y = np.load(y_path, mmap_mode='r')

                    logger.info(f"   å·²å¯ç”¨å†…å­˜æ˜ å°„: {seq_path} ({memory_mb:.1f} MB)")
                except Exception:
                    logger.warning("âš ï¸ åºåˆ—å†…å­˜æ˜ å°„å¤±è´¥ï¼Œå›é€€ä¸ºå†…å­˜æ•°ç»„")

            logger.info(f"âœ… {timeframe} åºåˆ—è¾“å…¥æ„é€ å®Œæˆ: {X_seq.shape} (æ ·æœ¬æ•°={valid_count}, åºåˆ—é•¿åº¦={seq_len}, ç‰¹å¾æ•°={n_features})")
            logger.info(f"   åŸå§‹æ ·æœ¬æ•°: {n_total}, åºåˆ—æ ·æœ¬æ•°: {valid_count}, å‡å°‘: {n_total - valid_count}ä¸ª")
            logger.info(f"   å†…å­˜å ç”¨: {memory_mb:.1f} MB (float32ä¼˜åŒ–)")

            # ğŸ”¥ ä¼˜åŒ–5ï¼šæ˜¾å¼åƒåœ¾å›æ”¶ï¼ˆé‡Šæ”¾X_all, y_allç­‰ä¸­é—´æ•°æ®ï¼‰
            del X_all, y_all
            gc.collect()

            return X_seq, y
            
        except Exception as e:
            logger.error(f"âŒ æ„é€ åºåˆ—è¾“å…¥å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return np.array([]), np.array([])
    
    async def train_all_timeframes(self) -> Dict[str, Any]:
        """
        è®­ç»ƒæ‰€æœ‰æ—¶é—´æ¡†æ¶çš„é›†æˆæ¨¡å‹
        
        Returns:
            è®­ç»ƒç»“æœå’ŒæŒ‡æ ‡
        """
        try:
            logger.info("ğŸš€ å¼€å§‹Stackingé›†æˆæ¨¡å‹è®­ç»ƒ...")
            if self.enable_informer2 and TORCH_AVAILABLE:
                logger.info(f"âœ¨ å››æ¨¡å‹èåˆ: LightGBM + XGBoost + CatBoost + Informer-2 (GMADLæŸå¤±)")
                logger.info(f"   è¶…å‚æ•°ä¼˜åŒ–: {'å¯ç”¨' if self.enable_hyperparameter_tuning else 'å…³é—­'}")
                logger.info(f"   æ·±åº¦å­¦ä¹ : GPU {'å¯ç”¨' if torch.cuda.is_available() else 'ä¸å¯ç”¨'}")
            else:
                logger.info(f"ä¸‰æ¨¡å‹èåˆ: LightGBM + XGBoost + CatBoost")
            logger.info(f"æ—¶é—´æ¡†æ¶: {settings.TIMEFRAMES}")
            logger.info("")
            
            results = {}
            
            for timeframe in settings.TIMEFRAMES:
                logger.info("=" * 60)
                logger.info(f"ğŸ“Š è®­ç»ƒ {timeframe} é›†æˆæ¨¡å‹...")
                logger.info("=" * 60)
                
                result = await self._train_ensemble_single_timeframe(timeframe)
                results[timeframe] = result
                
                logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {result['accuracy']:.4f}")
                logger.info("")
            
            # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
            avg_accuracy = np.mean([r['accuracy'] for r in results.values()])
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ Stackingé›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
            logger.info(f"æˆåŠŸè®­ç»ƒ: {len(results)}/{len(settings.TIMEFRAMES)} ä¸ªæ—¶é—´æ¡†æ¶")
            logger.info(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
            logger.info("=" * 60)
            logger.info("")
            
            # ğŸ”‘ ä¿å­˜æ¨¡å‹æŒ‡æ ‡åˆ°Redisç¼“å­˜ï¼ˆä¾›health_monitorè¯»å–ï¼‰
            metrics_cache = {
                'accuracy': float(avg_accuracy),
                'timeframes': {tf: float(r['accuracy']) for tf, r in results.items()},
                'training_date': datetime.now().isoformat(),
                'method': 'Stacking Ensemble',
                'models': ['LightGBM', 'XGBoost', 'CatBoost']
            }
            await cache_manager.set_model_metrics(settings.SYMBOL, metrics_cache)
            
            return {
                'results': results,
                'average_accuracy': avg_accuracy,
                'training_date': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            raise
    
    async def _train_ensemble_single_timeframe(self, timeframe: str) -> Dict[str, Any]:
        """
        è®­ç»ƒå•ä¸ªæ—¶é—´æ¡†æ¶çš„Stackingé›†æˆæ¨¡å‹
        
        æ”¹è¿›ï¼šä¸‰ä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼Œå¢åŠ å¤šæ ·æ€§
        
        æµç¨‹:
        1. å‡†å¤‡ä¸‰ä»½ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼ˆä¸åŒå¤©æ•°ï¼‰
        2. è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆLightGBM, XGBoost, CatBoostï¼‰- å„ç”¨ä¸åŒæ•°æ®
        3. ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆåŸºç¡€æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ï¼‰
        4. è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰
        5. è¯„ä¼°é›†æˆæ•ˆæœ
        """
        # ğŸ”¥ ç”Ÿäº§çº§åˆ«ï¼šåå°è®­ç»ƒï¼Œä¸å½±å“é¢„æµ‹
        self.training_in_progress[timeframe] = True
        self.background_training = True
        logger.info(f"ğŸ”„ {timeframe} åå°è®­ç»ƒå·²å¼€å§‹ï¼ˆé¢„æµ‹åŠŸèƒ½ç»§ç»­è¿è¡Œï¼Œè®­ç»ƒå®Œæˆåçƒ­æ›´æ–°æ¨¡å‹ï¼‰")
        
        try:
            # 1ï¸âƒ£ ä¸ºä¸‰ä¸ªæ¨¡å‹å‡†å¤‡ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
            logger.info(f"ğŸ“¥ ä¸ºä¸‰ä¸ªæ¨¡å‹å‡†å¤‡å·®å¼‚åŒ–è®­ç»ƒæ•°æ®...")
            
            # LightGBM: ä½¿ç”¨è¾ƒæ–°æ•°æ®ï¼ˆæ ‡å‡†å¤©æ•°ï¼‰
            data_lgb = await self._prepare_training_data_for_timeframe(timeframe)
            logger.info(f"âœ… LightGBMæ•°æ®: {len(data_lgb)}æ¡ï¼ˆæ ‡å‡†ï¼‰")
            
            # XGBoost: ä½¿ç”¨æ›´å¤šæ•°æ®ï¼ˆ+50%å¤©æ•°ï¼‰
            data_xgb = await self._prepare_diverse_training_data(timeframe, days_multiplier=1.5)
            logger.info(f"âœ… XGBoostæ•°æ®: {len(data_xgb)}æ¡ï¼ˆ+50%å¤©æ•°ï¼‰")
            
            # CatBoost: ä½¿ç”¨æœ€å¤šæ•°æ®ï¼ˆ+100%å¤©æ•°ï¼‰
            data_cat = await self._prepare_diverse_training_data(timeframe, days_multiplier=2.0)
            logger.info(f"âœ… CatBoostæ•°æ®: {len(data_cat)}æ¡ï¼ˆ+100%å¤©æ•°ï¼‰")
            
            # 2ï¸âƒ£ å¤„ç†ä¸‰ä»½æ•°æ®ï¼ˆç‰¹å¾å·¥ç¨‹ + æ ‡ç­¾ + ç‰¹å¾é€‰æ‹©ï¼‰
            logger.info(f"ğŸ”§ å¤„ç†ä¸‰ä»½è®­ç»ƒæ•°æ®...")
            
            # å¤„ç†LightGBMæ•°æ®
            data_lgb = self.feature_engineer.create_features(data_lgb)
            data_lgb = self._create_labels(data_lgb, timeframe=timeframe)
            X_lgb, y_lgb = self._prepare_features_labels(data_lgb, timeframe)
            X_lgb_scaled = self._scale_features(X_lgb, timeframe, fit=True)
            
            # å¤„ç†XGBoostæ•°æ®ï¼ˆå¤ç”¨åŒä¸€ä¸ªscalerï¼‰
            data_xgb = self.feature_engineer.create_features(data_xgb)
            data_xgb = self._create_labels(data_xgb, timeframe=timeframe)
            X_xgb, y_xgb = self._prepare_features_labels_reuse(data_xgb, timeframe)
            X_xgb_scaled = self._scale_features(X_xgb, timeframe, fit=False)
            
            # å¤„ç†CatBoostæ•°æ®ï¼ˆå¤ç”¨åŒä¸€ä¸ªscalerï¼‰
            data_cat = self.feature_engineer.create_features(data_cat)
            data_cat = self._create_labels(data_cat, timeframe=timeframe)
            X_cat, y_cat = self._prepare_features_labels_reuse(data_cat, timeframe)
            X_cat_scaled = self._scale_features(X_cat, timeframe, fit=False)
            
            logger.info(f"âœ… ä¸‰ä»½æ•°æ®å¤„ç†å®Œæˆ: LGB={len(X_lgb)}, XGB={len(X_xgb)}, CAT={len(X_cat)}")
            
            # ğŸ†• æ„é€ åºåˆ—è¾“å…¥ï¼ˆä»…ç”¨äºInformer-2ï¼‰
            X_seq_lgb, y_seq_lgb = None, None
            if self.enable_informer2 and TORCH_AVAILABLE:
                seq_len = self.seq_len_config.get(timeframe, 96)
                logger.info(f"ğŸ”§ æ„é€ Informer-2åºåˆ—è¾“å…¥ï¼ˆseq_len={seq_len}ï¼‰...")
                X_seq_lgb, y_seq_lgb = self._create_sequence_input(data_lgb, seq_len, timeframe)
                
                if len(X_seq_lgb) == 0:
                    logger.warning(f"âš ï¸ åºåˆ—è¾“å…¥æ„é€ å¤±è´¥ï¼Œå°†è·³è¿‡Informer-2è®­ç»ƒ")
                    self.enable_informer2 = False
            
            # 3ï¸âƒ£ æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆä½¿ç”¨æœ€çŸ­çš„æ•°æ®é•¿åº¦ä½œä¸ºéªŒè¯é›†åŸºå‡†ï¼‰
            min_len = min(len(X_lgb_scaled), len(X_xgb_scaled), len(X_cat_scaled))
            split_idx = int(min_len * 0.8)
            
            # ğŸ”‘ åˆ†å‰²æ•°æ®ï¼ˆå–æœ€æ–°çš„æ•°æ®ï¼Œä¿è¯æ—¶é—´å¯¹é½ï¼‰
            if isinstance(X_lgb_scaled, np.ndarray):
                X_lgb_train, X_lgb_val = X_lgb_scaled[-min_len:][:split_idx], X_lgb_scaled[-min_len:][split_idx:]
                X_xgb_train, X_xgb_val = X_xgb_scaled[-min_len:][:split_idx], X_xgb_scaled[-min_len:][split_idx:]
                X_cat_train, X_cat_val = X_cat_scaled[-min_len:][:split_idx], X_cat_scaled[-min_len:][split_idx:]
            else:
                X_lgb_train, X_lgb_val = X_lgb_scaled.iloc[-min_len:][:split_idx], X_lgb_scaled.iloc[-min_len:][split_idx:]
                X_xgb_train, X_xgb_val = X_xgb_scaled.iloc[-min_len:][:split_idx], X_xgb_scaled.iloc[-min_len:][split_idx:]
                X_cat_train, X_cat_val = X_cat_scaled.iloc[-min_len:][:split_idx], X_cat_scaled.iloc[-min_len:][split_idx:]
            
            y_lgb_train, y_lgb_val = y_lgb.iloc[-min_len:][:split_idx], y_lgb.iloc[-min_len:][split_idx:]
            y_xgb_train, y_xgb_val = y_xgb.iloc[-min_len:][:split_idx], y_xgb.iloc[-min_len:][split_idx:]
            y_cat_train, y_cat_val = y_cat.iloc[-min_len:][:split_idx], y_cat.iloc[-min_len:][split_idx:]
            
            # ğŸ†• åˆ†å‰²åºåˆ—æ•°æ®ï¼ˆç”¨äºInformer-2ï¼‰
            X_seq_train, X_seq_val, y_seq_train, y_seq_val = None, None, None, None
            if self.enable_informer2 and X_seq_lgb is not None:
                seq_split_idx = int(len(X_seq_lgb) * 0.8)
                X_seq_train = X_seq_lgb[:seq_split_idx]
                X_seq_val = X_seq_lgb[seq_split_idx:]
                y_seq_train = y_seq_lgb[:seq_split_idx]
                y_seq_val = y_seq_lgb[seq_split_idx:]
                logger.info(f"ğŸ“Š {timeframe} åºåˆ—æ•°æ®åˆ†å‰²: è®­ç»ƒ{len(X_seq_train)}æ¡, éªŒè¯{len(X_seq_val)}æ¡")
                
                # ğŸ”‘ å…³é”®ä¿®å¤ï¼šå¯¹é½ä¼ ç»Ÿæ¨¡å‹çš„éªŒè¯é›†åˆ°åºåˆ—æ•°æ®çš„é•¿åº¦
                # åºåˆ—æ•°æ®æ¯”åŸå§‹æ•°æ®å°‘seq_lenä¸ªæ ·æœ¬ï¼Œéœ€è¦å¯¹é½
                seq_val_len = len(X_seq_val)
                if seq_val_len < len(X_lgb_val):
                    logger.warning(f"âš ï¸ å¯¹é½éªŒè¯é›†ï¼šä¼ ç»Ÿæ¨¡å‹{len(X_lgb_val)}æ¡ â†’ Informer-2{seq_val_len}æ¡")
                    # å–ä¼ ç»Ÿæ¨¡å‹éªŒè¯é›†çš„æœ€åseq_val_lenä¸ªæ ·æœ¬ï¼ˆæ—¶é—´å¯¹é½ï¼‰
                    if isinstance(X_lgb_val, np.ndarray):
                        X_lgb_val = X_lgb_val[-seq_val_len:]
                        X_xgb_val = X_xgb_val[-seq_val_len:]
                        X_cat_val = X_cat_val[-seq_val_len:]
                    else:
                        X_lgb_val = X_lgb_val.iloc[-seq_val_len:]
                        X_xgb_val = X_xgb_val.iloc[-seq_val_len:]
                        X_cat_val = X_cat_val.iloc[-seq_val_len:]
                    
                    y_lgb_val = y_lgb_val.iloc[-seq_val_len:]
                    y_xgb_val = y_xgb_val.iloc[-seq_val_len:]
                    y_cat_val = y_cat_val.iloc[-seq_val_len:]
            
            logger.info(f"ğŸ“Š {timeframe} ä¼ ç»Ÿæ¨¡å‹æ•°æ®åˆ†å‰²: è®­ç»ƒ{len(X_lgb_train)}æ¡ï¼ˆå¯¹é½åï¼‰, éªŒè¯{len(X_lgb_val)}æ¡")
            
            # 4ï¸âƒ£ è®­ç»ƒStackingé›†æˆæ¨¡å‹ï¼ˆä½¿ç”¨å·®å¼‚åŒ–æ•°æ® + åºåˆ—è¾“å…¥ï¼‰
            logger.info(f"ğŸš‚ å¼€å§‹è®­ç»ƒ {timeframe} Stackingé›†æˆï¼ˆå·®å¼‚åŒ–æ•°æ®ï¼‰...")
            ensemble_result = self._train_stacking_diverse(
                X_lgb_train, y_lgb_train, X_lgb_val, y_lgb_val,
                X_xgb_train, y_xgb_train, X_xgb_val, y_xgb_val,
                X_cat_train, y_cat_train, X_cat_val, y_cat_val,
                X_seq_train, y_seq_train, X_seq_val, y_seq_val,
                timeframe
            )
            
            # 8ï¸âƒ£ ä¿å­˜é›†æˆæ¨¡å‹
            self._save_ensemble_models(timeframe)
            
            logger.info(f"â±ï¸ {timeframe} è®­ç»ƒè€—æ—¶: {ensemble_result['training_time']:.2f}ç§’")
            
            return ensemble_result
            
        except Exception as e:
            logger.error(f"âŒ {timeframe} é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def _train_stacking_diverse(
        self,
        X_lgb_train, y_lgb_train, X_lgb_val, y_lgb_val,
        X_xgb_train, y_xgb_train, X_xgb_val, y_xgb_val,
        X_cat_train, y_cat_train, X_cat_val, y_cat_val,
        X_seq_train, y_seq_train, X_seq_val, y_seq_val,
        timeframe: str
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å·®å¼‚åŒ–æ•°æ®è®­ç»ƒStackingé›†æˆæ¨¡å‹ï¼ˆæ”¯æŒåºåˆ—è¾“å…¥ï¼‰
        
        Args:
            X_lgb_train, y_lgb_train: LightGBMè®­ç»ƒæ•°æ®
            X_lgb_val, y_lgb_val: LightGBMéªŒè¯æ•°æ®
            X_xgb_train, y_xgb_train: XGBoostè®­ç»ƒæ•°æ®
            X_xgb_val, y_xgb_val: XGBoostéªŒè¯æ•°æ®
            X_cat_train, y_cat_train: CatBoostè®­ç»ƒæ•°æ®
            X_cat_val, y_cat_val: CatBoostéªŒè¯æ•°æ®
            X_seq_train, y_seq_train: Informer-2åºåˆ—è®­ç»ƒæ•°æ®
            X_seq_val, y_seq_val: Informer-2åºåˆ—éªŒè¯æ•°æ®
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        start_time = time.time()
        
        try:
            # ğŸ”§ Optunaè¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            lgb_params_optimized = None
            xgb_params_optimized = None
            cat_params_optimized = None
            inf_params_optimized = None
            
            if self.enable_hyperparameter_tuning:
                # ğŸ¤– ä¼˜å…ˆä¼˜åŒ–Informer-2ï¼ˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰
                if self.enable_informer2 and self.optimize_informer2 and TORCH_AVAILABLE and X_seq_train is not None:
                    logger.info(f"ğŸ¤– å¯åŠ¨Informer-2è¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰- ä¼˜å…ˆä¼˜åŒ–...")
                    logger.info(f"   GPUåŠ é€Ÿ: {'å¯ç”¨' if self.use_gpu else 'å…³é—­'}")
                    logger.info(f"   è¯•éªŒæ¬¡æ•°: {self.informer_n_trials}æ¬¡, è¶…æ—¶: {self.informer_timeout}ç§’")
                    logger.info(f"   åºåˆ—è¾“å…¥å½¢çŠ¶: {X_seq_train.shape} (æ ·æœ¬æ•°, åºåˆ—é•¿åº¦, ç‰¹å¾æ•°)")
                    
                    # ğŸ”‘ å…³é”®ä¿®å¤ï¼šä½¿ç”¨åºåˆ—æ•°æ®è€Œä¸æ˜¯2Dæ•°æ®
                    inf_optimizer = HyperparameterOptimizer(
                        X=X_seq_train,  # ä½¿ç”¨3Dåºåˆ—æ•°æ®
                        y=y_seq_train,  # ä½¿ç”¨å¯¹åº”çš„åºåˆ—æ ‡ç­¾
                        timeframe=timeframe,
                        model_type="informer2",
                        use_gpu=self.use_gpu
                    )
                    inf_params_optimized = inf_optimizer.optimize(
                        n_trials=self.informer_n_trials,
                        timeout=self.informer_timeout,
                        show_progress=False
                    )
                    logger.info(f"âœ… Informer-2è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ: æœ€ä½³CVå‡†ç¡®ç‡={inf_optimizer.best_score:.4f}")
                
                # ğŸ”§ ç„¶åä¼˜åŒ–ä¼ ç»Ÿæ¨¡å‹
                if self.optimize_all_models:
                    logger.info(f"ğŸ”§ å¯åŠ¨ä¼ ç»Ÿæ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaï¼‰- ä¼˜åŒ–å…¨éƒ¨3ä¸ªä¼ ç»Ÿæ¨¡å‹...")
                else:
                    logger.info(f"ğŸ”§ å¯åŠ¨ä¼ ç»Ÿæ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaï¼‰- ä»…ä¼˜åŒ–LightGBM...")
                logger.info(f"   GPUåŠ é€Ÿ: {'å¯ç”¨' if self.use_gpu else 'å…³é—­'}")
                logger.info(f"   æ¯æ¨¡å‹è¯•éªŒ: {self.optuna_n_trials}æ¬¡, è¶…æ—¶: {self.optuna_timeout}ç§’")
                
                # ä¼˜åŒ–LightGBM
                logger.info(f"   ğŸ”§ [1/{'3' if self.optimize_all_models else '1'}] ä¼˜åŒ–LightGBM...")
                lgb_optimizer = HyperparameterOptimizer(
                    X=X_lgb_train.values if isinstance(X_lgb_train, pd.DataFrame) else X_lgb_train,
                    y=y_lgb_train,
                    timeframe=timeframe,
                    model_type="lightgbm",
                    use_gpu=self.use_gpu
                )
                lgb_params_optimized = lgb_optimizer.optimize(
                    n_trials=self.optuna_n_trials,
                    timeout=self.optuna_timeout,
                    show_progress=False  # å…³é—­è¿›åº¦æ¡ï¼ˆé¿å…æ··ä¹±ï¼‰
                )
                
                # ä¼˜åŒ–XGBoostï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.optimize_all_models:
                    logger.info(f"   ğŸ”§ [2/3] ä¼˜åŒ–XGBoost...")
                    xgb_optimizer = HyperparameterOptimizer(
                        X=X_xgb_train.values if isinstance(X_xgb_train, pd.DataFrame) else X_xgb_train,
                        y=y_xgb_train,
                        timeframe=timeframe,
                        model_type="xgboost",
                        use_gpu=self.use_gpu
                    )
                    xgb_params_optimized = xgb_optimizer.optimize(
                        n_trials=self.optuna_n_trials,
                        timeout=self.optuna_timeout,
                        show_progress=False
                    )
                
                # ä¼˜åŒ–CatBoostï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.optimize_all_models:
                    logger.info(f"   ğŸ”§ [3/3] ä¼˜åŒ–CatBoost...")
                    cat_optimizer = HyperparameterOptimizer(
                        X=X_cat_train.values if isinstance(X_cat_train, pd.DataFrame) else X_cat_train,
                        y=y_cat_train,
                        timeframe=timeframe,
                        model_type="catboost",
                        use_gpu=self.use_gpu
                    )
                    cat_params_optimized = cat_optimizer.optimize(
                        n_trials=self.optuna_n_trials,
                        timeout=self.optuna_timeout,
                        show_progress=False
                    )
                
                logger.info(f"âœ… ä¼ ç»Ÿæ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
                if lgb_params_optimized:
                    logger.info(f"   LightGBMæœ€ä½³CV: {lgb_optimizer.best_score:.4f}")
                if xgb_params_optimized:
                    logger.info(f"   XGBoostæœ€ä½³CV:  {xgb_optimizer.best_score:.4f}")
                if cat_params_optimized:
                    logger.info(f"   CatBoostæœ€ä½³CV: {cat_optimizer.best_score:.4f}")
            
            # 1ï¸âƒ£ è®­ç»ƒå››ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆInformer-2ä¼˜å…ˆè®­ç»ƒï¼‰
            # ğŸ¤– ä¼˜å…ˆè®­ç»ƒInformer-2ï¼ˆæ·±åº¦å­¦ä¹  + GMADLæŸå¤± + åºåˆ—è¾“å…¥ï¼‰
            inf_model = None
            if self.enable_informer2 and TORCH_AVAILABLE and X_seq_train is not None:
                logger.info(f"ğŸ¤– è®­ç»ƒInformer-2ï¼ˆæ·±åº¦å­¦ä¹  + GMADLæŸå¤± + åºåˆ—è¾“å…¥ï¼‰- ä¼˜å…ˆè®­ç»ƒ...")
                inf_model = self._train_informer2(X_seq_train, y_seq_train, timeframe, custom_params=inf_params_optimized)
            
            # ğŸ”§ ç„¶åè®­ç»ƒä¼ ç»Ÿæ¨¡å‹
            logger.info(f"ğŸš‚ è®­ç»ƒLightGBMï¼ˆ{timeframe} æ ‡å‡†æ•°æ®ï¼‰...")
            lgb_model = self._train_lightgbm(X_lgb_train, y_lgb_train, timeframe, custom_params=lgb_params_optimized)
            
            logger.info(f"ğŸš‚ è®­ç»ƒXGBoostï¼ˆ{timeframe} +50%æ•°æ®ï¼‰...")
            xgb_model = self._train_xgboost(X_xgb_train, y_xgb_train, timeframe, custom_params=xgb_params_optimized)
            
            logger.info(f"ğŸš‚ è®­ç»ƒCatBoostï¼ˆ{timeframe} +100%æ•°æ®ï¼‰...")
            cat_model = self._train_catboost(X_cat_train, y_cat_train, timeframe, custom_params=cat_params_optimized)
            
            # 2ï¸âƒ£ ç”ŸæˆéªŒè¯é›†çš„é¢„æµ‹æ¦‚ç‡ï¼ˆå…ƒç‰¹å¾ï¼‰
            logger.info(f"ğŸ“Š ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆåŸºäºå¯¹é½çš„éªŒè¯é›†ï¼‰...")
            
            # ä½¿ç”¨å„è‡ªçš„éªŒè¯é›†ç”Ÿæˆé¢„æµ‹
            lgb_pred_proba = lgb_model.predict_proba(X_lgb_val)
            xgb_pred_proba = xgb_model.predict_proba(X_xgb_val)
            cat_pred_proba = cat_model.predict_proba(X_cat_val)
            
            # Informer-2é¢„æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼Œä½¿ç”¨åºåˆ—éªŒè¯æ•°æ®ï¼‰
            if inf_model is not None and X_seq_val is not None:
                inf_pred_proba = inf_model.predict_proba(X_seq_val)
                logger.info(f"   Informer-2æ¦‚ç‡å½¢çŠ¶: {inf_pred_proba.shape}")
            
            logger.info(f"æ¦‚ç‡å½¢çŠ¶: lgb={lgb_pred_proba.shape}, xgb={xgb_pred_proba.shape}, cat={cat_pred_proba.shape}")
            
            # ğŸ”‘ éªŒè¯å½¢çŠ¶ä¸€è‡´æ€§
            assert lgb_pred_proba.shape == xgb_pred_proba.shape == cat_pred_proba.shape, \
                f"æ¦‚ç‡æ•°ç»„å½¢çŠ¶ä¸ä¸€è‡´: {lgb_pred_proba.shape} vs {xgb_pred_proba.shape} vs {cat_pred_proba.shape}"
            
            # è·å–é¢„æµ‹ç±»åˆ«
            lgb_pred_raw = lgb_model.predict(X_lgb_val)
            xgb_pred_raw = xgb_model.predict(X_xgb_val)
            cat_pred_raw = cat_model.predict(X_cat_val)
            
            # ğŸ”‘ ç»Ÿä¸€è½¬æ¢ä¸º1Dæ•°ç»„ï¼ˆCatBoostè¿”å›2Dï¼Œéœ€è¦ravelï¼‰
            lgb_pred = lgb_pred_raw.ravel()
            xgb_pred = xgb_pred_raw.ravel()
            cat_pred = cat_pred_raw.ravel()
            
            # ğŸ”‘ ä¸¥æ ¼éªŒè¯é¢„æµ‹æ•°ç»„å½¢çŠ¶
            expected_shape = (len(y_lgb_val),)
            assert lgb_pred.shape == expected_shape, f"lgb_predå½¢çŠ¶é”™è¯¯: {lgb_pred.shape} != {expected_shape}"
            assert xgb_pred.shape == expected_shape, f"xgb_predå½¢çŠ¶é”™è¯¯: {xgb_pred.shape} != {expected_shape}"
            assert cat_pred.shape == expected_shape, f"cat_predå½¢çŠ¶é”™è¯¯: {cat_pred.shape} != {expected_shape}"
            
            logger.info(f"é¢„æµ‹ç±»åˆ«å½¢çŠ¶éªŒè¯é€šè¿‡: {lgb_pred.shape} (å·²ç»Ÿä¸€ä¸º1Dæ•°ç»„)")
            
            # ğŸ†• å¢å¼ºå…ƒç‰¹å¾ï¼ˆæå‡å…ƒå­¦ä¹ å™¨å†³ç­–èƒ½åŠ›ï¼‰
            logger.info(f"ç”Ÿæˆå¢å¼ºå…ƒç‰¹å¾...")
            
            # 1. æ¨¡å‹ä¸€è‡´æ€§ï¼ˆ3ä¸ªæ¨¡å‹é¢„æµ‹æ˜¯å¦ä¸€è‡´ï¼‰
            # ğŸ”‘ å·²ç¡®è®¤éƒ½æ˜¯1Dæ•°ç»„ï¼Œç›´æ¥æ¯”è¾ƒ
            agreement_bool = (lgb_pred == xgb_pred) & (xgb_pred == cat_pred)  # (6757,) boolean
            agreement = agreement_bool.astype(float).reshape(-1, 1)  # (6757, 1)
            
            # éªŒè¯ç»´åº¦
            assert agreement.shape == (len(y_lgb_val), 1), f"agreementå½¢çŠ¶é”™è¯¯: {agreement.shape}"
            logger.debug(f"âœ“ agreement: {agreement.shape}")
            
            # 2. æœ€å¤§æ¦‚ç‡ï¼ˆæ¯ä¸ªæ¨¡å‹çš„æœ€é«˜ç½®ä¿¡åº¦ï¼‰
            lgb_max_prob = lgb_pred_proba.max(axis=1).reshape(-1, 1)
            xgb_max_prob = xgb_pred_proba.max(axis=1).reshape(-1, 1)
            cat_max_prob = cat_pred_proba.max(axis=1).reshape(-1, 1)
            assert lgb_max_prob.shape == (len(y_lgb_val), 1), f"lgb_max_probå½¢çŠ¶é”™è¯¯: {lgb_max_prob.shape}"
            logger.debug(f"âœ“ max_prob: {lgb_max_prob.shape}")
            
            # 3. æ¦‚ç‡ç†µï¼ˆä¸ç¡®å®šæ€§ï¼Œç†µè¶Šé«˜è¶Šä¸ç¡®å®šï¼‰
            lgb_entropy = entr(lgb_pred_proba).sum(axis=1).reshape(-1, 1)
            xgb_entropy = entr(xgb_pred_proba).sum(axis=1).reshape(-1, 1)
            cat_entropy = entr(cat_pred_proba).sum(axis=1).reshape(-1, 1)
            assert lgb_entropy.shape == (len(y_lgb_val), 1), f"lgb_entropyå½¢çŠ¶é”™è¯¯: {lgb_entropy.shape}"
            logger.debug(f"âœ“ entropy: {lgb_entropy.shape}")
            
            # Informer-2çš„å¢å¼ºç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if inf_model is not None:
                inf_max_prob = inf_pred_proba.max(axis=1).reshape(-1, 1)
                inf_entropy = entr(inf_pred_proba).sum(axis=1).reshape(-1, 1)
                logger.debug(f"âœ“ inf_max_prob: {inf_max_prob.shape}, inf_entropy: {inf_entropy.shape}")
            
            # 4. å¹³å‡æ¦‚ç‡ï¼ˆä¸‰ä¸ªæˆ–å››ä¸ªæ¨¡å‹çš„å¹³å‡é¢„æµ‹æ¦‚ç‡ï¼‰
            if inf_model is not None:
                avg_proba = (lgb_pred_proba + xgb_pred_proba + cat_pred_proba + inf_pred_proba) / 4
            else:
                avg_proba = (lgb_pred_proba + xgb_pred_proba + cat_pred_proba) / 3
            assert avg_proba.shape == lgb_pred_proba.shape, f"avg_probaå½¢çŠ¶é”™è¯¯: {avg_proba.shape}"
            logger.debug(f"âœ“ avg_proba: {avg_proba.shape}")
            
            # 5. æ¦‚ç‡æ ‡å‡†å·®ï¼ˆæ¨¡å‹é—´çš„é¢„æµ‹å·®å¼‚ï¼‰
            if inf_model is not None:
                prob_std = np.std(np.stack([lgb_pred_proba, xgb_pred_proba, cat_pred_proba, inf_pred_proba]), axis=0)
            else:
                prob_std = np.std(np.stack([lgb_pred_proba, xgb_pred_proba, cat_pred_proba]), axis=0)
            prob_std_max = prob_std.max(axis=1).reshape(-1, 1)
            assert prob_std_max.shape == (len(y_lgb_val), 1), f"prob_std_maxå½¢çŠ¶é”™è¯¯: {prob_std_max.shape}"
            logger.debug(f"âœ“ prob_std_max: {prob_std_max.shape}")
            
            # ğŸ”‘ æ‹¼æ¥æ‰€æœ‰å…ƒç‰¹å¾ï¼ˆä¸¥æ ¼éªŒè¯æ¯ä¸€æ­¥ï¼‰
            logger.info(f"å¼€å§‹æ‹¼æ¥å…ƒç‰¹å¾...")
            
            # é€æ­¥æ‹¼æ¥å¹¶éªŒè¯
            if inf_model is not None:
                # åŒ…å«Informer-2ï¼ˆ25ä¸ªç‰¹å¾ï¼‰
                meta_list = [
                    lgb_pred_proba,      # (N, 3)
                    xgb_pred_proba,      # (N, 3)
                    cat_pred_proba,      # (N, 3)
                    inf_pred_proba,      # (N, 3) â† æ–°å¢
                    agreement,           # (N, 1)
                    lgb_max_prob,        # (N, 1)
                    xgb_max_prob,        # (N, 1)
                    cat_max_prob,        # (N, 1)
                    inf_max_prob,        # (N, 1) â† æ–°å¢
                    lgb_entropy,         # (N, 1)
                    xgb_entropy,         # (N, 1)
                    cat_entropy,         # (N, 1)
                    inf_entropy,         # (N, 1) â† æ–°å¢
                    avg_proba,           # (N, 3)
                    prob_std_max         # (N, 1)
                ]
                expected_features = 25  # 3+3+3+3+1+1+1+1+1+1+1+1+1+3+1 = 25
            else:
                # ä»…ä¼ ç»Ÿæ¨¡å‹ï¼ˆ20ä¸ªç‰¹å¾ï¼‰
                meta_list = [
                    lgb_pred_proba,      # (N, 3)
                    xgb_pred_proba,      # (N, 3)
                    cat_pred_proba,      # (N, 3)
                    agreement,           # (N, 1)
                    lgb_max_prob,        # (N, 1)
                    xgb_max_prob,        # (N, 1)
                    cat_max_prob,        # (N, 1)
                    lgb_entropy,         # (N, 1)
                    xgb_entropy,         # (N, 1)
                    cat_entropy,         # (N, 1)
                    avg_proba,           # (N, 3)
                    prob_std_max         # (N, 1)
                ]
                expected_features = 20  # 3+3+3+1+1+1+1+1+1+1+3+1
            
            # éªŒè¯æ‰€æœ‰æ•°ç»„çš„ç¬¬0ç»´åº¦éƒ½ç›¸åŒ
            expected_rows = len(y_lgb_val)
            for i, arr in enumerate(meta_list):
                assert arr.shape[0] == expected_rows, \
                    f"å…ƒç‰¹å¾{i}ç¬¬0ç»´åº¦é”™è¯¯: {arr.shape[0]} != {expected_rows}, å®Œæ•´å½¢çŠ¶: {arr.shape}"
            
            # æ‹¼æ¥
            meta_features_val = np.hstack(meta_list)
            
            # æœ€ç»ˆéªŒè¯
            assert meta_features_val.shape == (expected_rows, expected_features), \
                f"å…ƒç‰¹å¾æœ€ç»ˆå½¢çŠ¶é”™è¯¯: {meta_features_val.shape} != ({expected_rows}, {expected_features})"
            
            # å…ƒæ ‡ç­¾ï¼ˆä½¿ç”¨LightGBMçš„y_valï¼Œå› ä¸ºéªŒè¯é›†å·²å¯¹é½ï¼‰
            meta_labels_val = y_lgb_val
            
            if inf_model is not None:
                logger.info(f"âœ… å¢å¼ºå…ƒç‰¹å¾ç”Ÿæˆå®Œæˆ: {meta_features_val.shape} (åŸºç¡€12+å¢å¼º13=25ä¸ªï¼Œå«Informer-2)")
            else:
                logger.info(f"âœ… å¢å¼ºå…ƒç‰¹å¾ç”Ÿæˆå®Œæˆ: {meta_features_val.shape} (åŸºç¡€9+å¢å¼º11=20ä¸ª)")
            
            # 3ï¸âƒ£ è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰ - å‡çº§ä¸ºLightGBM + åŠ¨æ€HOLDæƒ©ç½š
            logger.info(f"ğŸ§  è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆLightGBM - æ›´å¼ºå¤§çš„å†³ç­–èƒ½åŠ›ï¼‰...")
            
            # ğŸ”‘ æ£€æŸ¥HOLDæ¯”ä¾‹ï¼ŒåŠ¨æ€è°ƒæ•´æƒ©ç½šç³»æ•°
            hold_ratio = (meta_labels_val == 1).sum() / len(meta_labels_val)
            
            # ğŸ”‘ æ ¹æ®HOLDæ¯”ä¾‹åŠ¨æ€è°ƒæ•´æƒ©ç½šï¼ˆå¹³è¡¡ç­–ç•¥ï¼‰
            if hold_ratio > 0.60:  # HOLDå æ¯”>60%ï¼Œé‡æƒ©ç½š
                meta_hold_penalty_weight = 0.45
            elif hold_ratio > 0.50:  # HOLDå æ¯”>50%ï¼Œä¸­ç­‰
                meta_hold_penalty_weight = 0.55
            elif hold_ratio > 0.40:  # HOLDå æ¯”>40%ï¼Œè½»åº¦
                meta_hold_penalty_weight = 0.65
            else:  # HOLDå æ¯”<=40%ï¼Œæ­£å¸¸
                meta_hold_penalty_weight = 0.75
            
            logger.info(f"   HOLDå æ¯”: {hold_ratio*100:.1f}% â†’ æƒ©ç½šç³»æ•°: {meta_hold_penalty_weight}")
            
            meta_class_weights = compute_sample_weight('balanced', meta_labels_val)
            # âœ… æ·»åŠ æ—¶é—´è¡°å‡æƒé‡ï¼ˆä¸åŸºç¡€æ¨¡å‹ä¿æŒä¸€è‡´ï¼‰
            meta_time_decay = np.exp(-np.arange(len(meta_features_val)) / (len(meta_features_val) * 0.1))[::-1]
            meta_hold_penalty = np.where(meta_labels_val == 1, meta_hold_penalty_weight, 1.0)
            meta_sample_weights = meta_class_weights * meta_time_decay * meta_hold_penalty
            
            # ğŸ”‘ å…ƒå­¦ä¹ å™¨ï¼šä¸“ä¸šé…ç½®å¹³è¡¡æ€§èƒ½å’Œé˜²è¿‡æ‹Ÿåˆ
            meta_learner = lgb.LGBMClassifier(
                n_estimators=150,    # âœ… é€‚å½“å¢åŠ æ ‘æ•°é‡ 50â†’150
                max_depth=6,         # âœ… ä¸­ç­‰æ·±åº¦å¹³è¡¡è¡¨è¾¾èƒ½åŠ› 3â†’6
                learning_rate=0.05,  # âœ… é™ä½å­¦ä¹ ç‡æ›´ç¨³å®šæ”¶æ•› 0.15â†’0.05
                num_leaves=31,       # âœ… 2^5-1æ ‡å‡†é…ç½® 7â†’31
                min_child_samples=20,  # âœ… é€‚åº¦æœ€å°æ ·æœ¬ 30â†’20
                subsample=0.8,       # âœ… è¡Œé‡‡æ ·é˜²è¿‡æ‹Ÿåˆ 0.7â†’0.8
                colsample_bytree=0.8,  # âœ… åˆ—é‡‡æ ·é˜²è¿‡æ‹Ÿåˆ 0.7â†’0.8
                reg_alpha=0.1,       # âœ… é€‚åº¦L1æ­£åˆ™åŒ– 0.3â†’0.1
                reg_lambda=0.1,      # âœ… é€‚åº¦L2æ­£åˆ™åŒ– 0.3â†’0.1
                random_state=42,
                verbose=-1
            )
            meta_learner.fit(meta_features_val, meta_labels_val, sample_weight=meta_sample_weights)
            
            logger.info(f"âœ… å…ƒå­¦ä¹ å™¨è®­ç»ƒå®Œæˆï¼ˆåŠ¨æ€HOLDæƒ©ç½š={meta_hold_penalty_weight}ï¼‰")
            
            # 4ï¸âƒ£ æ„é€ å…¨æ–°çš„æ¨¡å‹å­—å…¸ï¼ˆé¿å…è®­ç»ƒæœŸé—´è¯»åˆ°åŠæ›´æ–°çŠ¶æ€ï¼‰
            models: Dict[str, Any] = {
                'lgb': lgb_model,
                'xgb': xgb_model,
                'cat': cat_model,
                'meta': meta_learner
            }

            # ä¿å­˜Informer-2æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if inf_model is not None:
                models['inf'] = inf_model
            
            # 5ï¸âƒ£ è¯„ä¼°é›†æˆæ¨¡å‹ - ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            logger.info(f"ğŸ“Š {timeframe} æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¯„ä¼°...")
            
            # ğŸ†• æ—¶é—´åºåˆ—5æŠ˜äº¤å‰éªŒè¯ï¼ˆæ›´å¯é çš„è¯„ä¼°ï¼‰
            tscv = TimeSeriesSplit(n_splits=5)
            cv_scores = []
            
            # å¯¹éªŒè¯é›†è¿›è¡Œäº¤å‰éªŒè¯
            for fold, (train_idx, test_idx) in enumerate(tscv.split(meta_features_val), 1):
                meta_train, meta_test = meta_features_val[train_idx], meta_features_val[test_idx]
                y_train, y_test = meta_labels_val.iloc[train_idx], meta_labels_val.iloc[test_idx]
                
                # è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆæ¯ä¸ªfoldï¼‰- ä¸æœ€ç»ˆæ¨¡å‹å®Œå…¨ä¸€è‡´çš„é…ç½®
                fold_meta = lgb.LGBMClassifier(
                    n_estimators=50, max_depth=3, learning_rate=0.15,
                    num_leaves=7, min_child_samples=30, subsample=0.7,
                    colsample_bytree=0.7, reg_alpha=0.3, reg_lambda=0.3,
                    random_state=42, verbose=-1
                )
                
                # ğŸ”‘ HOLDæƒ©ç½šï¼ˆä¸æœ€ç»ˆæ¨¡å‹ä¸€è‡´ï¼Œä½¿ç”¨ç›¸åŒçš„åŠ¨æ€ç­–ç•¥ï¼‰
                fold_weights = compute_sample_weight('balanced', y_train)
                fold_hold_ratio = (y_train == 1).sum() / len(y_train)
                
                # åŠ¨æ€æƒ©ç½šï¼ˆå¹³è¡¡ç­–ç•¥ï¼Œä¸æœ€ç»ˆæ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
                if fold_hold_ratio > 0.60:
                    fold_penalty = 0.45
                elif fold_hold_ratio > 0.50:
                    fold_penalty = 0.55
                elif fold_hold_ratio > 0.40:
                    fold_penalty = 0.65
                else:
                    fold_penalty = 0.75
                
                fold_hold_penalty = np.where(y_train == 1, fold_penalty, 1.0)
                fold_sample_weights = fold_weights * fold_hold_penalty
                
                fold_meta.fit(meta_train, y_train, sample_weight=fold_sample_weights)
                fold_pred = fold_meta.predict(meta_test)
                fold_acc = accuracy_score(y_test, fold_pred)
                cv_scores.append(fold_acc)
                
                logger.debug(f"  Fold {fold}: å‡†ç¡®ç‡={fold_acc:.4f}")
            
            # äº¤å‰éªŒè¯å‡†ç¡®ç‡
            cv_mean = np.mean(cv_scores)
            cv_std = np.std(cv_scores)
            
            logger.info(f"âœ… {timeframe} æ—¶é—´åºåˆ—CVç»“æœ: {cv_mean:.4f} Â± {cv_std:.4f}")
            logger.info(f"   CVåˆ†æ•°: {[f'{s:.4f}' for s in cv_scores]}")
            
            # ä½¿ç”¨å®Œæ•´éªŒè¯é›†è¯„ä¼°æœ€ç»ˆæ¨¡å‹
            ensemble_pred = meta_learner.predict(meta_features_val)
            ensemble_proba = meta_learner.predict_proba(meta_features_val)
            accuracy = accuracy_score(meta_labels_val, ensemble_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(
                meta_labels_val, ensemble_pred, average='weighted', zero_division=0
            )
            
            # ğŸ†• ç±»åˆ«çº§åˆ«è¯¦ç»†æŒ‡æ ‡
            class_report = classification_report(
                meta_labels_val, ensemble_pred, 
                target_names=['SHORT', 'HOLD', 'LONG'], 
                output_dict=True,
                zero_division=0
            )
            
            # ğŸ†• æ··æ·†çŸ©é˜µå’Œè‡´å‘½é”™è¯¯åˆ†æ
            cm = confusion_matrix(meta_labels_val, ensemble_pred)
            
            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ··æ·†çŸ©é˜µè‡³å°‘æ˜¯3x3
            if cm.shape[0] >= 3 and cm.shape[1] >= 3:
                fatal_errors = int(cm[0, 2] + cm[2, 0])  # SHORTâ†’LONG + LONGâ†’SHORT
                fatal_error_rate = fatal_errors / len(meta_labels_val) if len(meta_labels_val) > 0 else 0.0
                long_to_short = int(cm[2, 0])  # LONGâ†’SHORT
                short_to_long = int(cm[0, 2])  # SHORTâ†’LONG
            else:
                logger.warning(f"âš ï¸ æ··æ·†çŸ©é˜µç»´åº¦å¼‚å¸¸: {cm.shape}ï¼Œè·³è¿‡è‡´å‘½é”™è¯¯åˆ†æ")
                fatal_errors = 0
                fatal_error_rate = 0.0
                long_to_short = 0
                short_to_long = 0
            
            # ğŸ†• ä¿¡å·è´¨é‡åˆ†æ
            signal_mask = ensemble_pred != 1  # éHOLDé¢„æµ‹
            signal_count = int(np.sum(signal_mask))
            signal_frequency = float(np.mean(signal_mask))
            hold_ratio = 1.0 - signal_frequency
            
            # åªåœ¨æœ‰ä¿¡å·æ—¶è®¡ç®—ä¿¡å·å‡†ç¡®ç‡
            if signal_count > 0:
                signal_labels = meta_labels_val[signal_mask]
                signal_preds = ensemble_pred[signal_mask]
                # ä¿¡å·å‡†ç¡®ç‡ï¼šåªçœ‹LONG/SHORTçš„é¢„æµ‹å‡†ç¡®ç‡
                signal_accuracy = float(accuracy_score(signal_labels, signal_preds))
            else:
                signal_accuracy = 0.0
            
            # ğŸ†• æ¦‚ç‡æ ¡å‡†æŒ‡æ ‡
            try:
                log_loss_score = float(log_loss(meta_labels_val, ensemble_proba))
            except Exception as e:
                logger.warning(f"âš ï¸ Log Lossè®¡ç®—å¤±è´¥: {e}")
                log_loss_score = 0.0
            
            try:
                confidence_avg = float(np.mean(np.max(ensemble_proba, axis=1)))
            except Exception as e:
                logger.warning(f"âš ï¸ ç½®ä¿¡åº¦è®¡ç®—å¤±è´¥: {e}")
                confidence_avg = 0.0
            
            # ğŸ†• æ¨¡å‹ç¨³å®šæ€§æŒ‡æ ‡
            cv_stability = float(cv_std / cv_mean if cv_mean > 0 else 0)  # å˜å¼‚ç³»æ•°
            cv_min = float(np.min(cv_scores))
            cv_max = float(np.max(cv_scores))
            
            # åŸºç¡€æ¨¡å‹ä¸€è‡´æ€§
            model_agreement = float(np.mean([
                (lgb_pred == xgb_pred).mean(),
                (lgb_pred == cat_pred).mean(),
                (xgb_pred == cat_pred).mean()
            ]))
            
            # ğŸ†• äº¤æ˜“ç»æµæ€§æŒ‡æ ‡
            trade_efficiency = float(signal_accuracy / signal_frequency if signal_frequency > 0 else 0)
            fee_impact = float(signal_frequency * 0.0007 * 100)  # é¢„ä¼°æ—¥æ‰‹ç»­è´¹æŸè€—%
            required_winrate = float(0.5 + (0.0007 / 0.02))  # ç›ˆäºæ¯”1:1æ—¶çš„ç›ˆäºå¹³è¡¡èƒœç‡
            
            # ğŸ†• é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ
            try:
                confidence_values = np.max(ensemble_proba, axis=1)
                confidence_quantiles = np.quantile(confidence_values, [0.25, 0.5, 0.75, 0.9])
                confidence_q25 = float(confidence_quantiles[0])
                confidence_median = float(confidence_quantiles[1])
                confidence_q75 = float(confidence_quantiles[2])
                confidence_q90 = float(confidence_quantiles[3])
            except Exception as e:
                logger.warning(f"âš ï¸ ç½®ä¿¡åº¦åˆ†ä½æ•°è®¡ç®—å¤±è´¥: {e}")
                confidence_q25 = 0.0
                confidence_median = 0.0
                confidence_q75 = 0.0
                confidence_q90 = 0.0
            
            # é«˜ç½®ä¿¡åº¦é¢„æµ‹çš„å‡†ç¡®ç‡
            try:
                high_confidence_mask = confidence_values > 0.7
                if np.sum(high_confidence_mask) > 0:
                    high_confidence_accuracy = float(accuracy_score(
                        meta_labels_val[high_confidence_mask],
                        ensemble_pred[high_confidence_mask]
                    ))
                    high_confidence_ratio = float(np.mean(high_confidence_mask))
                else:
                    high_confidence_accuracy = 0.0
                    high_confidence_ratio = 0.0
            except Exception as e:
                logger.warning(f"âš ï¸ é«˜ç½®ä¿¡åº¦æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
                high_confidence_accuracy = 0.0
                high_confidence_ratio = 0.0
            
            # ğŸ†• ç±»åˆ«å¹³è¡¡æ€§æŒ‡æ ‡
            try:
                pred_distribution = np.bincount(ensemble_pred, minlength=3) / len(ensemble_pred)
                prediction_entropy = float(scipy_entropy(pred_distribution))  # ç†µè¶Šé«˜è¶Šå¹³è¡¡
                prediction_balance_score = float(1 - np.std(pred_distribution))  # å¹³è¡¡åˆ†æ•°
                short_ratio = float(pred_distribution[0])
                long_ratio = float(pred_distribution[2])
            except Exception as e:
                logger.warning(f"âš ï¸ ç±»åˆ«å¹³è¡¡æ€§æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
                prediction_entropy = 0.0
                prediction_balance_score = 0.0
                short_ratio = 0.0
                long_ratio = 0.0
            
            # ğŸ†• é”™è¯¯ä¸¥é‡æ€§åŠ æƒæŒ‡æ ‡
            try:
                fatal_weight = 3.0
                total_errors = len(meta_labels_val) - np.sum(ensemble_pred == meta_labels_val)
                normal_errors = max(0, total_errors - fatal_errors)  # ç¡®ä¿éè´Ÿ
                if len(meta_labels_val) > 0:
                    weighted_error_rate = float((fatal_errors * fatal_weight + normal_errors) / (len(meta_labels_val) * fatal_weight))
                else:
                    weighted_error_rate = 0.0
                fatal_error_ratio_in_errors = float(fatal_errors / total_errors if total_errors > 0 else 0)
            except Exception as e:
                logger.warning(f"âš ï¸ é”™è¯¯ä¸¥é‡æ€§æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
                weighted_error_rate = 0.0
                fatal_error_ratio_in_errors = 0.0
            
            logger.info(f"ğŸ“Š {timeframe} æœ€ç»ˆæ¨¡å‹éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f} (CV: {cv_mean:.4f}Â±{cv_std:.4f})")
            
            # 6ï¸âƒ£ è¯„ä¼°å„åŸºç¡€æ¨¡å‹
            lgb_pred = lgb_model.predict(X_lgb_val)
            xgb_pred = xgb_model.predict(X_xgb_val)
            cat_pred = cat_model.predict(X_cat_val)
            
            lgb_acc = accuracy_score(y_lgb_val, lgb_pred)
            xgb_acc = accuracy_score(y_xgb_val, xgb_pred)
            cat_acc = accuracy_score(y_cat_val, cat_pred)
            
            # Informer-2å‡†ç¡®ç‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if inf_model is not None and X_seq_val is not None:
                # ğŸ”‘ ä¿®å¤ï¼šä½¿ç”¨åºåˆ—éªŒè¯æ•°æ®è€Œä¸æ˜¯2Dæ•°æ®
                inf_pred = inf_model.predict(X_seq_val)
                inf_acc = accuracy_score(y_seq_val, inf_pred)
            else:
                inf_acc = 0.0
            
            training_time = time.time() - start_time
            
            result = {
                # åŸºç¡€æŒ‡æ ‡
                'accuracy': cv_mean,  # ğŸ”‘ ä½¿ç”¨CVå‡å€¼ä½œä¸ºä¸»å‡†ç¡®ç‡ï¼ˆæ›´å¯é ï¼‰
                'cv_mean': cv_mean,   # äº¤å‰éªŒè¯å‡å€¼
                'cv_std': cv_std,     # äº¤å‰éªŒè¯æ ‡å‡†å·®
                'cv_scores': cv_scores,  # å„æŠ˜åˆ†æ•°
                'val_accuracy': accuracy,  # éªŒè¯é›†å‡†ç¡®ç‡
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                
                # åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡
                'lgb_accuracy': lgb_acc,
                'xgb_accuracy': xgb_acc,
                'cat_accuracy': cat_acc,
                'inf_accuracy': inf_acc if inf_model else 0.0,
                
                # ğŸ†• ç±»åˆ«çº§åˆ«æŒ‡æ ‡
                'class_metrics': {
                    'SHORT': class_report.get('SHORT', {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0}),
                    'HOLD': class_report.get('HOLD', {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0}),
                    'LONG': class_report.get('LONG', {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0})
                },
                
                # ğŸ†• æ··æ·†çŸ©é˜µå’Œè‡´å‘½é”™è¯¯
                'confusion_matrix': cm.tolist(),
                'fatal_errors': fatal_errors,
                'fatal_error_rate': fatal_error_rate,
                'long_to_short_errors': long_to_short,
                'short_to_long_errors': short_to_long,
                
                # ğŸ†• ä¿¡å·è´¨é‡åˆ†æ
                'signal_frequency': signal_frequency,
                'signal_accuracy': signal_accuracy,
                'signal_count': signal_count,
                'hold_ratio': hold_ratio,
                
                # ğŸ†• æ¦‚ç‡æ ¡å‡†æŒ‡æ ‡
                'log_loss': log_loss_score,
                'confidence_avg': confidence_avg,
                'confidence_q25': confidence_q25,
                'confidence_median': confidence_median,
                'confidence_q75': confidence_q75,
                'confidence_q90': confidence_q90,
                
                # ğŸ†• é«˜ç½®ä¿¡åº¦æŒ‡æ ‡
                'high_confidence_accuracy': high_confidence_accuracy,
                'high_confidence_ratio': high_confidence_ratio,
                
                # ğŸ†• æ¨¡å‹ç¨³å®šæ€§æŒ‡æ ‡
                'cv_stability': cv_stability,
                'cv_min': cv_min,
                'cv_max': cv_max,
                'model_agreement': model_agreement,
                
                # ğŸ†• äº¤æ˜“ç»æµæ€§æŒ‡æ ‡
                'trade_efficiency': trade_efficiency,
                'fee_impact': fee_impact,
                'required_winrate': required_winrate,
                
                # ğŸ†• ç±»åˆ«å¹³è¡¡æ€§æŒ‡æ ‡
                'prediction_entropy': prediction_entropy,
                'prediction_balance_score': prediction_balance_score,
                'short_ratio': short_ratio,
                'long_ratio': long_ratio,
                
                # ğŸ†• é”™è¯¯ä¸¥é‡æ€§åŠ æƒæŒ‡æ ‡
                'weighted_error_rate': weighted_error_rate,
                'fatal_error_ratio_in_errors': fatal_error_ratio_in_errors,
                
                # å…¶ä»–ä¿¡æ¯
                'training_time': training_time,
                'ensemble_size': len(models),
                'meta_features_count': meta_features_val.shape[1]  # å…ƒç‰¹å¾æ•°é‡
            }
            
            logger.info(f"âœ… Stackingè®­ç»ƒå®Œæˆï¼ˆå·®å¼‚åŒ–æ•°æ®ï¼‰:")
            logger.info(f"")
            logger.info(f"  ğŸ“Š åŸºç¡€æ¨¡å‹è¡¨ç°:")
            logger.info(f"     LightGBM(360å¤©): {lgb_acc:.4f}")
            logger.info(f"     XGBoost(540å¤©):  {xgb_acc:.4f}")
            logger.info(f"     CatBoost(720å¤©): {cat_acc:.4f}")
            if inf_model:
                logger.info(f"     Informer-2:      {inf_acc:.4f} ğŸ¤–")
            logger.info(f"")
            logger.info(f"  ğŸ¯ é›†æˆæ¨¡å‹è¡¨ç°:")
            logger.info(f"     éªŒè¯é›†å‡†ç¡®ç‡:   {accuracy:.4f}")
            logger.info(f"     æ—¶é—´åºåˆ—CV:     {cv_mean:.4f} Â± {cv_std:.4f} (5-fold)")
            logger.info(f"     Precision:      {precision:.4f}")
            logger.info(f"     Recall:         {recall:.4f}")
            logger.info(f"     F1-Score:       {f1:.4f}")
            logger.info(f"")
            logger.info(f"  ğŸ“ˆ ç±»åˆ«çº§åˆ«è¡¨ç°:")
            short_metrics = class_report.get('SHORT', {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0})
            hold_metrics = class_report.get('HOLD', {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0})
            long_metrics = class_report.get('LONG', {'precision': 0, 'recall': 0, 'f1-score': 0, 'support': 0})
            logger.info(f"     SHORT - P:{short_metrics['precision']:.4f} R:{short_metrics['recall']:.4f} F1:{short_metrics['f1-score']:.4f} (æ ·æœ¬:{int(short_metrics['support'])})")
            logger.info(f"     HOLD  - P:{hold_metrics['precision']:.4f} R:{hold_metrics['recall']:.4f} F1:{hold_metrics['f1-score']:.4f} (æ ·æœ¬:{int(hold_metrics['support'])})")
            logger.info(f"     LONG  - P:{long_metrics['precision']:.4f} R:{long_metrics['recall']:.4f} F1:{long_metrics['f1-score']:.4f} (æ ·æœ¬:{int(long_metrics['support'])})")
            logger.info(f"")
            logger.info(f"  ğŸ² ä¿¡å·è´¨é‡åˆ†æ:")
            logger.info(f"     ä¿¡å·é¢‘ç‡:       {signal_frequency*100:.2f}% ({signal_count}ä¸ªä¿¡å·)")
            logger.info(f"     ä¿¡å·å‡†ç¡®ç‡:     {signal_accuracy:.4f}")
            logger.info(f"     HOLDæ¯”ä¾‹:       {hold_ratio*100:.2f}%")
            logger.info(f"     å¹³å‡ç½®ä¿¡åº¦:     {confidence_avg:.4f}")
            logger.info(f"")
            logger.info(f"  âš ï¸ é”™è¯¯åˆ†æ:")
            logger.info(f"     è‡´å‘½é”™è¯¯:       {fatal_errors}æ¬¡ ({fatal_error_rate*100:.2f}%)")
            logger.info(f"     LONGâ†’SHORT:     {long_to_short}æ¬¡")
            logger.info(f"     SHORTâ†’LONG:     {short_to_long}æ¬¡")
            logger.info(f"     åŠ æƒé”™è¯¯ç‡:     {weighted_error_rate:.4f} (è‡´å‘½Ã—3æƒé‡)")
            logger.info(f"     è‡´å‘½é”™è¯¯å æ¯”:   {fatal_error_ratio_in_errors*100:.2f}% (åœ¨æ€»é”™è¯¯ä¸­)")
            logger.info(f"     Log Loss:       {log_loss_score:.4f}")
            logger.info(f"")
            logger.info(f"  ğŸ¯ é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ:")
            logger.info(f"     å¹³å‡å€¼:         {confidence_avg:.4f}")
            logger.info(f"     ä¸­ä½æ•°:         {confidence_median:.4f}")
            logger.info(f"     Q25-Q75:        {confidence_q25:.4f} - {confidence_q75:.4f}")
            logger.info(f"     Q90:            {confidence_q90:.4f}")
            logger.info(f"     é«˜ç½®ä¿¡(>0.7):   {high_confidence_ratio*100:.2f}% (å‡†ç¡®ç‡:{high_confidence_accuracy:.4f})")
            logger.info(f"")
            logger.info(f"  ğŸ“Š ç±»åˆ«é¢„æµ‹åˆ†å¸ƒ:")
            logger.info(f"     SHORTæ¯”ä¾‹:      {short_ratio*100:.2f}%")
            logger.info(f"     HOLDæ¯”ä¾‹:       {hold_ratio*100:.2f}%")
            logger.info(f"     LONGæ¯”ä¾‹:       {long_ratio*100:.2f}%")
            logger.info(f"     é¢„æµ‹ç†µ:         {prediction_entropy:.4f} (è¶Šé«˜è¶Šå¹³è¡¡)")
            logger.info(f"     å¹³è¡¡åˆ†æ•°:       {prediction_balance_score:.4f}")
            logger.info(f"")
            logger.info(f"  ğŸ’° äº¤æ˜“ç»æµæ€§åˆ†æ:")
            logger.info(f"     äº¤æ˜“æ•ˆç‡:       {trade_efficiency:.4f} (å‡†ç¡®ç‡/é¢‘ç‡)")
            logger.info(f"     æ‰‹ç»­è´¹å½±å“:     {fee_impact:.4f}% (æ—¥é¢„ä¼°)")
            logger.info(f"     ç›ˆäºå¹³è¡¡èƒœç‡:   {required_winrate*100:.2f}% (ç›ˆäºæ¯”1:1)")
            logger.info(f"")
            logger.info(f"  ğŸ”§ æ¨¡å‹ç¨³å®šæ€§:")
            logger.info(f"     CVå˜å¼‚ç³»æ•°:     {cv_stability:.4f} (è¶Šå°è¶Šç¨³å®š)")
            logger.info(f"     CVèŒƒå›´:         {cv_min:.4f} - {cv_max:.4f}")
            logger.info(f"     æ¨¡å‹ä¸€è‡´æ€§:     {model_agreement*100:.2f}% (åŸºç¡€æ¨¡å‹å…±è¯†)")
            logger.info(f"")
            logger.info(f"  ğŸ“Š æ¨¡å‹é…ç½®:")
            n_base = 12 if inf_model else 9
            n_enhanced = 11
            logger.info(f"     å…ƒç‰¹å¾æ•°é‡:     {meta_features_val.shape[1]}ä¸ªï¼ˆåŸºç¡€{n_base}+å¢å¼º{n_enhanced}ï¼‰")
            logger.info(f"     è®­ç»ƒè€—æ—¶:       {training_time:.2f}ç§’")
            
            # ğŸ”„ ç”Ÿäº§çº§åˆ«ï¼šçƒ­æ›´æ–°æ¨¡å‹ï¼ˆåŸå­æ€§æ›¿æ¢ï¼‰
            logger.info(f"ğŸ”„ {timeframe} è®­ç»ƒå®Œæˆï¼Œå‡†å¤‡çƒ­æ›´æ–°æ¨¡å‹...")
            
            # åŸå­æ€§æ›¿æ¢æ¨¡å‹ï¼ˆç¡®ä¿é¢„æµ‹ä¸ä¼šä½¿ç”¨åŠæ›´æ–°çš„æ¨¡å‹ï¼‰
            old_version = self.model_versions.get(timeframe, 0)
            new_version = old_version + 1
            
            # âœ… åŸå­æ€§æ›¿æ¢ï¼šä¸€æ¬¡æ€§æ›´æ–°æ¨¡å‹å­—å…¸
            self.ensemble_models[timeframe] = models
            
            # æ›´æ–°ç‰ˆæœ¬å’ŒçŠ¶æ€ï¼ˆåŸå­æ“ä½œï¼‰
            self.model_versions[timeframe] = new_version
            self.models_ready[timeframe] = True
            self.training_in_progress[timeframe] = False
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–æ—¶é—´æ¡†æ¶åœ¨è®­ç»ƒ
            self.background_training = any(self.training_in_progress.values())
            
            logger.info(f"âœ… {timeframe} æ¨¡å‹å·²çƒ­æ›´æ–°ï¼ˆv{old_version} â†’ v{new_version}ï¼‰ï¼Œé¢„æµ‹åŠŸèƒ½æ— ç¼è¡”æ¥")
            
            if not self.background_training:
                logger.info(f"âœ… æ‰€æœ‰æ—¶é—´æ¡†æ¶è®­ç»ƒå®Œæˆï¼Œç³»ç»Ÿè¿è¡Œåœ¨æœ€æ–°æ¨¡å‹ç‰ˆæœ¬")
            
            return result
            
        except Exception as e:
            logger.error(f"å·®å¼‚åŒ–Stackingè®­ç»ƒå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            # ğŸ”“ è®­ç»ƒå¤±è´¥æ—¶æ¸…é™¤çŠ¶æ€ï¼ˆä¿æŒæ—§æ¨¡å‹ç»§ç»­è¿è¡Œï¼‰
            self.training_in_progress[timeframe] = False
            self.background_training = any(self.training_in_progress.values())
            
            logger.warning(f"âš ï¸ {timeframe} è®­ç»ƒå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æ—§æ¨¡å‹ï¼ˆé¢„æµ‹åŠŸèƒ½ä¸å—å½±å“ï¼‰")
            
            raise
    
    def _train_lightgbm(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str, custom_params: Optional[Dict[str, Any]] = None):
        """
        è®­ç»ƒLightGBMæ¨¡å‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œç»Ÿä¸€ä¸‰æ¨¡å‹è®­ç»ƒä»£ç ä½ç½®ï¼‰
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            timeframe: æ—¶é—´æ¡†æ¶
            custom_params: è‡ªå®šä¹‰å‚æ•°ï¼ˆOptunaä¼˜åŒ–åçš„å‚æ•°ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
        """
        try:
            # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒå‰æ¸…ç†
            self.clear_gpu_memory()
            
            # æ ·æœ¬åŠ æƒï¼ˆæœ‰æ•ˆæ ·æœ¬æ•° Ã— æ—¶é—´è¡°å‡ Ã— HOLDæƒ©ç½šï¼‰
            class_weights = self._compute_effective_sample_weights(y_train, timeframe)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            
            # ğŸ”‘ HOLDç±»åˆ«é™æƒï¼ˆæŒ‰HOLDå æ¯”è‡ªé€‚åº”ï¼‰
            hold_ratio = float((y_train == 1).sum()) / max(len(y_train), 1)
            if timeframe == '3m':
                hold_weight = float(max(0.35, min(0.70, 0.80 - 0.6 * hold_ratio)))
            else:
                hold_weight = float(max(0.50, min(0.75, 0.85 - 0.5 * hold_ratio)))
            hold_penalty = np.where(y_train == 1, hold_weight, 1.0)
            
            sample_weights = class_weights * time_decay * hold_penalty
            
            logger.info(f"âœ… æ ·æœ¬åŠ æƒå·²å¯ç”¨ï¼šæœ‰æ•ˆæ ·æœ¬æ•° Ã— æ—¶é—´è¡°å‡ Ã— HOLDæƒ©ç½š({hold_weight:.2f})")
            
            # ç¡®å®šæœ€ç»ˆå‚æ•°ï¼ˆä¼˜å…ˆçº§ï¼šcustom_params > timeframe_params > base_paramsï¼‰
            if custom_params:
                params = custom_params
                logger.info(f"ğŸ¯ ä½¿ç”¨Optunaä¼˜åŒ–å‚æ•°")
            else:
                # è·å–æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–å‚æ•°
                timeframe_params = self.lgb_params_by_timeframe.get(timeframe, {})
                # åˆå¹¶åŸºç¡€å‚æ•°å’Œå·®å¼‚åŒ–å‚æ•°
                params = {**self.lgb_params, **timeframe_params}
            
            # ğŸ® å¯ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
            if self.use_gpu:
                params['device'] = 'gpu'
                params['gpu_platform_id'] = 0
                params['gpu_device_id'] = 0
                logger.info(f"ğŸš€ LightGBM GPUåŠ é€Ÿå·²å¯ç”¨")
            
            logger.info(f"ğŸ“Š {timeframe} LightGBMå‚æ•°: num_leaves={params.get('num_leaves')}, "
                       f"reg_alpha={params.get('reg_alpha', 0)}, reg_lambda={params.get('reg_lambda', 0)}")
            
            # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹ï¼ˆparamsä¸­å·²åŒ…å«random_state=42ï¼‰
            model = lgb.LGBMClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights)
            
            # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒåæ¸…ç†
            self.clear_gpu_memory()
            
            return model
            
        except Exception as e:
            logger.error(f"LightGBMè®­ç»ƒå¤±è´¥: {e}")
            # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šå¼‚å¸¸æ—¶æ¸…ç†
            self.clear_gpu_memory()
            raise
    
    def _train_xgboost(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str, custom_params: Optional[Dict[str, Any]] = None):
        """è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆé˜²è¿‡æ‹Ÿåˆ + GPUå†…å­˜ç®¡ç†ï¼‰"""
        try:
            # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒå‰æ¸…ç†
            self.clear_gpu_memory()
            
            # æ ·æœ¬åŠ æƒï¼ˆæœ‰æ•ˆæ ·æœ¬æ•° Ã— æ—¶é—´è¡°å‡ Ã— HOLDæƒ©ç½šï¼‰
            class_weights = self._compute_effective_sample_weights(y_train, timeframe)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            
            # ğŸ”‘ HOLDç±»åˆ«é™æƒï¼ˆæŒ‰HOLDå æ¯”è‡ªé€‚åº”ï¼‰
            hold_ratio = float((y_train == 1).sum()) / max(len(y_train), 1)
            if timeframe == '3m':
                hold_weight = float(max(0.35, min(0.70, 0.80 - 0.6 * hold_ratio)))
            else:
                hold_weight = float(max(0.50, min(0.75, 0.85 - 0.5 * hold_ratio)))
            hold_penalty = np.where(y_train == 1, hold_weight, 1.0)
            
            sample_weights = class_weights * time_decay * hold_penalty
            
            # ğŸ”‘ æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–é…ç½®ï¼ˆé˜²æ­¢2h/4hè¿‡æ‹Ÿåˆï¼‰
            if custom_params:
                # ä½¿ç”¨Optunaä¼˜åŒ–çš„å‚æ•°
                params = custom_params.copy()
                logger.info(f"ğŸ¯ ä½¿ç”¨Optunaä¼˜åŒ–å‚æ•°")
            else:
                # ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆä»…3m/5m/15mï¼‰
                if timeframe == '15m':
                    params = {
                        'max_depth': 6,
                        'learning_rate': 0.05,
                        'n_estimators': 300,
                        'reg_alpha': 0.3,
                        'reg_lambda': 0.3
                    }
                elif timeframe == '5m':
                    params = {
                        'max_depth': 5,
                        'learning_rate': 0.06,
                        'n_estimators': 220,
                        'reg_alpha': 0.5,
                        'reg_lambda': 0.5
                    }
                else:  # 3m
                    params = {
                        'max_depth': 5,
                        'learning_rate': 0.07,
                        'n_estimators': 180,
                        'reg_alpha': 0.6,
                        'reg_lambda': 0.6
                    }
            
            # é€šç”¨å‚æ•°
            params.update({
                'objective': 'multi:softprob',
                'num_class': 3,
                'eval_metric': 'mlogloss',
                'random_state': 42,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            })
            
            # ğŸ® GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_gpu:
                params['tree_method'] = 'hist'  # æ–°ç‰ˆæœ¬ä½¿ç”¨ hist
                params['device'] = 'cuda'  # ä½¿ç”¨ device å‚æ•°æŒ‡å®š GPU
                logger.info(f"ğŸš€ XGBoost GPUåŠ é€Ÿå·²å¯ç”¨")
            else:
                params['tree_method'] = 'hist'
            
            model = xgb.XGBClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
            
            # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒåæ¸…ç†
            self.clear_gpu_memory()
            
            return model
            
        except Exception as e:
            logger.error(f"XGBoostè®­ç»ƒå¤±è´¥: {e}")
            # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šå¼‚å¸¸æ—¶æ¸…ç†
            self.clear_gpu_memory()
            raise
    
    def _train_catboost(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str, custom_params: Optional[Dict[str, Any]] = None):
        """è®­ç»ƒCatBoostæ¨¡å‹ï¼ˆé˜²è¿‡æ‹Ÿåˆ + GPUå†…å­˜ç®¡ç†ï¼‰"""
        try:
            # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒå‰æ¸…ç†
            self.clear_gpu_memory()
            
            # æ ·æœ¬åŠ æƒï¼ˆæœ‰æ•ˆæ ·æœ¬æ•° Ã— æ—¶é—´è¡°å‡ Ã— HOLDæƒ©ç½šï¼‰
            class_weights = self._compute_effective_sample_weights(y_train, timeframe)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            
            # ğŸ”‘ HOLDç±»åˆ«é™æƒï¼ˆæŒ‰HOLDå æ¯”è‡ªé€‚åº”ï¼‰
            hold_ratio = float((y_train == 1).sum()) / max(len(y_train), 1)
            if timeframe == '3m':
                hold_weight = float(max(0.35, min(0.70, 0.80 - 0.6 * hold_ratio)))
            else:
                hold_weight = float(max(0.50, min(0.75, 0.85 - 0.5 * hold_ratio)))
            hold_penalty = np.where(y_train == 1, hold_weight, 1.0)
            
            sample_weights = class_weights * time_decay * hold_penalty
            
            # ğŸ”‘ æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–é…ç½®ï¼ˆé˜²æ­¢2h/4hè¿‡æ‹Ÿåˆï¼‰
            if custom_params:
                # ä½¿ç”¨Optunaä¼˜åŒ–çš„å‚æ•°
                params = custom_params.copy()
                logger.info(f"ğŸ¯ ä½¿ç”¨Optunaä¼˜åŒ–å‚æ•°")
            else:
                # ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆä»…3m/5m/15mï¼‰
                if timeframe == '15m':
                    params = {
                        'iterations': 300,
                        'learning_rate': 0.05,
                        'depth': 6,
                        'l2_leaf_reg': 3.0
                    }
                elif timeframe == '5m':
                    params = {
                        'iterations': 220,
                        'learning_rate': 0.06,
                        'depth': 6,
                        'l2_leaf_reg': 4.0
                    }
                else:  # 3m
                    params = {
                        'iterations': 180,
                        'learning_rate': 0.07,
                        'depth': 6,
                        'l2_leaf_reg': 5.0
                    }
            
            # é€šç”¨å‚æ•°
            params.update({
                'loss_function': 'MultiClass',
                'random_seed': 42,
                'verbose': False,
                'bootstrap_type': 'Bernoulli',
                'subsample': 0.8,
                'allow_writing_files': False
            })
            
            # ğŸ® GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_gpu:
                params['task_type'] = 'GPU'
                params['devices'] = '0'
                logger.info(f"ğŸš€ CatBoost GPUåŠ é€Ÿå·²å¯ç”¨")
            
            model = CatBoostClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
            
            # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒåæ¸…ç†
            self.clear_gpu_memory()
            
            return model
            
        except Exception as e:
            logger.error(f"CatBoostè®­ç»ƒå¤±è´¥: {e}")
            # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šå¼‚å¸¸æ—¶æ¸…ç†
            self.clear_gpu_memory()
            raise
    
    def _train_informer2(self, X_seq_train: np.ndarray, y_seq_train: np.ndarray, timeframe: str, custom_params: Optional[Dict[str, Any]] = None):
        """
        è®­ç»ƒInformer-2æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆä½¿ç”¨GMADLæŸå¤±å‡½æ•° + åºåˆ—è¾“å…¥ï¼‰
        
        Args:
            X_seq_train: åºåˆ—è®­ç»ƒç‰¹å¾ (n_samples, seq_len, n_features)
            y_seq_train: è®­ç»ƒæ ‡ç­¾ (n_samples,)
            timeframe: æ—¶é—´æ¡†æ¶ï¼ˆä»…æ”¯æŒ3m/5m/15mï¼‰
            custom_params: è‡ªå®šä¹‰å‚æ•°ï¼ˆæ¥è‡ªOptunaä¼˜åŒ–ï¼‰
        
        Returns:
            è®­ç»ƒå¥½çš„Informer-2æ¨¡å‹ï¼ˆå…¼å®¹scikit-learnæ¥å£ï¼‰
        """
        if not TORCH_AVAILABLE:
            logger.warning("âš ï¸ PyTorchæœªå®‰è£…ï¼Œè·³è¿‡Informer-2è®­ç»ƒ")
            return None
        
        try:
            start_time = time.time()
            
            # ğŸ® GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒå‰æ¸…ç†
            self.clear_gpu_memory()
            
            logger.info(f"ğŸ¤– è®­ç»ƒInformer-2ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆåºåˆ—è¾“å…¥ï¼‰...")
            logger.info(f"   è¾“å…¥å½¢çŠ¶: {X_seq_train.shape} (æ ·æœ¬æ•°, åºåˆ—é•¿åº¦, ç‰¹å¾æ•°)")
            
            # 1. æ•°æ®å‡†å¤‡ï¼ˆNumPy â†’ PyTorchï¼Œå†…å­˜ä¼˜åŒ–ï¼‰
            # ğŸ”¥ ä¼˜åŒ–ï¼šç¡®ä¿è¾“å…¥ä¸ºfloat32ï¼Œå¹¶ä½¿ç”¨from_numpyé¿å…æ•°æ®å¤åˆ¶
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®æ˜¯è¿ç»­çš„numpyæ•°ç»„ï¼ˆé¿å…å†…å­˜æ˜ å°„é—®é¢˜ï¼‰
            if not isinstance(X_seq_train, np.ndarray):
                X_seq_train = np.asarray(X_seq_train, dtype=np.float32)
            elif X_seq_train.dtype != np.float32:
                logger.debug(f"   è½¬æ¢åºåˆ—æ•°æ®ä¸ºfloat32ï¼ˆåŸç±»å‹: {X_seq_train.dtype}ï¼‰")
                X_seq_train = X_seq_train.astype(np.float32)
            
            if not X_seq_train.flags['C_CONTIGUOUS']:
                logger.debug(f"   è½¬æ¢X_seq_trainä¸ºè¿ç»­æ•°ç»„")
                X_seq_train = np.ascontiguousarray(X_seq_train)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç»Ÿä¸€å¤„ç†y_seq_trainçš„æ•°æ®ç±»å‹
            if not isinstance(y_seq_train, np.ndarray):
                y_seq_train = np.asarray(y_seq_train, dtype=np.int64)
            elif y_seq_train.dtype != np.int64:
                y_seq_train = y_seq_train.astype(np.int64)
            
            if not y_seq_train.flags['C_CONTIGUOUS']:
                logger.debug(f"   è½¬æ¢y_seq_trainä¸ºè¿ç»­æ•°ç»„")
                y_seq_train = np.ascontiguousarray(y_seq_train)
            
            # âœ… å…³é”®ä¿®å¤ï¼šå¯¹3Dåºåˆ—æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼ˆé˜²æ­¢æ•°å€¼æº¢å‡ºï¼‰
            logger.info(f"ğŸ”§ å¯¹åºåˆ—æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼ˆé˜²æ­¢æ•°å€¼æº¢å‡ºï¼‰...")
            logger.info(f"   å½’ä¸€åŒ–å‰ç»Ÿè®¡: èŒƒå›´=[{X_seq_train.min():.4f}, {X_seq_train.max():.4f}], å‡å€¼={X_seq_train.mean():.4f}, æ ‡å‡†å·®={X_seq_train.std():.4f}")
            
            # æ–¹æ³•ï¼šå°†3Dæ•°æ®reshapeä¸º2Dï¼ŒæŒ‰ç‰¹å¾å½’ä¸€åŒ–ï¼Œå†reshapeå›3D
            # è¿™æ ·æ¯ä¸ªç‰¹å¾åœ¨æ‰€æœ‰æ ·æœ¬å’Œæ—¶é—´æ­¥ä¸Šéƒ½è¢«å½’ä¸€åŒ–
            original_shape = X_seq_train.shape
            n_features = original_shape[2]
            
            # Reshapeä¸º2D: (n_samples * seq_len, n_features)
            X_seq_train_2d = X_seq_train.reshape(-1, n_features)
            
            # ä½¿ç”¨StandardScalerå½’ä¸€åŒ–
            scaler = StandardScaler()
            X_seq_train_2d_scaled = scaler.fit_transform(X_seq_train_2d)
            
            # Reshapeå›3D: (n_samples, seq_len, n_features)
            X_seq_train = X_seq_train_2d_scaled.reshape(original_shape).astype(np.float32)
            
            logger.info(f"   âœ… å½’ä¸€åŒ–å®Œæˆ")
            logger.info(f"   å½’ä¸€åŒ–åç»Ÿè®¡: èŒƒå›´=[{X_seq_train.min():.4f}, {X_seq_train.max():.4f}], å‡å€¼={X_seq_train.mean():.4f}, æ ‡å‡†å·®={X_seq_train.std():.4f}")
            
            # ä¿å­˜scalerç”¨äºé¢„æµ‹æ—¶ä½¿ç”¨
            if timeframe not in self.scalers:
                self.scalers[timeframe] = {}
            self.scalers[timeframe]['informer2'] = scaler
            
            X_tensor = torch.from_numpy(X_seq_train)  # (n_samples, seq_len, n_features) - é¿å…å¤åˆ¶
            y_tensor = torch.from_numpy(y_seq_train)  # (n_samples,) - LongTensoréœ€è¦int64
            
            # ğŸ“Š å†…å­˜ç›‘æ§ï¼šå¼ é‡å ç”¨
            tensor_memory_mb = (X_tensor.element_size() * X_tensor.nelement() + 
                               y_tensor.element_size() * y_tensor.nelement()) / (1024 ** 2)
            logger.info(f"   å¼ é‡å†…å­˜å ç”¨: {tensor_memory_mb:.1f} MB")
            
            # 2. æ£€æµ‹GPU
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            logger.info(f"   è®¾å¤‡: {device} {'ğŸš€ (GPUåŠ é€Ÿ)' if device.type == 'cuda' else 'ğŸ’» (CPU)'}")
            
            # ğŸ® GPUå†…å­˜ç›‘æ§
            if torch.cuda.is_available():
                gpu_status = self.monitor_gpu_memory()
                logger.info(f"   GPUå†…å­˜çŠ¶æ€: ä½¿ç”¨{gpu_status['usage_percent']:.1f}% ({gpu_status['used']/1024**3:.1f}GB/{gpu_status['total']/1024**3:.1f}GB)")
            
            # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
            class NumpyTimeSeriesDataset(Dataset):
                def __init__(self, X_np, y_np):
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®æ˜¯è¿ç»­çš„numpyæ•°ç»„
                    self.X_np = np.ascontiguousarray(X_np) if not X_np.flags['C_CONTIGUOUS'] else X_np
                    self.y_np = np.ascontiguousarray(y_np) if not y_np.flags['C_CONTIGUOUS'] else y_np
                def __len__(self):
                    return len(self.y_np)
                def __getitem__(self, idx):
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨copy()é¿å…å†…å­˜æ˜ å°„é—®é¢˜
                    return (
                        torch.from_numpy(self.X_np[idx].copy()).to(dtype=torch.float32),
                        torch.tensor(self.y_np[idx], dtype=torch.long)
                    )

            dataset = NumpyTimeSeriesDataset(X_seq_train, y_seq_train)
            dataloader = DataLoader(
                dataset,
                batch_size=self.informer_batch_size,
                shuffle=True,
                num_workers=0  # Windowså…¼å®¹
            )
            
            # 4. ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
            if custom_params:
                d_model = custom_params.get('d_model', self.informer_d_model)
                n_heads = custom_params.get('n_heads', self.informer_n_heads)
                n_layers = custom_params.get('n_layers', self.informer_n_layers)
                dropout = custom_params.get('dropout', 0.1)
                epochs = custom_params.get('epochs', self.informer_epochs)
                batch_size = custom_params.get('batch_size', self.informer_batch_size)
                lr = custom_params.get('lr', self.informer_lr)
                alpha = custom_params.get('alpha', settings.GMADL_ALPHA)
                beta = custom_params.get('beta', settings.GMADL_BETA)
                logger.info(f"ğŸ¯ ä½¿ç”¨ä¼˜åŒ–å‚æ•°: d_model={d_model}, n_heads={n_heads}, n_layers={n_layers}, epochs={epochs}")
            else:
                d_model = self.informer_d_model
                n_heads = self.informer_n_heads
                n_layers = self.informer_n_layers
                dropout = 0.1
                epochs = self.informer_epochs
                batch_size = self.informer_batch_size
                lr = self.informer_lr
                alpha = settings.GMADL_ALPHA
                beta = settings.GMADL_BETA
            
            # 5. åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ”¯æŒåºåˆ—è¾“å…¥ + æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰
            n_features = X_seq_train.shape[2]  # ç‰¹å¾æ•°é‡ï¼ˆä»åºåˆ—çš„æœ€åä¸€ç»´è·å–ï¼‰
            model = Informer2ForClassification(
                n_features=n_features,
                n_classes=3,  # ç±»åˆ«æ•°
                d_model=d_model,
                n_heads=n_heads,
                n_layers=n_layers,
                dropout=dropout,
                use_distilling=True,  # å¯ç”¨è’¸é¦å±‚ï¼ˆå®Œæ•´Informeræ¶æ„ï¼‰
                use_gradient_checkpointing=self.use_gradient_checkpointing  # ğŸ”¥ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            ).to(device)
            
            logger.info(f"   æ¨¡å‹å‚æ•°: d_model={d_model}, n_heads={n_heads}, n_layers={n_layers}")
            logger.info(f"   è®­ç»ƒå‚æ•°: epochs={epochs}, batch_size={batch_size}, lr={lr}")
            
            # 6. å®šä¹‰æŸå¤±å‡½æ•°ï¼ˆæ”¯æŒGMADL/äº¤å‰ç†µä¸¤ç§æ¨¡å¼ï¼‰
            # ğŸ”‘ GMADLçš„HOLDæƒ©ç½šæŒ‰ç±»åˆ«å æ¯”è‡ªé€‚åº”ï¼ˆ3mæ›´å¼ºä»¥å¯¹æŠ—æç«¯ä¸å¹³è¡¡ï¼‰
            hold_ratio_informer = float((y_seq_train == 1).sum()) / max(len(y_seq_train), 1)
            if timeframe == '3m':
                hold_penalty_nn = float(max(0.35, min(0.70, 0.80 - 0.6 * hold_ratio_informer)))
            else:
                hold_penalty_nn = float(max(0.50, min(0.75, 0.85 - 0.5 * hold_ratio_informer)))

            criterion = create_trade_loss(
                use_gmadl=settings.USE_GMADL_LOSS,
                hold_penalty=hold_penalty_nn,
                alpha=alpha,
                beta=beta
            )

            if settings.USE_GMADL_LOSS:
                logger.info(
                    f"   æŸå¤±å‡½æ•°: GMADL + HOLDæƒ©ç½š (alpha={alpha:.2f}, beta={beta:.2f})"
                )
            else:
                logger.info(
                    "   æŸå¤±å‡½æ•°: äº¤å‰ç†µ + HOLDæƒ©ç½š (ç¨³å®šæ¨¡å¼)"
                )
            
            # 7. å®šä¹‰ä¼˜åŒ–å™¨ï¼ˆæ”¯æŒ8-bit Adamï¼‰
            # ğŸ”¥ å°è¯•ä½¿ç”¨8-bit Adamä¼˜åŒ–å™¨ï¼ˆèŠ‚çœ75%ä¼˜åŒ–å™¨å†…å­˜ï¼‰
            optimizer_created = False
            if self.use_8bit_adam and device.type == 'cuda':
                try:
                    # ğŸ”¥ åŠ¨æ€å¯¼å…¥å¯é€‰ä¾èµ–ï¼ˆbitsandbyteså¯èƒ½æœªå®‰è£…ï¼‰
                    import bitsandbytes as bnb
                    optimizer = bnb.optim.Adam8bit(
                        model.parameters(),
                        lr=lr,
                        weight_decay=1e-5,
                        betas=(0.9, 0.999)
                    )
                    logger.info("   âœ… ä½¿ç”¨8-bit Adamä¼˜åŒ–å™¨ï¼ˆèŠ‚çœ75%ä¼˜åŒ–å™¨å†…å­˜ï¼‰")
                    optimizer_created = True
                except ImportError:
                    logger.warning("   âš ï¸ bitsandbytesæœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡†Adamä¼˜åŒ–å™¨")
                    logger.warning("   ğŸ’¡ å®‰è£…å‘½ä»¤: pip install bitsandbytes")
                except Exception as e:
                    logger.warning(f"   âš ï¸ 8-bit Adamåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨æ ‡å‡†Adam")
            
            # é™çº§åˆ°æ ‡å‡†Adam
            if not optimizer_created:
                optimizer = torch.optim.Adam(
                    model.parameters(),
                    lr=lr,
                    weight_decay=1e-5,  # L2æ­£åˆ™åŒ–
                    betas=(0.9, 0.999)
                )
            
            # âœ… ä¿®å¤C: æ·»åŠ Warmup + ReduceLROnPlateauç»„åˆè°ƒåº¦å™¨
            # Warmupé…ç½®
            warmup_epochs = 5  # å‰5ä¸ªepoch warmup
            target_lr = lr
            
            # ä¸»è°ƒåº¦å™¨ï¼šReduceLROnPlateauï¼ˆç”¨äºwarmupåçš„å­¦ä¹ ç‡è°ƒæ•´ï¼‰
            scheduler = ReduceLROnPlateau(
                optimizer,
                mode='min',
                factor=0.5,
                patience=5,
                min_lr=1e-6,
                threshold=1e-4,
                threshold_mode='rel',
                cooldown=2,
                verbose=True
            )
            
            logger.info(f"   âœ… å­¦ä¹ ç‡è°ƒåº¦: Warmup({warmup_epochs}è½®) + ReduceLROnPlateau")
            logger.info(f"      ç›®æ ‡LR: {target_lr:.6f}, Warmupåè‡ªåŠ¨è°ƒæ•´")
            
            # ğŸš€ 9. æ¢¯åº¦ç´¯ç§¯é…ç½®ï¼ˆè§£å†³GPU OOMé—®é¢˜ï¼Œä¸é™ä½æ¨¡å‹å¤æ‚åº¦ï¼‰
            # å°†å¤§æ‰¹æ¬¡åˆ†æˆå°æ‰¹æ¬¡ï¼Œç´¯ç§¯æ¢¯åº¦ï¼Œä¿æŒç­‰æ•ˆè®­ç»ƒæ•ˆæœ
            effective_batch_size = batch_size  # ä¿æŒåŸå§‹æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
            actual_batch_size = max(8, batch_size // 8)  # ç‰©ç†æ‰¹æ¬¡å¤§å°ç¼©å°8å€ï¼ˆèŠ‚çœ8å€GPUå†…å­˜ï¼‰
            accumulation_steps = effective_batch_size // actual_batch_size  # ç´¯ç§¯æ­¥æ•°
            
            # é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨æ›´å°çš„ç‰©ç†æ‰¹æ¬¡ï¼‰
            dataloader = DataLoader(
                dataset,
                batch_size=actual_batch_size,
                shuffle=True,
                num_workers=0,  # Windowså…¼å®¹
                pin_memory=True if device.type == 'cuda' else False  # åŠ é€ŸGPUæ•°æ®ä¼ è¾“
            )
            
            logger.info(f"   ğŸ® æ¢¯åº¦ç´¯ç§¯ç­–ç•¥: æœ‰æ•ˆæ‰¹æ¬¡={effective_batch_size}, ç‰©ç†æ‰¹æ¬¡={actual_batch_size}, ç´¯ç§¯æ­¥æ•°={accumulation_steps}")
            logger.info(f"   ğŸ’¾ é¢„æœŸGPUå†…å­˜èŠ‚çœ: ~{100*(1-actual_batch_size/batch_size):.0f}%")
            
            # âœ… ä¿®å¤D: åŠ¨æ€æ··åˆç²¾åº¦é…ç½®ï¼ˆè€Œéå®Œå…¨ç¦ç”¨ï¼‰
            use_amp = device.type == 'cuda' and torch.cuda.is_available()

            if use_amp:
                # æ ¹æ®æ¨¡å‹è§„æ¨¡åŠ¨æ€è°ƒæ•´åˆå§‹ç¼©æ”¾å› å­
                num_params = sum(p.numel() for p in model.parameters())
                
                if num_params > 10_000_000:  # >10Må‚æ•°ï¼šå¤§æ¨¡å‹
                    init_scale = 2.**12  # 4096
                    logger.info(f"   æ£€æµ‹åˆ°å¤§æ¨¡å‹({num_params/1e6:.1f}Må‚æ•°)ï¼Œä½¿ç”¨init_scale=2^12")
                elif num_params > 1_000_000:  # 1M-10Må‚æ•°ï¼šä¸­ç­‰æ¨¡å‹
                    init_scale = 2.**14  # 16384
                    logger.info(f"   æ£€æµ‹åˆ°ä¸­ç­‰æ¨¡å‹({num_params/1e6:.1f}Må‚æ•°)ï¼Œä½¿ç”¨init_scale=2^14")
                else:  # <1Må‚æ•°ï¼šå°æ¨¡å‹
                    init_scale = 2.**16  # 65536ï¼ˆé»˜è®¤å€¼ï¼‰
                    logger.info(f"   æ£€æµ‹åˆ°å°æ¨¡å‹({num_params/1e6:.1f}Må‚æ•°)ï¼Œä½¿ç”¨init_scale=2^16")
                
                # âœ… ä¿®å¤ï¼šä½¿ç”¨æ–°çš„torch.amp.GradScaler APIï¼ˆPyTorch 2.0+ï¼‰
                scaler = torch.amp.GradScaler(
                    'cuda',
                    init_scale=init_scale,  # åŠ¨æ€è°ƒæ•´çš„åˆå§‹ç¼©æ”¾
                    growth_factor=1.5,      # å¢é•¿å› å­ï¼ˆé»˜è®¤2.0ï¼Œæ”¹ä¸º1.5æ›´æ¸©å’Œï¼‰
                    backoff_factor=0.5,     # å›é€€å› å­ï¼ˆæ£€æµ‹åˆ°æº¢å‡ºæ—¶ï¼‰
                    growth_interval=1000,   # å¢é•¿é—´éš”ï¼ˆé»˜è®¤2000ï¼Œæ”¹ä¸º1000æ›´è°¨æ…ï¼‰
                    enabled=True
                )
                logger.info("   æ··åˆç²¾åº¦è®­ç»ƒ: å¯ç”¨ï¼ˆåŠ¨æ€ç¼©æ”¾ç­–ç•¥ï¼‰")
                logger.info(f"      åˆå§‹ç¼©æ”¾: {init_scale}, å¢é•¿å› å­: 1.5")
            else:
                scaler = None
                logger.info("   æ··åˆç²¾åº¦è®­ç»ƒ: ç¦ç”¨ï¼ˆCPUç¯å¢ƒï¼‰")
            
            # å¯é€‰ï¼šå¦‚æœæœªæ¥éœ€è¦é‡æ–°å¯ç”¨AMPï¼Œä½¿ç”¨ä¿å®ˆç­–ç•¥
            # if settings.USE_GMADL_LOSS and use_amp:
            #     logger.info("   âš ï¸ GMADLå¼€å¯ â†’ ä¸ºä¿éšœæ•°å€¼ç¨³å®šï¼Œç¦ç”¨AMPæ”¹ç”¨FP32è®­ç»ƒ")
            #     use_amp = False
            # 
            # # ğŸ”¥ æ¿€è¿›æ··åˆç²¾åº¦ä¼˜åŒ–
            # if use_amp and self.use_aggressive_amp:
            #     # è®¾ç½®æ›´é«˜çš„åˆå§‹ç¼©æ”¾å› å­
            #     scaler = torch.amp.GradScaler('cuda', init_scale=2.**16)
            #     
            #     # å¯ç”¨TF32ï¼ˆAmpereæ¶æ„GPUï¼šRTX 30/40ç³»åˆ—ï¼‰
            #     torch.backends.cuda.matmul.allow_tf32 = True
            #     torch.backends.cudnn.allow_tf32 = True
            #     
            #     logger.info(f"   âš¡ å¯ç”¨æ¿€è¿›æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16 + TF32 + é«˜ç¼©æ”¾å› å­ï¼‰")
            # elif use_amp:
            #     scaler = torch.amp.GradScaler('cuda')
            #     logger.info(f"   âš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰ï¼šFP16è®¡ç®— + åŠ¨æ€æŸå¤±ç¼©æ”¾")
            # else:
            #     scaler = None
            
            # âœ… ä¿®å¤E: è®­ç»ƒå‰æ•°æ®è´¨é‡æ£€æŸ¥
            logger.info("ğŸ” æ‰§è¡Œè®­ç»ƒå‰æ•°æ®è´¨é‡æ£€æŸ¥...")
            
            # æ£€æŸ¥ç‰¹å¾æ•°æ®
            if torch.isnan(X_tensor).any():
                nan_count = torch.isnan(X_tensor).sum().item()
                logger.error(f"âŒ è®­ç»ƒæ•°æ®åŒ…å«{nan_count}ä¸ªNaNå€¼ï¼Œè®­ç»ƒç»ˆæ­¢ï¼")
                raise ValueError(f"è®­ç»ƒæ•°æ®åŒ…å«NaNå€¼ï¼š{nan_count}ä¸ª")
            
            if torch.isinf(X_tensor).any():
                inf_count = torch.isinf(X_tensor).sum().item()
                logger.error(f"âŒ è®­ç»ƒæ•°æ®åŒ…å«{inf_count}ä¸ªINFå€¼ï¼Œè®­ç»ƒç»ˆæ­¢ï¼")
                raise ValueError(f"è®­ç»ƒæ•°æ®åŒ…å«INFå€¼ï¼š{inf_count}ä¸ª")
            
            # æ£€æŸ¥æ ‡ç­¾æ•°æ®
            if torch.isnan(y_tensor.float()).any() or torch.isinf(y_tensor.float()).any():
                logger.error(f"âŒ è®­ç»ƒæ ‡ç­¾åŒ…å«NaN/INFå€¼ï¼Œè®­ç»ƒç»ˆæ­¢ï¼")
                raise ValueError("è®­ç»ƒæ ‡ç­¾åŒ…å«NaN/INFå€¼")
            
            # æ£€æŸ¥æ ‡ç­¾èŒƒå›´
            unique_labels = torch.unique(y_tensor)
            if not all(label in [0, 1, 2] for label in unique_labels.tolist()):
                logger.error(f"âŒ è®­ç»ƒæ ‡ç­¾åŒ…å«éæ³•å€¼ï¼š{unique_labels.tolist()}ï¼ŒæœŸæœ›[0,1,2]")
                raise ValueError(f"è®­ç»ƒæ ‡ç­¾åŒ…å«éæ³•å€¼ï¼š{unique_labels.tolist()}")
            
            # ç»Ÿè®¡æ•°æ®èŒƒå›´
            logger.info(f"   ç‰¹å¾èŒƒå›´: [{X_tensor.min().item():.4f}, {X_tensor.max().item():.4f}]")
            logger.info(f"   ç‰¹å¾å‡å€¼: {X_tensor.mean().item():.4f}, æ ‡å‡†å·®: {X_tensor.std().item():.4f}")
            logger.info(f"   æ ‡ç­¾åˆ†å¸ƒ: {torch.bincount(y_tensor.long()).tolist()}")
            logger.info(f"âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡")
            
            # ğŸ” æ¨¡å‹æƒé‡åˆå§‹åŒ–æ£€æŸ¥
            logger.info("ğŸ” æ£€æŸ¥æ¨¡å‹æƒé‡åˆå§‹åŒ–...")
            has_nan_weights = False
            has_inf_weights = False
            
            for name, param in model.named_parameters():
                if torch.isnan(param).any():
                    logger.error(f"âŒ æ¨¡å‹å‚æ•° {name} åŒ…å«NaNå€¼ï¼")
                    has_nan_weights = True
                if torch.isinf(param).any():
                    logger.error(f"âŒ æ¨¡å‹å‚æ•° {name} åŒ…å«INFå€¼ï¼")
                    has_inf_weights = True
            
            if has_nan_weights or has_inf_weights:
                logger.error("âŒ æ¨¡å‹æƒé‡åˆå§‹åŒ–å¼‚å¸¸ï¼Œè®­ç»ƒç»ˆæ­¢ï¼")
                raise ValueError("æ¨¡å‹æƒé‡åˆå§‹åŒ–åŒ…å«NaN/INFå€¼")
            
            logger.info("âœ… æ¨¡å‹æƒé‡åˆå§‹åŒ–æ­£å¸¸")
            
            # 11. è®­ç»ƒå¾ªç¯ï¼ˆå¸¦æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦ï¼‰
            model.train()
            best_loss = float('inf')
            
            # âœ… ä¿®å¤F: å¹³è¡¡çš„æ—©æœŸç»ˆæ­¢é˜ˆå€¼
            nan_inf_count = 0  # ç»Ÿè®¡nan/infå‡ºç°æ¬¡æ•°
            max_nan_inf_tolerance = 30  # ä»50é™ä½åˆ°30ï¼ˆå¹³è¡¡å€¼ï¼‰
            consecutive_nan_inf = 0  # è¿ç»­nan/infæ¬¡æ•°
            max_consecutive_nan_inf = 8  # ä»10é™ä½åˆ°8ï¼ˆå¹³è¡¡å€¼ï¼‰
            
            logger.info(f"   æ—©æœŸç»ˆæ­¢é˜ˆå€¼: è¿ç»­{max_consecutive_nan_inf}æ¬¡ æˆ– ç´¯è®¡{max_nan_inf_tolerance}æ¬¡")
            
            for epoch in range(epochs):
                epoch_loss = 0.0
                correct = 0
                total = 0
                processed_batches = 0  # å®é™…å¤„ç†çš„batchæ•°ï¼ˆæ’é™¤nan/infçš„batchï¼‰
                epoch_nan_inf_count = 0  # æœ¬epochçš„nan/infæ¬¡æ•°
                
                optimizer.zero_grad()  # åˆå§‹åŒ–æ¢¯åº¦
                
                for i, (batch_X, batch_y) in enumerate(dataloader):
                    batch_X = batch_X.to(device, non_blocking=True)
                    batch_y = batch_y.to(device, non_blocking=True)
                    
                    # âœ… ä¿®å¤A - è¯Šæ–­1: æ£€æŸ¥è¾“å…¥æ•°æ®
                    if torch.isnan(batch_X).any() or torch.isinf(batch_X).any():
                        logger.error(f"âŒ Batch {i+1}: è¾“å…¥æ•°æ®åŒ…å«NaN/INF")
                        logger.error(f"   NaNæ•°é‡: {torch.isnan(batch_X).sum().item()}")
                        logger.error(f"   INFæ•°é‡: {torch.isinf(batch_X).sum().item()}")
                        # ä¿å­˜å¼‚å¸¸batchç”¨äºç¦»çº¿åˆ†æ
                        try:
                            torch.save({'X': batch_X.cpu(), 'y': batch_y.cpu()}, 
                                      f'debug_batch_{epoch}_{i}.pt')
                        except:
                            pass
                        optimizer.zero_grad()
                        continue
                    
                    # ğŸ¯ æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                    if use_amp:
                        with torch.amp.autocast('cuda'):
                            logits = model(batch_X)
                            # ç»Ÿä¸€dtypeä¸lossè¾“å…¥ï¼šlogitsç”¨float32ï¼Œtargetsç”¨long
                            loss = criterion(logits.float(), batch_y.long())
                            loss = loss / accumulation_steps  # å½’ä¸€åŒ–æŸå¤±ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
                    else:
                        logits = model(batch_X)
                        loss = criterion(logits.float(), batch_y.long())
                        loss = loss / accumulation_steps
                    
                    # âœ… ä¿®å¤A - è¯Šæ–­2: æ£€æŸ¥æ¨¡å‹è¾“å‡º
                    if torch.isnan(logits).any() or torch.isinf(logits).any():
                        logger.error(f"âŒ Batch {i+1}: æ¨¡å‹è¾“å‡º(logits)åŒ…å«NaN/INF")
                        logger.error(f"   è¾“å…¥èŒƒå›´: [{batch_X.min().item():.4f}, {batch_X.max().item():.4f}]")
                        
                        # é€å±‚è¯Šæ–­ï¼ˆä½¿ç”¨forward hooksï¼Œæ›´å®‰å…¨ï¼‰
                        logger.error("   ğŸ” é€å±‚è¯Šæ–­ï¼ˆä½¿ç”¨hooksï¼‰:")
                        activation_stats = {}
                        hooks = []
                        
                        def get_activation_hook(name):
                            def hook(module, input, output):
                                if isinstance(output, torch.Tensor):
                                    has_nan = torch.isnan(output).any().item()
                                    has_inf = torch.isinf(output).any().item()
                                    activation_stats[name] = {
                                        'has_nan': has_nan,
                                        'has_inf': has_inf,
                                        'min': output.min().item() if not (has_nan or has_inf) else None,
                                        'max': output.max().item() if not (has_nan or has_inf) else None
                                    }
                            return hook
                        
                        # æ³¨å†Œhooks
                        with torch.no_grad():
                            for name, module in model.named_modules():
                                if len(list(module.children())) == 0:  # åªå¯¹å¶å­æ¨¡å—
                                    hook = module.register_forward_hook(get_activation_hook(name))
                                    hooks.append(hook)
                            
                            # é‡æ–°æ‰§è¡Œforward
                            try:
                                _ = model(batch_X)
                                
                                # æ‰“å°å¼‚å¸¸å±‚
                                for name, stats in activation_stats.items():
                                    if stats['has_nan'] or stats['has_inf']:
                                        logger.error(f"      {name}: NaN={stats['has_nan']}, INF={stats['has_inf']}")
                            except Exception as e:
                                logger.error(f"      é€å±‚è¯Šæ–­å¤±è´¥: {e}")
                            finally:
                                # ç§»é™¤æ‰€æœ‰hooks
                                for hook in hooks:
                                    hook.remove()
                        
                        optimizer.zero_grad()
                        continue
                    
                    # âœ… ä¿®å¤A - è¯Šæ–­3: æ£€æŸ¥æŸå¤±å€¼
                    if torch.isnan(loss) or torch.isinf(loss):
                        nan_inf_count += 1
                        consecutive_nan_inf += 1
                        epoch_nan_inf_count += 1
                        
                        logger.error(f"âŒ Batch {i+1}: æŸå¤±ä¸ºNaN/INF")
                        logger.error(f"   Logitsç»Ÿè®¡: min={logits.min().item():.4f}, "
                                    f"max={logits.max().item():.4f}, "
                                    f"mean={logits.mean().item():.4f}")
                        logger.error(f"   Targetåˆ†å¸ƒ: {torch.bincount(batch_y.long()).tolist()}")
                        
                        # âœ… ä¿®å¤A - è¯Šæ–­4: æ£€æŸ¥æ¢¯åº¦
                        loss.backward()
                        max_grad_norm = 0.0
                        for name, param in model.named_parameters():
                            if param.grad is not None:
                                grad_norm = param.grad.norm().item()
                                max_grad_norm = max(max_grad_norm, grad_norm)
                                if grad_norm > 1000 or grad_norm != grad_norm:  # æ¢¯åº¦çˆ†ç‚¸æˆ–NaN
                                    logger.error(f"   {name}: æ¢¯åº¦å¼‚å¸¸ norm={grad_norm:.4f}")
                        
                        logger.error(f"   æœ€å¤§æ¢¯åº¦èŒƒæ•°: {max_grad_norm:.4f}")
                        
                        # ä»…åœ¨å‰5æ¬¡æˆ–æ¯50æ¬¡æ‰“å°è­¦å‘Šï¼Œé¿å…æ—¥å¿—åˆ·å±
                        if nan_inf_count <= 5 or nan_inf_count % 50 == 0:
                            logger.warning(f"âš ï¸ Epoch {epoch+1} Batch {i+1}: æ£€æµ‹åˆ°æŸå¤±ä¸ºnan/infï¼ˆç´¯è®¡{nan_inf_count}æ¬¡ï¼Œè¿ç»­{consecutive_nan_inf}æ¬¡ï¼‰")
                        
                        # ğŸš¨ æ£€æŸ¥æ˜¯å¦è¶…è¿‡å®¹å¿é˜ˆå€¼
                        if consecutive_nan_inf >= max_consecutive_nan_inf:
                            logger.error(f"âŒ è¿ç»­{consecutive_nan_inf}ä¸ªbatchå‡ºç°nan/infæŸå¤±ï¼Œè®­ç»ƒç»ˆæ­¢ï¼")
                            logger.error(f"   å¯èƒ½åŸå› ï¼š1) å­¦ä¹ ç‡è¿‡å¤§ 2) GMADLæŸå¤±å‡½æ•°æ•°å€¼ä¸ç¨³å®š 3) æ•°æ®å¼‚å¸¸")
                            logger.error(f"   å»ºè®®ï¼š1) é™ä½å­¦ä¹ ç‡ 2) ä½¿ç”¨FP32ç²¾åº¦ 3) æ£€æŸ¥æ•°æ®è´¨é‡")
                            raise ValueError(f"è®­ç»ƒè¿‡ç¨‹æ•°å€¼ä¸ç¨³å®šï¼šè¿ç»­{consecutive_nan_inf}ä¸ªbatchå‡ºç°nan/infæŸå¤±")
                        
                        if nan_inf_count >= max_nan_inf_tolerance:
                            logger.error(f"âŒ ç´¯è®¡{nan_inf_count}ä¸ªbatchå‡ºç°nan/infæŸå¤±ï¼ˆè¶…è¿‡é˜ˆå€¼{max_nan_inf_tolerance}ï¼‰ï¼Œè®­ç»ƒç»ˆæ­¢ï¼")
                            raise ValueError(f"è®­ç»ƒè¿‡ç¨‹æ•°å€¼ä¸ç¨³å®šï¼šç´¯è®¡{nan_inf_count}ä¸ªbatchå‡ºç°nan/infæŸå¤±")
                        
                        optimizer.zero_grad()
                        continue
                    
                    # æˆåŠŸå¤„ç†batchï¼Œé‡ç½®è¿ç»­nan/infè®¡æ•°å™¨
                    consecutive_nan_inf = 0
                    
                    # ğŸ¯ æ··åˆç²¾åº¦åå‘ä¼ æ’­
                    if use_amp:
                        scaler.scale(loss).backward()
                    else:
                        loss.backward()
                    
                    # âœ… ä¿®å¤B: æ¢¯åº¦è£å‰ªï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰â­
                    # ğŸ¯ æ¢¯åº¦ç´¯ç§¯ï¼šæ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
                    if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):
                        # âš ï¸ é‡è¦ï¼šæ··åˆç²¾åº¦è®­ç»ƒæ—¶å¿…é¡»å…ˆunscale_()å†è£å‰ª
                        if use_amp:
                            scaler.unscale_(optimizer)  # å…ˆåç¼©æ”¾æ¢¯åº¦ï¼Œå¦åˆ™è£å‰ªæ— æ•ˆ
                        
                        # â­ æ ¸å¿ƒä¿®å¤ï¼šæ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
                        torch.nn.utils.clip_grad_norm_(
                            model.parameters(), 
                            max_norm=1.0,      # æ¢¯åº¦èŒƒæ•°ä¸Šé™ï¼ˆInformer2å»ºè®®1.0ï¼‰
                            norm_type=2.0       # L2èŒƒæ•°
                        )
                        
                        # ä¼˜åŒ–å™¨æ­¥è¿›
                        if use_amp:
                            scaler.step(optimizer)
                            scaler.update()
                        else:
                            optimizer.step()
                        
                        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
                        
                        # ğŸ§¹ å®šæœŸæ¸…ç†GPUç¼“å­˜ï¼ˆæ¯10ä¸ªç´¯ç§¯å‘¨æœŸï¼‰
                        if (i + 1) % (accumulation_steps * 10) == 0 and device.type == 'cuda':
                            torch.cuda.empty_cache()
                    
                    # ç»Ÿè®¡ï¼ˆä½¿ç”¨æœªå½’ä¸€åŒ–çš„æŸå¤±ï¼‰
                    processed_batches += 1
                    epoch_loss += loss.item() * accumulation_steps
                    with torch.no_grad():
                        _, predicted = torch.max(logits, 1)
                        total += batch_y.size(0)
                        correct += (predicted == batch_y).sum().item()
                
                # ğŸ§¹ æ¯ä¸ªepochç»“æŸåæ¸…ç†GPUç¼“å­˜
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
                
                # âœ… ä¿®å¤F: Epochçº§åˆ«æ£€æŸ¥
                total_batches = len(dataloader)
                
                if processed_batches == 0:
                    logger.error(f"âŒ Epoch {epoch+1}: æ²¡æœ‰ä»»ä½•batchæˆåŠŸå¤„ç†ï¼ˆå…¨éƒ¨{total_batches}ä¸ªbatchå‡ä¸ºnan/infï¼‰ï¼Œè®­ç»ƒç»ˆæ­¢ï¼")
                    raise ValueError(f"Epoch {epoch+1}æ‰€æœ‰batchå‡å‡ºç°nan/infï¼Œè®­ç»ƒæ— æ³•ç»§ç»­")
                
                if epoch_nan_inf_count > total_batches * 0.5:
                    logger.error(f"âŒ Epoch {epoch+1}: {epoch_nan_inf_count}/{total_batches} batchå‡ºç°nan/inf "
                                f"({100*epoch_nan_inf_count/total_batches:.1f}%ï¼Œè¶…è¿‡50%é˜ˆå€¼ï¼‰ï¼Œè®­ç»ƒç»ˆæ­¢ï¼")
                    raise ValueError(f"Epoch {epoch+1}è¶…è¿‡50%çš„batchå‡ºç°nan/infï¼Œè®­ç»ƒè´¨é‡æ— æ³•ä¿è¯")
                
                if epoch_nan_inf_count > total_batches * 0.3:
                    logger.warning(f"âš ï¸ Epoch {epoch+1}: {epoch_nan_inf_count}/{total_batches} batchå‡ºç°nan/inf "
                                  f"({100*epoch_nan_inf_count/total_batches:.1f}%ï¼‰ï¼Œæ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼")
                
                # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
                avg_loss = epoch_loss / max(processed_batches, 1)
                accuracy = 100.0 * correct / max(total, 1)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if avg_loss < best_loss:
                    best_loss = avg_loss
                
                # âœ… ä¿®å¤C: å­¦ä¹ ç‡è°ƒåº¦ï¼ˆç®€åŒ–çš„Warmup + ReduceLROnPlateauï¼‰
                if epoch < warmup_epochs:
                    # Warmupé˜¶æ®µï¼šçº¿æ€§å¢é•¿å­¦ä¹ ç‡
                    warmup_lr = target_lr * (epoch + 1) / warmup_epochs
                    for param_group in optimizer.param_groups:
                        param_group['lr'] = warmup_lr
                    phase = "Warmup"
                    current_lr = warmup_lr
                else:
                    # ä¸»è°ƒåº¦é˜¶æ®µï¼šæ ¹æ®lossè‡ªåŠ¨è°ƒæ•´
                    scheduler.step(avg_loss)
                    phase = "Main"
                    current_lr = optimizer.param_groups[0]['lr']
                
                # æ¯10è½®æˆ–æœ€å1è½®æ‰“å°è¿›åº¦ï¼ˆå¸¦å­¦ä¹ ç‡ï¼‰
                if (epoch + 1) % 10 == 0 or epoch == epochs - 1:
                    nan_info = f", nan/infè·³è¿‡: {epoch_nan_inf_count}" if epoch_nan_inf_count > 0 else ""
                    logger.info(
                        f"   Epoch [{epoch+1}/{epochs}] "
                        f"Loss: {avg_loss:.4f}, Acc: {accuracy:.2f}%, "
                        f"LR: {current_lr:.6f} ({phase}){nan_info}"
                    )
            
            # ğŸ“Š è®­ç»ƒå®Œæˆæ€»ç»“
            if nan_inf_count > 0:
                logger.warning(f"âš ï¸ Informer2è®­ç»ƒå®Œæˆï¼Œä½†å‡ºç°{nan_inf_count}æ¬¡nan/infæŸå¤±ï¼ˆå·²è·³è¿‡ï¼‰")
                logger.warning(f"   æ•°å€¼ç¨³å®šæ€§é—®é¢˜å¯èƒ½å½±å“æ¨¡å‹è´¨é‡ï¼Œå»ºè®®ï¼š")
                logger.warning(f"   1. é™ä½å­¦ä¹ ç‡ï¼ˆå½“å‰ï¼š{lr}ï¼‰")
                logger.warning(f"   2. ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆuse_amp=Falseï¼‰")
                logger.warning(f"   3. è°ƒæ•´GMADLæŸå¤±å‡½æ•°å‚æ•°")
            else:
                logger.info(f"âœ… Informer2è®­ç»ƒå®Œæˆï¼Œæ— æ•°å€¼ç¨³å®šæ€§é—®é¢˜")
            
            # 9. åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
            model.eval()
            
            # 10. åŒ…è£…æ¨¡å‹ä»¥å…¼å®¹scikit-learnæ¥å£ï¼ˆæ”¯æŒåºåˆ—è¾“å…¥ï¼‰
            # âœ… ä½¿ç”¨æ¨¡å—çº§åˆ«çš„InformerWrapperç±»ï¼ˆæ”¯æŒpickleåºåˆ—åŒ–ï¼‰
            wrapped_model = InformerWrapper(model, device)
            
            # ğŸ® GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒåæ¸…ç†
            self.clear_gpu_memory()
            
            training_time = time.time() - start_time
            logger.info(f"âœ… Informer-2è®­ç»ƒå®Œæˆ: æœ€ä½³Loss={best_loss:.4f}, "
                       f"è€—æ—¶={training_time:.2f}ç§’")
            
            return wrapped_model
            
        except Exception as e:
            logger.error(f"Informer-2è®­ç»ƒå¤±è´¥: {e}")
            # ğŸ® GPUå†…å­˜ç®¡ç†ï¼šå¼‚å¸¸æ—¶æ¸…ç†
            self.clear_gpu_memory()
            logger.warning("âš ï¸ å°†è·³è¿‡Informer-2ï¼Œä»…ä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹")
            return None
    
    async def predict(
        self, 
        data: pd.DataFrame, 
        timeframe: str
    ) -> Dict[str, Any]:
        """
        é›†æˆé¢„æµ‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼‰
        
        Args:
            data: Kçº¿æ•°æ®DataFrame
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            é¢„æµ‹ç»“æœï¼Œå¦‚æœæ¨¡å‹è®­ç»ƒä¸­åˆ™è¿”å›None
        """
        try:
            # ï¿½ æ£€ç”Ÿäº§çº§åˆ«ï¼šåå°è®­ç»ƒä¸å½±å“é¢„æµ‹
            # è®­ç»ƒå’Œé¢„æµ‹å¹¶è¡Œè¿è¡Œï¼Œè®­ç»ƒå®Œæˆåçƒ­æ›´æ–°æ¨¡å‹
            if self.background_training:
                training_tfs = [tf for tf, status in self.training_in_progress.items() if status]
                logger.debug(f"ğŸ”„ åå°è®­ç»ƒä¸­ï¼ˆ{', '.join(training_tfs)}ï¼‰ï¼Œé¢„æµ‹ç»§ç»­ä½¿ç”¨å½“å‰æ¨¡å‹")
            
            # ä»…åœ¨é¦–æ¬¡è®­ç»ƒæ—¶ï¼ˆæ¨¡å‹ä¸å­˜åœ¨ï¼‰æ‰é˜»æ­¢é¢„æµ‹
            if timeframe not in self.ensemble_models and not self.models_ready.get(timeframe, False):
                if self.training_in_progress.get(timeframe, False):
                    logger.debug(f"â³ {timeframe} é¦–æ¬¡è®­ç»ƒä¸­ï¼Œç­‰å¾…æ¨¡å‹å°±ç»ª")
                    return None
                else:
                    logger.debug(f"â¸ï¸ {timeframe} æ¨¡å‹æœªå°±ç»ªï¼Œç­‰å¾…è®­ç»ƒå®Œæˆ")
                    return None
            
            # æ£€æŸ¥é›†æˆæ¨¡å‹æ˜¯å¦å­˜åœ¨
            if timeframe not in self.ensemble_models:
                # å¦‚æœæ¨¡å‹æœªå°±ç»ªä¸”ä¸åœ¨è®­ç»ƒä¸­ï¼Œå°è¯•åŠ è½½
                if not self.models_ready.get(timeframe, False):
                    logger.debug(f"â¸ï¸ {timeframe} æ¨¡å‹æœªå°±ç»ªï¼Œç­‰å¾…è®­ç»ƒå®Œæˆ")
                    return None
                logger.warning(f"âš ï¸ {timeframe} é›†æˆæ¨¡å‹æœªè®­ç»ƒï¼Œé™çº§åˆ°å•æ¨¡å‹")
                return await super().predict(data, timeframe)
            
            # ç‰¹å¾å·¥ç¨‹
            processed_data = self.feature_engineer.create_features(data.copy())
            if processed_data.empty:
                return None
            
            # å‡†å¤‡ç‰¹å¾ï¼ˆä½¿ç”¨è¯¥æ—¶é—´æ¡†æ¶çš„ç‰¹å¾åˆ—ï¼‰
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            if not feature_columns:
                logger.error(f"{timeframe} ç‰¹å¾åˆ—æœªæ‰¾åˆ°")
                return None
            
            X = processed_data.iloc[-1:][feature_columns]
            if len(X) == 0:
                return None
            
            # ç‰¹å¾ç¼©æ”¾
            X_scaled = self._scale_features(X, timeframe, fit=False)
            
            # è·å–é›†æˆæ¨¡å‹
            models = self.ensemble_models[timeframe]
            
            # ä¸‰ä¸ªåŸºç¡€æ¨¡å‹é¢„æµ‹ï¼ˆX_scaledå¯èƒ½æ˜¯numpyæ•°ç»„æˆ–DataFrameï¼‰
            if isinstance(X_scaled, np.ndarray):
                X_pred = X_scaled
            else:
                X_pred = X_scaled.iloc[[-1]] if hasattr(X_scaled, 'iloc') else X_scaled
            
            # ğŸ”‘ åŸºç¡€æ¨¡å‹é¢„æµ‹ï¼ˆä½¿ç”¨çŸ­é”®åï¼‰
            lgb_proba = models['lgb'].predict_proba(X_pred)[0]
            xgb_proba = models['xgb'].predict_proba(X_pred)[0]
            cat_proba = models['cat'].predict_proba(X_pred)[0]
            
            lgb_pred = models['lgb'].predict(X_pred)[0]
            xgb_pred = models['xgb'].predict(X_pred)[0]
            cat_pred = models['cat'].predict(X_pred)[0]
            
            # ğŸ¤– Informer-2é¢„æµ‹ï¼ˆå¦‚æœå­˜åœ¨ï¼Œéœ€è¦åºåˆ—è¾“å…¥ï¼‰
            if 'inf' in models:
                # æ„é€ åºåˆ—è¾“å…¥ï¼ˆå–æœ€æ–°seq_lenä¸ªæ—¶é—´æ­¥ï¼‰
                seq_len = self.seq_len_config.get(timeframe, 96)
                
                if len(processed_data) < seq_len:
                    logger.warning(f"âš ï¸ æ•°æ®ä¸è¶³ï¼šéœ€è¦{seq_len}ä¸ªæ—¶é—´æ­¥ï¼Œå®é™…{len(processed_data)}ä¸ªï¼Œè·³è¿‡Informer-2é¢„æµ‹")
                    inf_proba = None
                    inf_pred = None
                else:
                    # å–æœ€æ–°seq_lenä¸ªæ—¶é—´æ­¥æ„é€ åºåˆ—
                    latest_seq = processed_data.iloc[-seq_len:][feature_columns].values
                    latest_seq = latest_seq.reshape(1, seq_len, -1)  # (1, seq_len, n_features)
                    
                    # âœ… å…³é”®ä¿®å¤ï¼šé¢„æµ‹æ—¶ä¹Ÿéœ€è¦å½’ä¸€åŒ–ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„scalerï¼‰
                    if timeframe in self.scalers and 'informer2' in self.scalers[timeframe]:
                        scaler = self.scalers[timeframe]['informer2']
                        # Reshapeä¸º2Dè¿›è¡Œå½’ä¸€åŒ–
                        original_shape = latest_seq.shape
                        n_features = original_shape[2]
                        latest_seq_2d = latest_seq.reshape(-1, n_features)
                        latest_seq_2d_scaled = scaler.transform(latest_seq_2d)
                        latest_seq = latest_seq_2d_scaled.reshape(original_shape).astype(np.float32)
                        logger.debug(f"   âœ… Informer-2é¢„æµ‹æ•°æ®å·²å½’ä¸€åŒ–")
                    else:
                        logger.warning(f"âš ï¸ {timeframe} Informer-2 scaleræœªæ‰¾åˆ°ï¼Œé¢„æµ‹æ•°æ®æœªå½’ä¸€åŒ–")
                    
                    inf_proba = models['inf'].predict_proba(latest_seq)[0]
                    inf_pred = models['inf'].predict(latest_seq)[0]
            else:
                inf_proba = None
                inf_pred = None
            
            # Stackingé¢„æµ‹ï¼ˆä½¿ç”¨å…ƒå­¦ä¹ å™¨ï¼‰
            if 'meta' in models:
                # ğŸ†• ç”Ÿæˆå¢å¼ºå…ƒç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                # 1. æ¨¡å‹ä¸€è‡´æ€§
                if inf_proba is not None:
                    agreement = float((lgb_pred == xgb_pred) and (xgb_pred == cat_pred) and (cat_pred == inf_pred))
                else:
                    agreement = float((lgb_pred == xgb_pred) and (xgb_pred == cat_pred))
                
                # 2. æœ€å¤§æ¦‚ç‡
                lgb_max_prob = lgb_proba.max()
                xgb_max_prob = xgb_proba.max()
                cat_max_prob = cat_proba.max()
                
                # 3. æ¦‚ç‡ç†µï¼ˆå•ä¸ªæ ·æœ¬ï¼‰
                lgb_entropy = entr(lgb_proba).sum()
                xgb_entropy = entr(xgb_proba).sum()
                cat_entropy = entr(cat_proba).sum()
                
                # 4. å¹³å‡æ¦‚ç‡
                if inf_proba is not None:
                    inf_max_prob = inf_proba.max()
                    inf_entropy = entr(inf_proba).sum()
                    avg_proba = (lgb_proba + xgb_proba + cat_proba + inf_proba) / 4
                else:
                    avg_proba = (lgb_proba + xgb_proba + cat_proba) / 3
                
                # 5. æ¦‚ç‡æ ‡å‡†å·®
                if inf_proba is not None:
                    prob_std = np.std(np.stack([lgb_proba, xgb_proba, cat_proba, inf_proba]), axis=0)
                else:
                    prob_std = np.std(np.stack([lgb_proba, xgb_proba, cat_proba]), axis=0)
                prob_std_max = prob_std.max()
                
                # ğŸ”‘ æ‹¼æ¥æ‰€æœ‰å…ƒç‰¹å¾ï¼ˆ20ä¸ªæˆ–23ä¸ªï¼‰
                if inf_proba is not None:
                    # åŒ…å«Informer-2ï¼ˆ23ä¸ªç‰¹å¾ï¼‰
                    meta_features = np.hstack([
                        lgb_proba,           # 3ä¸ª
                        xgb_proba,           # 3ä¸ª
                        cat_proba,           # 3ä¸ª
                        inf_proba,           # 3ä¸ª â† Informer-2
                        [agreement],         # 1ä¸ª
                        [lgb_max_prob],      # 1ä¸ª
                        [xgb_max_prob],      # 1ä¸ª
                        [cat_max_prob],      # 1ä¸ª
                        [inf_max_prob],      # 1ä¸ª â† Informer-2
                        [lgb_entropy],       # 1ä¸ª
                        [xgb_entropy],       # 1ä¸ª
                        [cat_entropy],       # 1ä¸ª
                        [inf_entropy],       # 1ä¸ª â† Informer-2
                        avg_proba,           # 3ä¸ª
                        [prob_std_max]       # 1ä¸ª
                    ]).reshape(1, -1)  # (1, 23)
                else:
                    # ä»…ä¼ ç»Ÿæ¨¡å‹ï¼ˆ20ä¸ªç‰¹å¾ï¼‰
                    meta_features = np.hstack([
                        lgb_proba,           # 3ä¸ª
                        xgb_proba,           # 3ä¸ª
                        cat_proba,           # 3ä¸ª
                        [agreement],         # 1ä¸ª
                        [lgb_max_prob],      # 1ä¸ª
                        [xgb_max_prob],      # 1ä¸ª
                        [cat_max_prob],      # 1ä¸ª
                        [lgb_entropy],       # 1ä¸ª
                        [xgb_entropy],       # 1ä¸ª
                        [cat_entropy],       # 1ä¸ª
                        avg_proba,           # 3ä¸ª
                        [prob_std_max]       # 1ä¸ª
                    ]).reshape(1, -1)  # (1, 20)
                
                # å…ƒå­¦ä¹ å™¨é¢„æµ‹
                stacking_proba = models['meta'].predict_proba(meta_features)[0]
                final_pred = stacking_proba.argmax()
                confidence = stacking_proba[final_pred]
                final_probabilities = stacking_proba  # ä½¿ç”¨å…ƒå­¦ä¹ å™¨æ¦‚ç‡
            else:
                # é™çº§ï¼šç®€å•åŠ æƒå¹³å‡ï¼ˆå¦‚æœå…ƒå­¦ä¹ å™¨ä¸å­˜åœ¨ï¼‰
                weights = self.fallback_weights
                ensemble_proba = (
                    lgb_proba * weights['lgb'] +
                    xgb_proba * weights['xgb'] +
                    cat_proba * weights['cat']
                )
                final_pred = ensemble_proba.argmax()
                confidence = ensemble_proba[final_pred]
                final_probabilities = ensemble_proba  # ä½¿ç”¨åŠ æƒå¹³å‡æ¦‚ç‡
            
            # æ˜ å°„åˆ°ä¿¡å·ç±»å‹
            signal_map = {0: 'SHORT', 1: 'HOLD', 2: 'LONG'}
            signal_type = signal_map[final_pred]
            
            # ç®€æ´è®°å½•é¢„æµ‹ç»“æœ
            logger.info(f"ğŸ¯ {timeframe} Stackingé¢„æµ‹: {format_signal_type(signal_type)} "
                       f"(ç½®ä¿¡åº¦={confidence:.4f}, æ¦‚ç‡: ğŸ“‰{final_probabilities[0]:.2f} â¸ï¸{final_probabilities[1]:.2f} ğŸ“ˆ{final_probabilities[2]:.2f})")
            
            # è¿”å›å€¼æ ¼å¼ä¸çˆ¶ç±»ä¸€è‡´
            return {
                'signal_type': signal_type,
                'confidence': float(confidence),
                'probabilities': {
                    'short': float(final_probabilities[0]),
                    'hold': float(final_probabilities[1]),
                    'long': float(final_probabilities[2])
                },
                'timestamp': datetime.now(),
                'model_version': '2.0_stacking_ensemble'
            }
            
        except Exception as e:
            logger.error(f"é›†æˆé¢„æµ‹å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return {}
    
    def _save_ensemble_models(self, timeframe: str):
        """
        ä¿å­˜é›†æˆæ¨¡å‹ï¼ˆç”Ÿäº§çº§åˆ«ï¼šåŸå­æ€§ä¿å­˜ï¼‰
        
        ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶+åŸå­æ€§é‡å‘½åï¼Œç¡®ä¿ï¼š
        1. ä¿å­˜è¿‡ç¨‹ä¸­ä¸å½±å“æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹
        2. ä¿å­˜å¤±è´¥ä¸ä¼šç ´åç°æœ‰æ¨¡å‹
        3. ä¿å­˜æˆåŠŸåç«‹å³å¯ç”¨
        """
        try:
            models = self.ensemble_models[timeframe]
            model_dir = Path(self.model_dir)
            model_dir.mkdir(parents=True, exist_ok=True)
            
            # ğŸ”¥ ä½¿ç”¨ä¸´æ—¶ç›®å½•è¿›è¡ŒåŸå­æ€§ä¿å­˜
            with tempfile.TemporaryDirectory(dir=model_dir) as temp_dir:
                temp_path = Path(temp_dir)
                saved_count = 0
                
                # ğŸ”‘ ä¿å­˜æ¨¡å‹åˆ°ä¸´æ—¶ç›®å½•
                model_mapping = {
                    'lgb': 'lgb',
                    'xgb': 'xgb',
                    'cat': 'cat',
                    'meta': 'meta'
                }
                
                for short_name in model_mapping:
                    if short_name in models:
                        temp_file = temp_path / f"{settings.SYMBOL}_{timeframe}_{short_name}_model.pkl"
                        with open(temp_file, 'wb') as f:
                            pickle.dump(models[short_name], f)
                        saved_count += 1
                
                # ä¿å­˜Informer-2
                if 'inf' in models and TORCH_AVAILABLE:
                    temp_file = temp_path / f"{settings.SYMBOL}_{timeframe}_inf_model.pt"
                    with open(temp_file, 'wb') as f:
                        pickle.dump(models['inf'], f)
                    saved_count += 1
                
                # ä¿å­˜scalerå’Œç‰¹å¾åˆ—è¡¨
                if timeframe in self.scalers:
                    temp_file = temp_path / f"{settings.SYMBOL}_{timeframe}_scaler.pkl"
                    with open(temp_file, 'wb') as f:
                        pickle.dump(self.scalers[timeframe], f)
                    saved_count += 1
                
                if timeframe in self.feature_columns_dict:
                    temp_file = temp_path / f"{settings.SYMBOL}_{timeframe}_features.pkl"
                    with open(temp_file, 'wb') as f:
                        pickle.dump(self.feature_columns_dict[timeframe], f)
                    saved_count += 1
                
                # ğŸ”¥ åŸå­æ€§ç§»åŠ¨ï¼šä¸€æ¬¡æ€§æ›¿æ¢æ‰€æœ‰æ–‡ä»¶
                for temp_file in temp_path.glob(f"{settings.SYMBOL}_{timeframe}_*"):
                    target_file = model_dir / temp_file.name
                    # Windowsä¸‹ä½¿ç”¨replaceå®ç°åŸå­æ€§æ›¿æ¢
                    shutil.move(str(temp_file), str(target_file))
                
                logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹ä¿å­˜å®Œæˆï¼ˆ{saved_count}ä¸ªæ–‡ä»¶ï¼ŒåŸå­æ€§æ›´æ–°ï¼‰")
            
        except Exception as e:
            logger.error(f"ä¿å­˜é›†æˆæ¨¡å‹å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
    
    def _load_ensemble_models(self, timeframe: str) -> bool:
        """åŠ è½½é›†æˆæ¨¡å‹ï¼ˆæ”¯æŒInformer-2ï¼‰"""
        try:
            model_dir = Path(self.model_dir)  # ä½¿ç”¨çˆ¶ç±»çš„model_dir
            models = {}
            
            # ğŸ”‘ åŠ è½½ä¼ ç»Ÿæ¨¡å‹ï¼ˆå¿…éœ€ï¼‰
            model_mapping = {
                'lgb': 'lgb',
                'xgb': 'xgb',
                'cat': 'cat',
                'meta': 'meta'
            }
            
            # æ£€æŸ¥å¿…éœ€æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            for short_name in model_mapping:
                filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_{short_name}_model.pkl"
                if not filepath.exists():
                    logger.warning(f"âš ï¸ {timeframe} {short_name}æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                    return False
            
            # åŠ è½½æ‰€æœ‰ä¼ ç»Ÿæ¨¡å‹
            for short_name in model_mapping:
                filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_{short_name}_model.pkl"
                with open(filepath, 'rb') as f:
                    models[short_name] = pickle.load(f)
            
            # ğŸ¤– åŠ è½½Informer-2æ¨¡å‹ï¼ˆå¯é€‰ï¼Œå¦‚æœå­˜åœ¨ï¼‰
            if TORCH_AVAILABLE:
                inf_filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_inf_model.pt"
                if inf_filepath.exists():
                    with open(inf_filepath, 'rb') as f:
                        models['inf'] = pickle.load(f)
                    logger.info(f"   âœ… Informer-2æ¨¡å‹å·²åŠ è½½")
            
            self.ensemble_models[timeframe] = models
            
            # ğŸ”¥ åŠ è½½scalerå’Œfeaturesï¼ˆå…³é”®ï¼é¢„æµ‹æ—¶éœ€è¦ï¼‰
            scaler_path = model_dir / f"{settings.SYMBOL}_{timeframe}_scaler.pkl"
            if scaler_path.exists():
                with open(scaler_path, 'rb') as f:
                    self.scalers[timeframe] = pickle.load(f)
            
            features_path = model_dir / f"{settings.SYMBOL}_{timeframe}_features.pkl"
            if features_path.exists():
                with open(features_path, 'rb') as f:
                    self.feature_columns_dict[timeframe] = pickle.load(f)
            
            # ğŸ”“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ ‡è®°ä¸ºå°±ç»ª
            self.models_ready[timeframe] = True
            
            logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹åŠ è½½å®Œæˆï¼ˆ{len(models)}ä¸ªæ¨¡å‹ï¼‰")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½é›†æˆæ¨¡å‹å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return False
    
    async def train_model(self, force_retrain: bool = False) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œä½¿ç”¨Stackingé›†æˆï¼‰
        
        Args:
            force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒ
        
        Returns:
            è®­ç»ƒç»“æœå’Œå¹³å‡å‡†ç¡®ç‡
        """
        try:
            # è°ƒç”¨é›†æˆè®­ç»ƒ
            result = await self.train_all_timeframes()
            
            return {
                'accuracy': result['average_accuracy'],
                'timeframes': result['results'],
                'method': 'Stacking Ensemble',
                'models': ['LightGBM', 'XGBoost', 'CatBoost']
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def predict_with_optimizations(
        self,
        features: Dict[str, pd.DataFrame],
        price_data: Optional[pd.DataFrame] = None,
        previous_signal: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        å¸¦ä¼˜åŒ–çš„é›†æˆé¢„æµ‹
        
        Args:
            features: å„æ—¶é—´æ¡†æ¶ç‰¹å¾æ•°æ® {timeframe: DataFrame}
            price_data: ä»·æ ¼æ•°æ®ï¼ˆç”¨äºå¸‚åœºçŠ¶æ€åˆ†æï¼‰
            previous_signal: å‰ä¸€ä¸ªä¿¡å· (0=SHORT, 1=HOLD, 2=LONG)
        
        Returns:
            Dict[str, Any]: é¢„æµ‹ç»“æœå’Œä¼˜åŒ–ä¿¡æ¯
        """
        try:
            # 1. åŸºç¡€é¢„æµ‹
            predictions = {}
            probabilities = {}
            
            for timeframe, X in features.items():
                if timeframe in self.ensemble_models:
                    models = self.ensemble_models[timeframe]
                    
                    # ç¡®ä¿Xæ˜¯numpyæ•°ç»„æ ¼å¼
                    if isinstance(X, pd.DataFrame):
                        X_pred = X.values
                    else:
                        X_pred = X
                    
                    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                    if X_pred.size == 0:
                        logger.warning(f"âš ï¸ {timeframe} ç‰¹å¾æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡é¢„æµ‹")
                        continue
                    
                    # åŸºç¡€æ¨¡å‹é¢„æµ‹
                    lgb_pred = models['lgb'].predict(X_pred)
                    xgb_pred = models['xgb'].predict(X_pred)
                    cat_pred = models['cat'].predict(X_pred)
                    
                    # å…ƒå­¦ä¹ å™¨é¢„æµ‹
                    meta_features = self._generate_enhanced_meta_features(X_pred, models)
                    meta_pred = models['meta'].predict(meta_features)
                    meta_proba = models['meta'].predict_proba(meta_features)
                    
                    predictions[timeframe] = meta_pred[0]
                    probabilities[timeframe] = meta_proba[0]
            
            # 2. äº¤æ˜“æ–¹å‘ä¸€è‡´æ€§æ£€æŸ¥
            consistency_check = self.direction_checker.check_multi_timeframe_consistency(
                predictions, probabilities
            )
            
            # 3. é¢‘ç‡æ§åˆ¶æ£€æŸ¥
            frequency_control = None
            if price_data is not None:
                market_state = self.frequency_controller.calculate_market_state(price_data)
                recent_performance = self._get_recent_performance()
                
                frequency_control = self.frequency_controller.check_trade_frequency(
                    datetime.now(),
                    consistency_check.confidence_score,
                    market_state,
                    recent_performance
                )
            
            # 4. è‡´å‘½é”™è¯¯è¿‡æ»¤
            final_signal = predictions.get('15m', 1)  # é»˜è®¤HOLD
            filter_passed, filter_reason = self.direction_checker.filter_fatal_error_signals(
                final_signal, consistency_check, previous_signal
            )
            
            # 5. ç»¼åˆå†³ç­–
            if not filter_passed:
                final_signal = 1  # å¼ºåˆ¶HOLD
                logger.warning(f"âš ï¸ ä¿¡å·è¢«è¿‡æ»¤: {filter_reason}")
            
            if frequency_control and not frequency_control.allow_trade:
                final_signal = 1  # å¼ºåˆ¶HOLD
                logger.warning(f"âš ï¸ é¢‘ç‡æ§åˆ¶é˜»æ­¢äº¤æ˜“: {frequency_control.reason}")
            
            # 6. æ›´æ–°ä¼˜åŒ–æŒ‡æ ‡
            self._update_optimization_metrics(consistency_check, frequency_control)
            
            return {
                'signal': final_signal,
                'confidence': consistency_check.confidence_score,
                'consistency_check': {
                    'is_consistent': consistency_check.is_consistent,
                    'timeframe_agreement': consistency_check.timeframe_agreement,
                    'risk_level': consistency_check.risk_level
                },
                'frequency_control': {
                    'allow_trade': frequency_control.allow_trade if frequency_control else True,
                    'reason': frequency_control.reason if frequency_control else "æœªæ£€æŸ¥",
                    'fee_impact': frequency_control.fee_impact if frequency_control else 0.0
                },
                'filter_result': {
                    'passed': filter_passed,
                    'reason': filter_reason
                },
                'optimization_metrics': self.optimization_metrics.copy()
            }
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–é¢„æµ‹å¤±è´¥: {e}")
            return {
                'signal': 1,  # é»˜è®¤HOLD
                'confidence': 0.0,
                'error': str(e)
            }
    
    def _generate_enhanced_meta_features(
        self, 
        X_pred: np.ndarray, 
        models: Dict[str, Any]
    ) -> np.ndarray:
        """
        ç”Ÿæˆå¢å¼ºå…ƒç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        
        Args:
            X_pred: é¢„æµ‹ç‰¹å¾æ•°æ®
            models: æ¨¡å‹å­—å…¸
        
        Returns:
            np.ndarray: å¢å¼ºå…ƒç‰¹å¾
        """
        try:
            # åŸºç¡€æ¨¡å‹é¢„æµ‹æ¦‚ç‡
            lgb_proba = models['lgb'].predict_proba(X_pred)[0]
            xgb_proba = models['xgb'].predict_proba(X_pred)[0]
            cat_proba = models['cat'].predict_proba(X_pred)[0]
            
            # åŸºç¡€æ¨¡å‹é¢„æµ‹ç»“æœ
            lgb_pred = models['lgb'].predict(X_pred)[0]
            xgb_pred = models['xgb'].predict(X_pred)[0]
            cat_pred = models['cat'].predict(X_pred)[0]
            
            # Informer-2é¢„æµ‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'inf' in models:
                try:
                    # å°è¯•è·å–åºåˆ—è¾“å…¥ï¼ˆéœ€è¦ä»featuresä¸­æ„é€ ï¼‰
                    seq_len = self.seq_len_config.get('15m', 96)  # é»˜è®¤ä½¿ç”¨15mé…ç½®
                    # è¿™é‡Œéœ€è¦å®Œæ•´çš„åºåˆ—æ•°æ®ï¼Œæš‚æ—¶ä½¿ç”¨é»˜è®¤å€¼
                    inf_proba = np.array([0.33, 0.34, 0.33])  # é»˜è®¤å‡åŒ€åˆ†å¸ƒ
                    inf_pred = 1  # é»˜è®¤HOLD
                    logger.debug(f"âš ï¸ Informer-2ä½¿ç”¨é»˜è®¤é¢„æµ‹ï¼ˆéœ€è¦åºåˆ—è¾“å…¥ï¼‰")
                except Exception as e:
                    logger.warning(f"âš ï¸ Informer-2é¢„æµ‹å¤±è´¥: {e}")
                    inf_proba = np.array([0.33, 0.34, 0.33])
                    inf_pred = 1
            else:
                inf_proba = None
                inf_pred = None
            
            # 1. åŸºç¡€å…ƒç‰¹å¾ï¼ˆ12ä¸ªï¼‰
            meta_features = np.concatenate([
                lgb_proba,  # 3ä¸ª
                xgb_proba,  # 3ä¸ª
                cat_proba   # 3ä¸ª
            ])
            
            if inf_proba is not None:
                meta_features = np.concatenate([meta_features, inf_proba])  # +3ä¸ª
            
            # 2. å¢å¼ºå…ƒç‰¹å¾ï¼ˆ11ä¸ªï¼‰
            # æ¨¡å‹ä¸€è‡´æ€§
            if inf_pred is not None:
                agreement = float((lgb_pred == xgb_pred) and (xgb_pred == cat_pred) and (cat_pred == inf_pred))
            else:
                agreement = float((lgb_pred == xgb_pred) and (xgb_pred == cat_pred))
            
            # æœ€å¤§æ¦‚ç‡
            lgb_max_prob = lgb_proba.max()
            xgb_max_prob = xgb_proba.max()
            cat_max_prob = cat_proba.max()
            
            # æ¦‚ç‡ç†µ
            lgb_entropy = entr(lgb_proba).sum()
            xgb_entropy = entr(xgb_proba).sum()
            cat_entropy = entr(cat_proba).sum()
            
            # å¹³å‡æ¦‚ç‡
            avg_proba = np.mean([lgb_proba, xgb_proba, cat_proba], axis=0)
            if inf_proba is not None:
                avg_proba = np.mean([lgb_proba, xgb_proba, cat_proba, inf_proba], axis=0)
            
            # æ¦‚ç‡æ ‡å‡†å·®
            prob_std = np.std([lgb_proba, xgb_proba, cat_proba], axis=0).mean()
            if inf_proba is not None:
                prob_std = np.std([lgb_proba, xgb_proba, cat_proba, inf_proba], axis=0).mean()
            
            # Informer-2å¢å¼ºç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if inf_proba is not None:
                inf_max_prob = inf_proba.max()
                inf_entropy = entr(inf_proba).sum()
                
                enhanced_features = np.array([
                    agreement, lgb_max_prob, xgb_max_prob, cat_max_prob,
                    lgb_entropy, xgb_entropy, cat_entropy,
                    avg_proba.mean(), prob_std,
                    inf_max_prob, inf_entropy
                ])
            else:
                enhanced_features = np.array([
                    agreement, lgb_max_prob, xgb_max_prob, cat_max_prob,
                    lgb_entropy, xgb_entropy, cat_entropy,
                    avg_proba.mean(), prob_std,
                    0.0, 0.0  # å ä½ç¬¦
                ])
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            all_features = np.concatenate([meta_features, enhanced_features])
            
            return all_features.reshape(1, -1)
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºå…ƒç‰¹å¾ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾
            default_features = np.zeros(23)  # 12 + 11
            return default_features.reshape(1, -1)

    def _get_recent_performance(self) -> Dict[str, float]:
        """è·å–è¿‘æœŸè¡¨ç°æŒ‡æ ‡"""
        try:
            # è¿™é‡Œåº”è¯¥ä»å®é™…äº¤æ˜“è®°å½•ä¸­è·å–
            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return {
                'win_rate': 0.55,
                'avg_profit': 0.02,
                'max_drawdown': 0.05
            }
        except Exception as e:
            logger.error(f"âŒ è·å–è¿‘æœŸè¡¨ç°å¤±è´¥: {e}")
            return {
                'win_rate': 0.5,
                'avg_profit': 0.0,
                'max_drawdown': 0.0
            }
    
    def _update_optimization_metrics(
        self,
        consistency_check: ConsistencyCheck,
        frequency_control: Optional[FrequencyControl]
    ) -> None:
        """æ›´æ–°ä¼˜åŒ–æŒ‡æ ‡"""
        try:
            # æ›´æ–°è‡´å‘½é”™è¯¯ç‡
            self.optimization_metrics['fatal_error_rate'] = 1.0 - consistency_check.direction_strength
            
            # æ›´æ–°æ‰‹ç»­è´¹å½±å“
            if frequency_control:
                self.optimization_metrics['fee_impact'] = frequency_control.fee_impact
            
            # æ›´æ–°ä¸€è‡´æ€§ç‡
            self.optimization_metrics['consistency_rate'] = consistency_check.timeframe_agreement
            
            logger.debug(f"ğŸ“Š ä¼˜åŒ–æŒ‡æ ‡æ›´æ–°: è‡´å‘½é”™è¯¯ç‡={self.optimization_metrics['fatal_error_rate']:.3f}, "
                        f"æ‰‹ç»­è´¹å½±å“={self.optimization_metrics['fee_impact']:.3f}%, "
                        f"ä¸€è‡´æ€§ç‡={self.optimization_metrics['consistency_rate']:.3f}")
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–æŒ‡æ ‡æ›´æ–°å¤±è´¥: {e}")
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æŠ¥å‘Š"""
        try:
            # è·å–é¢‘ç‡æ§åˆ¶ç»Ÿè®¡
            freq_stats = self.frequency_controller.get_frequency_statistics()
            
            # è·å–ç¨³å®šæ€§å»ºè®®
            stability_recommendations = []
            if hasattr(self, 'stability_metrics'):
                stability_recommendations = self.stability_enhancer.get_stability_recommendations(
                    self.stability_metrics
                )
            
            return {
                'optimization_metrics': self.optimization_metrics.copy(),
                'frequency_statistics': freq_stats,
                'stability_recommendations': stability_recommendations,
                'system_status': {
                    'direction_checker': 'active',
                    'frequency_controller': 'active',
                    'stability_enhancer': 'active'
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return {
                'error': str(e),
                'optimization_metrics': self.optimization_metrics.copy()
            }

    async def start(self):
        """å¯åŠ¨é›†æˆMLæœåŠ¡"""
        try:
            logger.info("å¯åŠ¨Stackingé›†æˆæœºå™¨å­¦ä¹ æœåŠ¡...")
            
            # å°è¯•åŠ è½½å·²æœ‰é›†æˆæ¨¡å‹
            all_loaded = True
            for timeframe in settings.TIMEFRAMES:
                if not self._load_ensemble_models(timeframe):
                    all_loaded = False
                    break
            
            if all_loaded:
                logger.info("âœ… æ‰€æœ‰é›†æˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°é›†æˆæ¨¡å‹ï¼Œéœ€è¦è®­ç»ƒ")
            
            logger.info("Stackingé›†æˆMLæœåŠ¡å¯åŠ¨å®Œæˆï¼ˆè®­ç»ƒç”±schedulerç®¡ç†ï¼‰")
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆMLæœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
            raise

# å…¨å±€é›†æˆMLæœåŠ¡å®ä¾‹
ensemble_ml_service = EnsembleMLService()

