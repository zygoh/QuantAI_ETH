"""
é›†æˆæœºå™¨å­¦ä¹ æœåŠ¡ - Stackingä¸‰æ¨¡å‹èåˆ
"""
import logging
from typing import Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
import lightgbm as lgb
from datetime import datetime
import pickle
from pathlib import Path

from app.services.ml_service import MLService
from app.core.config import settings
from app.core.cache import cache_manager

logger = logging.getLogger(__name__)

class EnsembleMLService(MLService):
    """
    é›†æˆæœºå™¨å­¦ä¹ æœåŠ¡ï¼ˆStackingï¼‰
    
    ä½¿ç”¨LightGBM + XGBoost + CatBoostä¸‰æ¨¡å‹Stackingèåˆ
    ç›®æ ‡ï¼šå‡†ç¡®ç‡ä»42.81%æå‡åˆ°50%+
    """
    
    def __init__(self):
        super().__init__()
        
        # é›†æˆæ¨¡å‹å­—å…¸ {timeframe: {lgb, xgb, cat, meta}}
        self.ensemble_models = {}
        
        # é›†æˆæƒé‡ï¼ˆStackingè‡ªåŠ¨å­¦ä¹ ï¼Œè¿™é‡Œä½œä¸ºé™çº§æ–¹æ¡ˆï¼‰
        self.fallback_weights = {
            'lgb': 0.4,
            'xgb': 0.3,
            'cat': 0.3
        }
        
        logger.info("âœ… é›†æˆMLæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆStackingä¸‰æ¨¡å‹èåˆï¼‰")
    
    async def _prepare_diverse_training_data(self, timeframe: str, days_multiplier: float = 1.0) -> pd.DataFrame:
        """
        å‡†å¤‡å·®å¼‚åŒ–è®­ç»ƒæ•°æ®ï¼ˆä¸åŒå¤©æ•°ï¼‰
        
        Args:
            timeframe: æ—¶é—´æ¡†æ¶
            days_multiplier: å¤©æ•°å€æ•°ï¼ˆ1.0=æ ‡å‡†ï¼Œ1.5=+50%ï¼Œ2.0=+100%ï¼‰
        
        Returns:
            Kçº¿æ•°æ®DataFrame
        """
        try:
            from app.services.binance_client import binance_client
            
            symbol = settings.SYMBOL
            
            # åŸºç¡€è®­ç»ƒå¤©æ•°é…ç½®
            base_days_config = {
                '15m': 360,
                '2h': 270,
                '4h': 360
            }
            base_days = base_days_config.get(timeframe, 360)
            
            # åº”ç”¨å€æ•°
            training_days = int(base_days * days_multiplier)
            
            # è®¡ç®—éœ€è¦çš„Kçº¿æ•°é‡
            interval_minutes = {
                '15m': 15, '2h': 120, '4h': 240
            }
            minutes = interval_minutes.get(timeframe, 60)
            required_klines = int((training_days * 24 * 60) / minutes)
            
            logger.info(f"ğŸ“¥ è·å–{timeframe}æ•°æ®ï¼ˆÃ—{days_multiplier}å€ï¼‰: {required_klines}æ¡Kçº¿ ({training_days}å¤©)")
            
            # åˆ†æ‰¹è·å–
            all_klines = []
            batch_size = 1500
            batches_needed = (required_klines + batch_size - 1) // batch_size
            
            end_time = None
            for batch in range(batches_needed):
                remaining = required_klines - len(all_klines)
                batch_limit = min(batch_size, remaining)
                
                klines = binance_client.get_klines(
                    symbol=symbol,
                    interval=timeframe,
                    limit=batch_limit,
                    end_time=end_time
                )
                
                if not klines:
                    break
                
                all_klines.extend(klines)
                
                if len(klines) < batch_limit:
                    break
                
                end_time = klines[0]['timestamp'] - 1
            
            # è½¬æ¢ä¸ºDataFrameï¼ˆä¸ä¾èµ–reverseï¼Œç›´æ¥ç”¨æ—¶é—´æˆ³æ’åºï¼‰
            df = pd.DataFrame(all_klines)
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            
            # ğŸ”‘ å…³é”®ï¼šä¾èµ–æ—¶é—´æˆ³æ’åºï¼Œè€Œä¸æ˜¯å‡è®¾APIè¿”å›é¡ºåº
            df = df.sort_values('timestamp', ascending=True)  # æ˜ç¡®æŒ‡å®šå‡åºï¼ˆæ—§â†’æ–°ï¼‰
            df = df.drop_duplicates(subset=['timestamp'], keep='last')
            df = df.set_index('timestamp')
            
            logger.info(f"âœ… è·å–æˆåŠŸ: {len(df)}æ¡ï¼ˆÃ—{days_multiplier}å€æ•°æ®ï¼‰")
            
            return df
            
        except Exception as e:
            logger.error(f"å‡†å¤‡å·®å¼‚åŒ–è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            raise
    
    def _prepare_features_labels_reuse(self, df: pd.DataFrame, timeframe: str) -> Tuple[pd.DataFrame, pd.Series]:
        """
        å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå¤ç”¨å·²é€‰æ‹©çš„ç‰¹å¾åˆ—ï¼‰
        
        ç”¨é€”ï¼šä¸ºXGBoostå’ŒCatBoostå‡†å¤‡æ•°æ®æ—¶ï¼Œå¤ç”¨LightGBMå·²é€‰æ‹©çš„ç‰¹å¾åˆ—
        
        Args:
            df: åŒ…å«labelåˆ—çš„DataFrame
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            (X, y): ç‰¹å¾DataFrameå’Œæ ‡ç­¾Series
        """
        try:
            # ä½¿ç”¨å·²é€‰æ‹©çš„ç‰¹å¾åˆ—ï¼ˆLightGBMè®­ç»ƒæ—¶å·²ç¡®å®šï¼‰
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            
            if not feature_columns:
                logger.error(f"{timeframe} ç‰¹å¾åˆ—æœªæ‰¾åˆ°ï¼Œæ— æ³•å¤ç”¨")
                return pd.DataFrame(), pd.Series()
            
            X = df[feature_columns].copy()
            y = df['label'].copy()
            
            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            mask = ~(X.isna().any(axis=1) | y.isna())
            X = X[mask]
            y = y[mask]
            
            
            return X, y
            
        except Exception as e:
            logger.error(f"å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå¤ç”¨ï¼‰å¤±è´¥: {e}")
            return pd.DataFrame(), pd.Series()
    
    async def train_all_timeframes(self) -> Dict[str, Any]:
        """
        è®­ç»ƒæ‰€æœ‰æ—¶é—´æ¡†æ¶çš„é›†æˆæ¨¡å‹
        
        Returns:
            è®­ç»ƒç»“æœå’ŒæŒ‡æ ‡
        """
        try:
            logger.info("ğŸš€ å¼€å§‹Stackingé›†æˆæ¨¡å‹è®­ç»ƒ...")
            logger.info(f"ä¸‰æ¨¡å‹èåˆ: LightGBM + XGBoost + CatBoost")
            logger.info(f"æ—¶é—´æ¡†æ¶: {settings.TIMEFRAMES}")
            logger.info("")
            
            results = {}
            
            for timeframe in settings.TIMEFRAMES:
                logger.info("=" * 60)
                logger.info(f"ğŸ“Š è®­ç»ƒ {timeframe} é›†æˆæ¨¡å‹...")
                logger.info("=" * 60)
                
                result = await self._train_ensemble_single_timeframe(timeframe)
                results[timeframe] = result
                
                logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {result['accuracy']:.4f}")
                logger.info("")
            
            # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
            avg_accuracy = np.mean([r['accuracy'] for r in results.values()])
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ Stackingé›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
            logger.info(f"æˆåŠŸè®­ç»ƒ: {len(results)}/{len(settings.TIMEFRAMES)} ä¸ªæ—¶é—´æ¡†æ¶")
            logger.info(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
            logger.info("=" * 60)
            logger.info("")
            
            # ğŸ”‘ ä¿å­˜æ¨¡å‹æŒ‡æ ‡åˆ°Redisç¼“å­˜ï¼ˆä¾›health_monitorè¯»å–ï¼‰
            metrics_cache = {
                'accuracy': float(avg_accuracy),
                'timeframes': {tf: float(r['accuracy']) for tf, r in results.items()},
                'training_date': datetime.now().isoformat(),
                'method': 'Stacking Ensemble',
                'models': ['LightGBM', 'XGBoost', 'CatBoost']
            }
            await cache_manager.set_model_metrics(settings.SYMBOL, metrics_cache)
            
            return {
                'results': results,
                'average_accuracy': avg_accuracy,
                'training_date': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    async def _train_ensemble_single_timeframe(self, timeframe: str) -> Dict[str, Any]:
        """
        è®­ç»ƒå•ä¸ªæ—¶é—´æ¡†æ¶çš„Stackingé›†æˆæ¨¡å‹
        
        æ”¹è¿›ï¼šä¸‰ä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼Œå¢åŠ å¤šæ ·æ€§
        
        æµç¨‹:
        1. å‡†å¤‡ä¸‰ä»½ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼ˆä¸åŒå¤©æ•°ï¼‰
        2. è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆLightGBM, XGBoost, CatBoostï¼‰- å„ç”¨ä¸åŒæ•°æ®
        3. ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆåŸºç¡€æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ï¼‰
        4. è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰
        5. è¯„ä¼°é›†æˆæ•ˆæœ
        """
        try:
            # 1ï¸âƒ£ ä¸ºä¸‰ä¸ªæ¨¡å‹å‡†å¤‡ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
            logger.info(f"ğŸ“¥ ä¸ºä¸‰ä¸ªæ¨¡å‹å‡†å¤‡å·®å¼‚åŒ–è®­ç»ƒæ•°æ®...")
            
            # LightGBM: ä½¿ç”¨è¾ƒæ–°æ•°æ®ï¼ˆæ ‡å‡†å¤©æ•°ï¼‰
            data_lgb = await self._prepare_training_data_for_timeframe(timeframe)
            logger.info(f"âœ… LightGBMæ•°æ®: {len(data_lgb)}æ¡ï¼ˆæ ‡å‡†ï¼‰")
            
            # XGBoost: ä½¿ç”¨æ›´å¤šæ•°æ®ï¼ˆ+50%å¤©æ•°ï¼‰
            data_xgb = await self._prepare_diverse_training_data(timeframe, days_multiplier=1.5)
            logger.info(f"âœ… XGBoostæ•°æ®: {len(data_xgb)}æ¡ï¼ˆ+50%å¤©æ•°ï¼‰")
            
            # CatBoost: ä½¿ç”¨æœ€å¤šæ•°æ®ï¼ˆ+100%å¤©æ•°ï¼‰
            data_cat = await self._prepare_diverse_training_data(timeframe, days_multiplier=2.0)
            logger.info(f"âœ… CatBoostæ•°æ®: {len(data_cat)}æ¡ï¼ˆ+100%å¤©æ•°ï¼‰")
            
            # 2ï¸âƒ£ å¤„ç†ä¸‰ä»½æ•°æ®ï¼ˆç‰¹å¾å·¥ç¨‹ + æ ‡ç­¾ + ç‰¹å¾é€‰æ‹©ï¼‰
            logger.info(f"ğŸ”§ å¤„ç†ä¸‰ä»½è®­ç»ƒæ•°æ®...")
            
            # å¤„ç†LightGBMæ•°æ®
            data_lgb = self.feature_engineer.create_features(data_lgb)
            data_lgb = self._create_labels(data_lgb, timeframe=timeframe)
            X_lgb, y_lgb = self._prepare_features_labels(data_lgb, timeframe)
            X_lgb_scaled = self._scale_features(X_lgb, timeframe, fit=True)
            
            # å¤„ç†XGBoostæ•°æ®ï¼ˆå¤ç”¨åŒä¸€ä¸ªscalerï¼‰
            data_xgb = self.feature_engineer.create_features(data_xgb)
            data_xgb = self._create_labels(data_xgb, timeframe=timeframe)
            X_xgb, y_xgb = self._prepare_features_labels_reuse(data_xgb, timeframe)
            X_xgb_scaled = self._scale_features(X_xgb, timeframe, fit=False)
            
            # å¤„ç†CatBoostæ•°æ®ï¼ˆå¤ç”¨åŒä¸€ä¸ªscalerï¼‰
            data_cat = self.feature_engineer.create_features(data_cat)
            data_cat = self._create_labels(data_cat, timeframe=timeframe)
            X_cat, y_cat = self._prepare_features_labels_reuse(data_cat, timeframe)
            X_cat_scaled = self._scale_features(X_cat, timeframe, fit=False)
            
            logger.info(f"âœ… ä¸‰ä»½æ•°æ®å¤„ç†å®Œæˆ: LGB={len(X_lgb)}, XGB={len(X_xgb)}, CAT={len(X_cat)}")
            
            # 3ï¸âƒ£ æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆä½¿ç”¨æœ€çŸ­çš„æ•°æ®é•¿åº¦ä½œä¸ºéªŒè¯é›†åŸºå‡†ï¼‰
            min_len = min(len(X_lgb_scaled), len(X_xgb_scaled), len(X_cat_scaled))
            split_idx = int(min_len * 0.8)
            
            # ğŸ”‘ åˆ†å‰²æ•°æ®ï¼ˆå–æœ€æ–°çš„æ•°æ®ï¼Œä¿è¯æ—¶é—´å¯¹é½ï¼‰
            if isinstance(X_lgb_scaled, np.ndarray):
                X_lgb_train, X_lgb_val = X_lgb_scaled[-min_len:][:split_idx], X_lgb_scaled[-min_len:][split_idx:]
                X_xgb_train, X_xgb_val = X_xgb_scaled[-min_len:][:split_idx], X_xgb_scaled[-min_len:][split_idx:]
                X_cat_train, X_cat_val = X_cat_scaled[-min_len:][:split_idx], X_cat_scaled[-min_len:][split_idx:]
            else:
                X_lgb_train, X_lgb_val = X_lgb_scaled.iloc[-min_len:][:split_idx], X_lgb_scaled.iloc[-min_len:][split_idx:]
                X_xgb_train, X_xgb_val = X_xgb_scaled.iloc[-min_len:][:split_idx], X_xgb_scaled.iloc[-min_len:][split_idx:]
                X_cat_train, X_cat_val = X_cat_scaled.iloc[-min_len:][:split_idx], X_cat_scaled.iloc[-min_len:][split_idx:]
            
            y_lgb_train, y_lgb_val = y_lgb.iloc[-min_len:][:split_idx], y_lgb.iloc[-min_len:][split_idx:]
            y_xgb_train, y_xgb_val = y_xgb.iloc[-min_len:][:split_idx], y_xgb.iloc[-min_len:][split_idx:]
            y_cat_train, y_cat_val = y_cat.iloc[-min_len:][:split_idx], y_cat.iloc[-min_len:][split_idx:]
            
            logger.info(f"ğŸ“Š {timeframe} æ•°æ®åˆ†å‰²: è®­ç»ƒ{len(X_lgb_train)}æ¡ï¼ˆå¯¹é½åï¼‰, éªŒè¯{len(X_lgb_val)}æ¡")
            
            # 4ï¸âƒ£ è®­ç»ƒStackingé›†æˆæ¨¡å‹ï¼ˆä½¿ç”¨å·®å¼‚åŒ–æ•°æ®ï¼‰
            logger.info(f"ğŸš‚ å¼€å§‹è®­ç»ƒ {timeframe} Stackingé›†æˆï¼ˆå·®å¼‚åŒ–æ•°æ®ï¼‰...")
            ensemble_result = self._train_stacking_diverse(
                X_lgb_train, y_lgb_train, X_lgb_val, y_lgb_val,
                X_xgb_train, y_xgb_train, X_xgb_val, y_xgb_val,
                X_cat_train, y_cat_train, X_cat_val, y_cat_val,
                timeframe
            )
            
            # 8ï¸âƒ£ ä¿å­˜é›†æˆæ¨¡å‹
            self._save_ensemble_models(timeframe)
            
            logger.info(f"â±ï¸ {timeframe} è®­ç»ƒè€—æ—¶: {ensemble_result['training_time']:.2f}ç§’")
            
            return ensemble_result
            
        except Exception as e:
            logger.error(f"âŒ {timeframe} é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def _train_stacking_diverse(
        self,
        X_lgb_train, y_lgb_train, X_lgb_val, y_lgb_val,
        X_xgb_train, y_xgb_train, X_xgb_val, y_xgb_val,
        X_cat_train, y_cat_train, X_cat_val, y_cat_val,
        timeframe: str
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å·®å¼‚åŒ–æ•°æ®è®­ç»ƒStackingé›†æˆæ¨¡å‹
        
        Args:
            X_lgb_train, y_lgb_train: LightGBMè®­ç»ƒæ•°æ®
            X_lgb_val, y_lgb_val: LightGBMéªŒè¯æ•°æ®
            X_xgb_train, y_xgb_train: XGBoostè®­ç»ƒæ•°æ®
            X_xgb_val, y_xgb_val: XGBoostéªŒè¯æ•°æ®
            X_cat_train, y_cat_train: CatBoostè®­ç»ƒæ•°æ®
            X_cat_val, y_cat_val: CatBoostéªŒè¯æ•°æ®
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        import time
        start_time = time.time()
        
        try:
            # 1ï¸âƒ£ è®­ç»ƒä¸‰ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆå„ç”¨è‡ªå·±çš„æ•°æ®ï¼‰
            logger.info(f"ğŸš‚ è®­ç»ƒLightGBMï¼ˆ360å¤©æ•°æ®ï¼‰...")
            lgb_model = self._train_lightgbm(X_lgb_train, y_lgb_train, timeframe)
            
            logger.info(f"ğŸš‚ è®­ç»ƒXGBoostï¼ˆ540å¤©æ•°æ®ï¼‰...")
            xgb_model = self._train_xgboost(X_xgb_train, y_xgb_train, timeframe)
            
            logger.info(f"ğŸš‚ è®­ç»ƒCatBoostï¼ˆ720å¤©æ•°æ®ï¼‰...")
            cat_model = self._train_catboost(X_cat_train, y_cat_train, timeframe)
            
            # 2ï¸âƒ£ ç”ŸæˆéªŒè¯é›†çš„é¢„æµ‹æ¦‚ç‡ï¼ˆå…ƒç‰¹å¾ï¼‰
            logger.info(f"ğŸ“Š ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆåŸºäºå¯¹é½çš„éªŒè¯é›†ï¼‰...")
            
            # ä½¿ç”¨å„è‡ªçš„éªŒè¯é›†ç”Ÿæˆé¢„æµ‹
            lgb_pred_proba = lgb_model.predict_proba(X_lgb_val)
            xgb_pred_proba = xgb_model.predict_proba(X_xgb_val)
            cat_pred_proba = cat_model.predict_proba(X_cat_val)
            
            # æ‹¼æ¥å…ƒç‰¹å¾ï¼ˆä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ï¼‰
            meta_features_val = np.hstack([
                lgb_pred_proba,
                xgb_pred_proba,
                cat_pred_proba
            ])
            
            # å…ƒæ ‡ç­¾ï¼ˆä½¿ç”¨LightGBMçš„y_valï¼Œå› ä¸ºéªŒè¯é›†å·²å¯¹é½ï¼‰
            meta_labels_val = y_lgb_val
            
            logger.info(f"âœ… å…ƒç‰¹å¾ç”Ÿæˆå®Œæˆ: shape={meta_features_val.shape}")
            
            # 3ï¸âƒ£ è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰ - å‡çº§ä¸ºLightGBM + HOLDæƒ©ç½š
            logger.info(f"ğŸ§  è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆLightGBM - æ›´å¼ºå¤§çš„å†³ç­–èƒ½åŠ›ï¼‰...")
            
            # ğŸ”‘ å…ƒå­¦ä¹ å™¨ä¹Ÿéœ€è¦HOLDæƒ©ç½šï¼ˆå…³é”®ä¿®å¤ï¼ï¼‰
            from sklearn.utils.class_weight import compute_sample_weight
            meta_class_weights = compute_sample_weight('balanced', meta_labels_val)
            meta_hold_penalty = np.where(meta_labels_val == 1, 0.6, 1.0)  # å…ƒå­¦ä¹ å™¨HOLDæƒ©ç½šæ›´é‡ï¼ˆ0.6ï¼Œæ›´å¹³è¡¡ï¼‰
            meta_sample_weights = meta_class_weights * meta_hold_penalty
            
            import lightgbm as lgb
            meta_learner = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=4,  # æµ…å±‚æ ‘ï¼Œé¿å…è¿‡æ‹Ÿåˆ
                learning_rate=0.1,
                num_leaves=15,
                min_child_samples=20,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                random_state=42,
                verbose=-1
            )
            meta_learner.fit(meta_features_val, meta_labels_val, sample_weight=meta_sample_weights)
            
            logger.info(f"âœ… å…ƒå­¦ä¹ å™¨è®­ç»ƒå®Œæˆï¼ˆå·²åº”ç”¨HOLDæƒ©ç½š0.6ï¼Œæ›´å¹³è¡¡ï¼‰")
            
            # 4ï¸âƒ£ ä¿å­˜æ¨¡å‹åˆ°å­—å…¸
            if timeframe not in self.ensemble_models:
                self.ensemble_models[timeframe] = {}
            
            self.ensemble_models[timeframe]['lightgbm'] = lgb_model
            self.ensemble_models[timeframe]['xgboost'] = xgb_model
            self.ensemble_models[timeframe]['catboost'] = cat_model
            self.ensemble_models[timeframe]['meta_learner'] = meta_learner
            
            # 5ï¸âƒ£ è¯„ä¼°é›†æˆæ¨¡å‹
            ensemble_pred = meta_learner.predict(meta_features_val)
            
            from sklearn.metrics import accuracy_score, precision_recall_fscore_support
            
            accuracy = accuracy_score(meta_labels_val, ensemble_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(
                meta_labels_val, ensemble_pred, average='weighted', zero_division=0
            )
            
            # 6ï¸âƒ£ è¯„ä¼°å„åŸºç¡€æ¨¡å‹
            lgb_pred = lgb_model.predict(X_lgb_val)
            xgb_pred = xgb_model.predict(X_xgb_val)
            cat_pred = cat_model.predict(X_cat_val)
            
            lgb_acc = accuracy_score(y_lgb_val, lgb_pred)
            xgb_acc = accuracy_score(y_xgb_val, xgb_pred)
            cat_acc = accuracy_score(y_cat_val, cat_pred)
            
            training_time = time.time() - start_time
            
            result = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'lgb_accuracy': lgb_acc,
                'xgb_accuracy': xgb_acc,
                'cat_accuracy': cat_acc,
                'training_time': training_time,
                'ensemble_size': len(self.ensemble_models[timeframe]),
                'meta_features_shape': meta_features_val.shape
            }
            
            logger.info(f"âœ… Stackingè®­ç»ƒå®Œæˆï¼ˆå·®å¼‚åŒ–æ•°æ®ï¼‰:")
            logger.info(f"  LightGBM(360å¤©): {lgb_acc:.4f}")
            logger.info(f"  XGBoost(540å¤©):  {xgb_acc:.4f}")
            logger.info(f"  CatBoost(720å¤©): {cat_acc:.4f}")
            logger.info(f"  Stackingé›†æˆ:    {accuracy:.4f}")
            logger.info(f"  è®­ç»ƒè€—æ—¶: {training_time:.2f}ç§’")
            
            return result
            
        except Exception as e:
            logger.error(f"å·®å¼‚åŒ–Stackingè®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def _train_stacking_ensemble(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        timeframe: str
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒStackingé›†æˆæ¨¡å‹
        
        Stackingæ–¹æ³•:
        1. è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆLightGBM, XGBoost, CatBoostï¼‰
        2. ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆé¢„æµ‹æ¦‚ç‡ï¼‰
        3. è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆLogisticRegressionï¼‰å­¦ä¹ å¦‚ä½•ç»„åˆ
        
        ä¼˜åŠ¿:
        - æ¯”ç®€å•åŠ æƒæ›´æ™ºèƒ½
        - è‡ªåŠ¨å­¦ä¹ æ¯ä¸ªæ¨¡å‹çš„å¼ºé¡¹
        - æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
        """
        import time
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
        
        start_time = time.time()
        
        logger.info(f"ğŸ¯ Stage 1: è®­ç»ƒ3ä¸ªåŸºç¡€æ¨¡å‹...")
        
        # 1. è®­ç»ƒLightGBMï¼ˆåŸºç¡€æ¨¡å‹1ï¼‰
        logger.info(f"  ğŸ“Š è®­ç»ƒLightGBM...")
        lgb_model = self._train_lightgbm(X_train, y_train, timeframe)
        
        # 2. è®­ç»ƒXGBoostï¼ˆåŸºç¡€æ¨¡å‹2ï¼‰
        logger.info(f"  ğŸ“Š è®­ç»ƒXGBoost...")
        xgb_model = self._train_xgboost(X_train, y_train, timeframe)
        
        # 3. è®­ç»ƒCatBoostï¼ˆåŸºç¡€æ¨¡å‹3ï¼‰
        logger.info(f"  ğŸ“Š è®­ç»ƒCatBoost...")
        cat_model = self._train_catboost(X_train, y_train, timeframe)
        
        logger.info(f"âœ… 3ä¸ªåŸºç¡€æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # 4. ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆè®­ç»ƒé›†ï¼‰
        logger.info(f"ğŸ¯ Stage 2: ç”Ÿæˆå…ƒç‰¹å¾...")
        lgb_pred_train = lgb_model.predict_proba(X_train)
        xgb_pred_train = xgb_model.predict_proba(X_train)
        cat_pred_train = cat_model.predict_proba(X_train)
        
        # åˆå¹¶å…ƒç‰¹å¾ï¼ˆ9ç»´ï¼šæ¯ä¸ªæ¨¡å‹3ä¸ªç±»åˆ«æ¦‚ç‡ï¼‰
        meta_features_train = np.hstack([
            lgb_pred_train,
            xgb_pred_train,
            cat_pred_train
        ])
        
        # 5. è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰ - å‡çº§ä¸ºLightGBM + HOLDæƒ©ç½š
        logger.info(f"ğŸ¯ Stage 3: è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆLightGBM - æ›´å¼ºå¤§çš„å†³ç­–èƒ½åŠ›ï¼‰...")
        
        # ğŸ”‘ å…ƒå­¦ä¹ å™¨ä¹Ÿéœ€è¦HOLDæƒ©ç½šï¼ˆå…³é”®ä¿®å¤ï¼ï¼‰
        from sklearn.utils.class_weight import compute_sample_weight
        meta_class_weights = compute_sample_weight('balanced', y_train)
        meta_hold_penalty = np.where(y_train == 1, 0.6, 1.0)  # å…ƒå­¦ä¹ å™¨HOLDæƒ©ç½šæ›´é‡ï¼ˆ0.6ï¼Œæ›´å¹³è¡¡ï¼‰
        meta_sample_weights = meta_class_weights * meta_hold_penalty
        
        import lightgbm as lgb
        meta_learner = lgb.LGBMClassifier(
            n_estimators=100,
            max_depth=4,  # æµ…å±‚æ ‘ï¼Œé¿å…è¿‡æ‹Ÿåˆ
            learning_rate=0.1,
            num_leaves=15,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            verbose=-1
        )
        meta_learner.fit(meta_features_train, y_train, sample_weight=meta_sample_weights)
        
        logger.info(f"âœ… å…ƒå­¦ä¹ å™¨è®­ç»ƒå®Œæˆï¼ˆå·²åº”ç”¨HOLDæƒ©ç½š0.6ï¼Œæ›´å¹³è¡¡ï¼‰")
        
        # 6. éªŒè¯é›†è¯„ä¼°
        logger.info(f"ğŸ¯ Stage 4: éªŒè¯é›†è¯„ä¼°...")
        
        # ç”ŸæˆéªŒè¯é›†å…ƒç‰¹å¾
        lgb_pred_val = lgb_model.predict_proba(X_val)
        xgb_pred_val = xgb_model.predict_proba(X_val)
        cat_pred_val = cat_model.predict_proba(X_val)
        
        meta_features_val = np.hstack([
            lgb_pred_val,
            xgb_pred_val,
            cat_pred_val
        ])
        
        # Stackingé¢„æµ‹
        stacking_pred = meta_learner.predict(meta_features_val)
        stacking_proba = meta_learner.predict_proba(meta_features_val)
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_val, stacking_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_val, stacking_pred, average='weighted', zero_division=0
        )
        
        # è®¡ç®—AUCï¼ˆå¤šåˆ†ç±»ä½¿ç”¨OvRï¼‰
        try:
            auc = roc_auc_score(y_val, stacking_proba, multi_class='ovr', average='weighted')
        except:
            auc = 0.5
        
        # å„åŸºç¡€æ¨¡å‹å•ç‹¬å‡†ç¡®ç‡
        lgb_acc = accuracy_score(y_val, lgb_model.predict(X_val))
        xgb_acc = accuracy_score(y_val, xgb_model.predict(X_val))
        cat_acc = accuracy_score(y_val, cat_model.predict(X_val))
        
        training_time = time.time() - start_time
        
        # ä¿å­˜é›†æˆæ¨¡å‹
        self.ensemble_models[timeframe] = {
            'lgb': lgb_model,
            'xgb': xgb_model,
            'cat': cat_model,
            'meta': meta_learner
        }
        
        # æ—¥å¿—è¾“å‡º
        logger.info(f"ğŸ“Š {timeframe} Stackingé›†æˆè¯„ä¼°:")
        logger.info(f"  åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡:")
        logger.info(f"    LightGBM: {lgb_acc:.4f}")
        logger.info(f"    XGBoost:  {xgb_acc:.4f}")
        logger.info(f"    CatBoost: {cat_acc:.4f}")
        logger.info(f"  Stackingå‡†ç¡®ç‡: {accuracy:.4f}")
        logger.info(f"  æå‡: +{(accuracy - max(lgb_acc, xgb_acc, cat_acc))*100:.2f}%")
        logger.info(f"  ç²¾ç¡®ç‡: {precision:.4f}")
        logger.info(f"  å¬å›ç‡: {recall:.4f}")
        logger.info(f"  F1åˆ†æ•°: {f1:.4f}")
        logger.info(f"  AUC: {auc:.4f}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'base_models': {
                'lgb': lgb_acc,
                'xgb': xgb_acc,
                'cat': cat_acc
            },
            'training_time': training_time
        }
    
    def _train_lightgbm(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str):
        """
        è®­ç»ƒLightGBMæ¨¡å‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œç»Ÿä¸€ä¸‰æ¨¡å‹è®­ç»ƒä»£ç ä½ç½®ï¼‰
        
        è¦†ç›–åŸå› ï¼šä¿è¯ä»£ç ç»“æ„ç»Ÿä¸€ï¼Œä¸‰ä¸ªæ¨¡å‹è®­ç»ƒéƒ½åœ¨ensemble_ml_service.py
        """
        try:
            import lightgbm as lgb
            from sklearn.utils.class_weight import compute_sample_weight
            
            # æ ·æœ¬åŠ æƒï¼ˆç±»åˆ«å¹³è¡¡ Ã— æ—¶é—´è¡°å‡ Ã— HOLDæƒ©ç½šï¼‰
            class_weights = compute_sample_weight('balanced', y_train)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            
            # ğŸ”‘ HOLDç±»åˆ«é™æƒï¼ˆæƒ©ç½šè¿‡åº¦ä¿å®ˆï¼‰
            hold_penalty = np.where(y_train == 1, 0.7, 1.0)  # HOLDæƒé‡0.7ï¼Œå…¶ä»–1.0
            
            sample_weights = class_weights * time_decay * hold_penalty
            
            logger.info(f"âœ… æ ·æœ¬åŠ æƒå·²å¯ç”¨ï¼šç±»åˆ«å¹³è¡¡ Ã— æ—¶é—´è¡°å‡ Ã— HOLDæƒ©ç½š(0.7)")
            
            # è·å–æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–å‚æ•°
            timeframe_params = self.lgb_params_by_timeframe.get(timeframe, {})
            
            # åˆå¹¶åŸºç¡€å‚æ•°å’Œå·®å¼‚åŒ–å‚æ•°
            params = {**self.lgb_params, **timeframe_params}
            
            logger.info(f"ğŸ“Š {timeframe} LightGBMå‚æ•°: num_leaves={params.get('num_leaves')}, "
                       f"reg_alpha={params.get('reg_alpha', 0)}, reg_lambda={params.get('reg_lambda', 0)}")
            
            # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹ï¼ˆparamsä¸­å·²åŒ…å«random_state=42ï¼‰
            model = lgb.LGBMClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights)
            
            return model
            
        except Exception as e:
            logger.error(f"LightGBMè®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def _train_xgboost(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str):
        """è®­ç»ƒXGBoostæ¨¡å‹"""
        try:
            import xgboost as xgb
            
            # æ ·æœ¬åŠ æƒï¼ˆä¸LightGBMä¸€è‡´ + HOLDæƒ©ç½šï¼‰
            from sklearn.utils.class_weight import compute_sample_weight
            class_weights = compute_sample_weight('balanced', y_train)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            
            # ğŸ”‘ HOLDç±»åˆ«é™æƒï¼ˆæƒ©ç½šè¿‡åº¦ä¿å®ˆï¼‰
            hold_penalty = np.where(y_train == 1, 0.7, 1.0)
            
            sample_weights = class_weights * time_decay * hold_penalty
            
            params = {
                'max_depth': 6,
                'learning_rate': 0.05,
                'n_estimators': 300,
                'objective': 'multi:softprob',
                'num_class': 3,
                'eval_metric': 'mlogloss',
                'random_state': 42,
                'tree_method': 'hist',
                'reg_alpha': 0.3,
                'reg_lambda': 0.3,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            }
            
            model = xgb.XGBClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
            
            return model
            
        except Exception as e:
            logger.error(f"XGBoostè®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def _train_catboost(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str):
        """è®­ç»ƒCatBoostæ¨¡å‹"""
        try:
            from catboost import CatBoostClassifier
            
            # æ ·æœ¬åŠ æƒï¼ˆä¸LightGBMä¸€è‡´ + HOLDæƒ©ç½šï¼‰
            from sklearn.utils.class_weight import compute_sample_weight
            class_weights = compute_sample_weight('balanced', y_train)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            
            # ğŸ”‘ HOLDç±»åˆ«é™æƒï¼ˆæƒ©ç½šè¿‡åº¦ä¿å®ˆï¼‰
            hold_penalty = np.where(y_train == 1, 0.7, 1.0)
            
            sample_weights = class_weights * time_decay * hold_penalty
            
            params = {
                'iterations': 300,
                'learning_rate': 0.05,
                'depth': 6,
                'loss_function': 'MultiClass',
                'random_seed': 42,
                'verbose': False,
                'l2_leaf_reg': 3.0,
                'bootstrap_type': 'Bernoulli',  # æ”¹ç”¨Bernoulliï¼ˆæ”¯æŒsubsampleï¼‰
                'subsample': 0.8,
                'allow_writing_files': False  # ğŸ”‘ ç¦æ­¢ç”Ÿæˆcatboost_infoç›®å½•
            }
            
            model = CatBoostClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
            
            return model
            
        except Exception as e:
            logger.error(f"CatBoostè®­ç»ƒå¤±è´¥: {e}")
            raise
    
    async def predict(
        self, 
        data: pd.DataFrame, 
        timeframe: str
    ) -> Dict[str, Any]:
        """
        é›†æˆé¢„æµ‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼‰
        
        Args:
            data: Kçº¿æ•°æ®DataFrame
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        try:
            # æ£€æŸ¥é›†æˆæ¨¡å‹æ˜¯å¦å­˜åœ¨
            if timeframe not in self.ensemble_models:
                logger.warning(f"âš ï¸ {timeframe} é›†æˆæ¨¡å‹æœªè®­ç»ƒï¼Œé™çº§åˆ°å•æ¨¡å‹")
                return await super().predict(data, timeframe)
            
            # ç‰¹å¾å·¥ç¨‹
            processed_data = self.feature_engineer.create_features(data.copy())
            if processed_data.empty:
                return None
            
            # å‡†å¤‡ç‰¹å¾ï¼ˆä½¿ç”¨è¯¥æ—¶é—´æ¡†æ¶çš„ç‰¹å¾åˆ—ï¼‰
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            if not feature_columns:
                logger.error(f"{timeframe} ç‰¹å¾åˆ—æœªæ‰¾åˆ°")
                return None
            
            X = processed_data.iloc[-1:][feature_columns]
            if len(X) == 0:
                return None
            
            # ç‰¹å¾ç¼©æ”¾
            X_scaled = self._scale_features(X, timeframe, fit=False)
            
            # è·å–é›†æˆæ¨¡å‹
            models = self.ensemble_models[timeframe]
            
            # ä¸‰ä¸ªåŸºç¡€æ¨¡å‹é¢„æµ‹ï¼ˆX_scaledå¯èƒ½æ˜¯numpyæ•°ç»„æˆ–DataFrameï¼‰
            if isinstance(X_scaled, np.ndarray):
                X_pred = X_scaled
            else:
                X_pred = X_scaled.iloc[[-1]] if hasattr(X_scaled, 'iloc') else X_scaled
            
            # ğŸ”‘ åŸºç¡€æ¨¡å‹é¢„æµ‹ï¼ˆä½¿ç”¨å®Œæ•´é”®åï¼‰
            lgb_proba = models['lightgbm'].predict_proba(X_pred)[0]
            xgb_proba = models['xgboost'].predict_proba(X_pred)[0]
            cat_proba = models['catboost'].predict_proba(X_pred)[0]
            
            # Stackingé¢„æµ‹ï¼ˆä½¿ç”¨å…ƒå­¦ä¹ å™¨ï¼‰
            if 'meta_learner' in models:
                # ç”Ÿæˆå…ƒç‰¹å¾
                meta_features = np.hstack([lgb_proba, xgb_proba, cat_proba]).reshape(1, -1)
                
                # å…ƒå­¦ä¹ å™¨é¢„æµ‹
                stacking_proba = models['meta_learner'].predict_proba(meta_features)[0]
                final_pred = stacking_proba.argmax()
                confidence = stacking_proba[final_pred]
                final_probabilities = stacking_proba  # ä½¿ç”¨å…ƒå­¦ä¹ å™¨æ¦‚ç‡
            else:
                # é™çº§ï¼šç®€å•åŠ æƒå¹³å‡ï¼ˆå¦‚æœå…ƒå­¦ä¹ å™¨ä¸å­˜åœ¨ï¼‰
                weights = self.fallback_weights
                ensemble_proba = (
                    lgb_proba * weights['lgb'] +
                    xgb_proba * weights['xgb'] +
                    cat_proba * weights['cat']
                )
                final_pred = ensemble_proba.argmax()
                confidence = ensemble_proba[final_pred]
                final_probabilities = ensemble_proba  # ä½¿ç”¨åŠ æƒå¹³å‡æ¦‚ç‡
            
            # æ˜ å°„åˆ°ä¿¡å·ç±»å‹
            signal_map = {0: 'SHORT', 1: 'HOLD', 2: 'LONG'}
            signal_type = signal_map[final_pred]
            
            # ç®€æ´è®°å½•é¢„æµ‹ç»“æœ
            from app.utils.helpers import format_signal_type
            logger.info(f"ğŸ¯ {timeframe} Stackingé¢„æµ‹: {format_signal_type(signal_type)} "
                       f"(ç½®ä¿¡åº¦={confidence:.4f}, æ¦‚ç‡: ğŸ“‰{final_probabilities[0]:.2f} â¸ï¸{final_probabilities[1]:.2f} ğŸ“ˆ{final_probabilities[2]:.2f})")
            
            # è¿”å›å€¼æ ¼å¼ä¸çˆ¶ç±»ä¸€è‡´
            return {
                'signal_type': signal_type,
                'confidence': float(confidence),
                'probabilities': {
                    'short': float(final_probabilities[0]),
                    'hold': float(final_probabilities[1]),
                    'long': float(final_probabilities[2])
                },
                'timestamp': datetime.now(),
                'model_version': '2.0_stacking_ensemble'
            }
            
        except Exception as e:
            logger.error(f"é›†æˆé¢„æµ‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {}
    
    def _save_ensemble_models(self, timeframe: str):
        """ä¿å­˜é›†æˆæ¨¡å‹"""
        try:
            models = self.ensemble_models[timeframe]
            model_dir = Path(self.model_dir)  # ä½¿ç”¨çˆ¶ç±»çš„model_dir
            model_dir.mkdir(parents=True, exist_ok=True)
            
            # ğŸ”‘ ä¿å­˜4ä¸ªæ¨¡å‹ï¼ˆä¿®å¤é”®åæ˜ å°„ï¼‰
            model_mapping = {
                'lightgbm': 'lgb',
                'xgboost': 'xgb',
                'catboost': 'cat',
                'meta_learner': 'meta'
            }
            
            saved_count = 0
            for full_name, short_name in model_mapping.items():
                if full_name in models:
                    filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_{short_name}_model.pkl"
                    with open(filepath, 'wb') as f:
                        pickle.dump(models[full_name], f)
                    saved_count += 1
            
            # ğŸ”¥ ä¿å­˜scalerå’Œfeaturesï¼ˆå…³é”®ï¼é¢„æµ‹æ—¶éœ€è¦ï¼‰
            if timeframe in self.scalers:
                scaler_path = model_dir / f"{settings.SYMBOL}_{timeframe}_scaler.pkl"
                with open(scaler_path, 'wb') as f:
                    pickle.dump(self.scalers[timeframe], f)
                saved_count += 1
            
            if timeframe in self.feature_columns_dict:
                features_path = model_dir / f"{settings.SYMBOL}_{timeframe}_features.pkl"
                with open(features_path, 'wb') as f:
                    pickle.dump(self.feature_columns_dict[timeframe], f)
                saved_count += 1
            
            if saved_count > 0:
                logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹ä¿å­˜å®Œæˆï¼ˆ{saved_count}ä¸ªæ–‡ä»¶ï¼‰")
            else:
                logger.warning(f"âš ï¸ {timeframe} æ²¡æœ‰æ¨¡å‹è¢«ä¿å­˜ï¼ˆé”®å: {list(models.keys())}ï¼‰")
            
        except Exception as e:
            logger.error(f"ä¿å­˜é›†æˆæ¨¡å‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def _load_ensemble_models(self, timeframe: str) -> bool:
        """åŠ è½½é›†æˆæ¨¡å‹"""
        try:
            model_dir = Path(self.model_dir)  # ä½¿ç”¨çˆ¶ç±»çš„model_dir
            models = {}
            
            # ğŸ”‘ åŠ è½½4ä¸ªæ¨¡å‹ï¼ˆä¿®å¤é”®åæ˜ å°„ï¼‰
            model_mapping = {
                'lightgbm': 'lgb',
                'xgboost': 'xgb',
                'catboost': 'cat',
                'meta_learner': 'meta'
            }
            
            # æ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            for full_name, short_name in model_mapping.items():
                filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_{short_name}_model.pkl"
                
                if not filepath.exists():
                    logger.warning(f"âš ï¸ {timeframe} {short_name}æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                    return False
            
            # åŠ è½½æ‰€æœ‰æ¨¡å‹
            for full_name, short_name in model_mapping.items():
                filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_{short_name}_model.pkl"
                with open(filepath, 'rb') as f:
                    models[full_name] = pickle.load(f)  # ğŸ”‘ ä½¿ç”¨å®Œæ•´é”®å
            
            self.ensemble_models[timeframe] = models
            
            # ğŸ”¥ åŠ è½½scalerå’Œfeaturesï¼ˆå…³é”®ï¼é¢„æµ‹æ—¶éœ€è¦ï¼‰
            scaler_path = model_dir / f"{settings.SYMBOL}_{timeframe}_scaler.pkl"
            if scaler_path.exists():
                with open(scaler_path, 'rb') as f:
                    self.scalers[timeframe] = pickle.load(f)
            
            features_path = model_dir / f"{settings.SYMBOL}_{timeframe}_features.pkl"
            if features_path.exists():
                with open(features_path, 'rb') as f:
                    self.feature_columns_dict[timeframe] = pickle.load(f)
            
            logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹åŠ è½½å®Œæˆï¼ˆ{len(models)}ä¸ªæ¨¡å‹ï¼‰")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½é›†æˆæ¨¡å‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    async def train_model(self, force_retrain: bool = False) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œä½¿ç”¨Stackingé›†æˆï¼‰
        
        Args:
            force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒ
        
        Returns:
            è®­ç»ƒç»“æœå’Œå¹³å‡å‡†ç¡®ç‡
        """
        try:
            # è°ƒç”¨é›†æˆè®­ç»ƒ
            result = await self.train_all_timeframes()
            
            return {
                'accuracy': result['average_accuracy'],
                'timeframes': result['results'],
                'method': 'Stacking Ensemble',
                'models': ['LightGBM', 'XGBoost', 'CatBoost']
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    async def start(self):
        """å¯åŠ¨é›†æˆMLæœåŠ¡"""
        try:
            logger.info("å¯åŠ¨Stackingé›†æˆæœºå™¨å­¦ä¹ æœåŠ¡...")
            
            # å°è¯•åŠ è½½å·²æœ‰é›†æˆæ¨¡å‹
            all_loaded = True
            for timeframe in settings.TIMEFRAMES:
                if not self._load_ensemble_models(timeframe):
                    all_loaded = False
                    break
            
            if all_loaded:
                logger.info("âœ… æ‰€æœ‰é›†æˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°é›†æˆæ¨¡å‹ï¼Œéœ€è¦è®­ç»ƒ")
            
            logger.info("Stackingé›†æˆMLæœåŠ¡å¯åŠ¨å®Œæˆï¼ˆè®­ç»ƒç”±schedulerç®¡ç†ï¼‰")
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆMLæœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
            raise

# å…¨å±€é›†æˆMLæœåŠ¡å®ä¾‹
ensemble_ml_service = EnsembleMLService()

