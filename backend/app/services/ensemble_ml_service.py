"""
é›†æˆæœºå™¨å­¦ä¹ æœåŠ¡ - Stackingä¸‰æ¨¡å‹èåˆ
"""
import logging
from typing import Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
import lightgbm as lgb
from datetime import datetime
import pickle
from pathlib import Path

from app.services.ml_service import MLService
from app.core.config import settings
from app.core.cache import cache_manager
from app.services.hyperparameter_optimizer import HyperparameterOptimizer

logger = logging.getLogger(__name__)

# æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆPyTorchï¼‰
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, TensorDataset
    from app.services.informer2_model import Informer2ForClassification
    from app.services.gmadl_loss import GMADLossWithHOLDPenalty
    TORCH_AVAILABLE = True
    logger.info("âœ… PyTorchå·²åŠ è½½ï¼ŒInformer-2æ¨¡å‹å¯ç”¨")
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("âš ï¸ PyTorchæœªå®‰è£…ï¼ŒInformer-2æ¨¡å‹å°†ä¸å¯ç”¨")

class EnsembleMLService(MLService):
    """
    é›†æˆæœºå™¨å­¦ä¹ æœåŠ¡ï¼ˆStackingï¼‰
    
    ä½¿ç”¨LightGBM + XGBoost + CatBoost + Informer-2 å››æ¨¡å‹Stackingèåˆ
    ç›®æ ‡ï¼šå‡†ç¡®ç‡ä»37%æå‡åˆ°50%+
    
    Phase 1: æ—¶é—´åºåˆ—CV + å…ƒç‰¹å¾ + HOLDæƒ©ç½š + é˜²è¿‡æ‹Ÿåˆ
    Phase 2A: 82ä¸ªé«˜çº§æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
    Phase 2B: Optunaè¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–
    Phase 3: Informer-2æ·±åº¦å­¦ä¹  + GMADLæŸå¤±å‡½æ•°
    """
    
    def __init__(self):
        super().__init__()
        
        # é›†æˆæ¨¡å‹å­—å…¸ {timeframe: {lgb, xgb, cat, inf, meta}}
        self.ensemble_models = {}
        
        # é›†æˆæƒé‡ï¼ˆStackingè‡ªåŠ¨å­¦ä¹ ï¼Œè¿™é‡Œä½œä¸ºé™çº§æ–¹æ¡ˆï¼‰
        self.fallback_weights = {
            'lgb': 0.4,
            'xgb': 0.3,
            'cat': 0.3
        }
        
        # ğŸ”§ è¶…å‚æ•°ä¼˜åŒ–é…ç½®
        self.enable_hyperparameter_tuning = True  # âœ… å·²å¯ç”¨ï¼ˆPhase 2Bï¼‰
        self.optimize_all_models = True  # âœ… GPUåŠ é€Ÿä¸‹ä¼˜åŒ–æ‰€æœ‰æ¨¡å‹
        self.optimize_informer2 = True  # âœ… ä¼˜åŒ–Informer-2ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰
        self.optuna_n_trials = 100  # Optunaè¯•éªŒæ¬¡æ•°ï¼ˆä¼ ç»Ÿæ¨¡å‹ï¼‰
        self.informer_n_trials = 50  # Informer-2è¯•éªŒæ¬¡æ•°ï¼ˆå‡å°‘ä»¥æ§åˆ¶æ—¶é—´ï¼‰
        self.optuna_timeout = 1800  # è¶…æ—¶30åˆ†é’Ÿï¼ˆGPUåŠ é€Ÿä¸‹è¶³å¤Ÿä¼˜åŒ–3ä¸ªæ¨¡å‹ï¼‰
        self.informer_timeout = 1200  # Informer-2è¶…æ—¶20åˆ†é’Ÿ
        
        # ğŸ¤– Informer-2æ·±åº¦å­¦ä¹ é…ç½®
        self.enable_informer2 = True  # âœ… å·²å¯ç”¨ï¼ˆPhase 3 - ç¥ç»ç½‘ç»œï¼‰
        self.informer_d_model = 128  # æ¨¡å‹ç»´åº¦
        self.informer_n_heads = 8  # æ³¨æ„åŠ›å¤´æ•°
        self.informer_n_layers = 3  # Encoderå±‚æ•°
        self.informer_epochs = 50  # è®­ç»ƒè½®æ•°ï¼ˆGPUåŠ é€Ÿï¼‰
        self.informer_batch_size = 256  # æ‰¹æ¬¡å¤§å°
        self.informer_lr = 0.001  # å­¦ä¹ ç‡
        
        # ğŸ® GPUé…ç½®ï¼ˆä»configè¯»å–ï¼‰
        self.use_gpu = settings.USE_GPU
        self.gpu_device = settings.GPU_DEVICE
        
        logger.info("âœ… é›†æˆMLæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆStackingå››æ¨¡å‹èåˆ + æ·±åº¦å­¦ä¹ ï¼‰")
        logger.info(f"   è¶…å‚æ•°ä¼˜åŒ–: {'å¯ç”¨' if self.enable_hyperparameter_tuning else 'å…³é—­'}")
        logger.info(f"   Informer-2ç¥ç»ç½‘ç»œ: {'å¯ç”¨' if self.enable_informer2 else 'å…³é—­'}")
        logger.info(f"   GPUåŠ é€Ÿ: {'å¯ç”¨' if self.use_gpu else 'å…³é—­'} (è®¾å¤‡: {self.gpu_device if self.use_gpu else 'CPU'})")
    
    async def _prepare_diverse_training_data(self, timeframe: str, days_multiplier: float = 1.0) -> pd.DataFrame:
        """
        å‡†å¤‡å·®å¼‚åŒ–è®­ç»ƒæ•°æ®ï¼ˆä¸åŒå¤©æ•°ï¼‰
        
        Args:
            timeframe: æ—¶é—´æ¡†æ¶
            days_multiplier: å¤©æ•°å€æ•°ï¼ˆ1.0=æ ‡å‡†ï¼Œ1.5=+50%ï¼Œ2.0=+100%ï¼‰
        
        Returns:
            Kçº¿æ•°æ®DataFrame
        """
        try:
            from app.services.binance_client import binance_client
            
            symbol = settings.SYMBOL
            
            # ğŸ”‘ åŸºç¡€è®­ç»ƒå¤©æ•°é…ç½®ï¼ˆ2h/4hå¢åŠ æ•°æ®é‡é˜²è¿‡æ‹Ÿåˆï¼‰
            base_days_config = {
                '15m': 360,  # ä¿æŒ360å¤©
                '2h': 540,   # 270â†’540ï¼ˆç¿»å€ï¼‰
                '4h': 720    # 360â†’720ï¼ˆç¿»å€ï¼‰
            }
            base_days = base_days_config.get(timeframe, 360)
            
            # åº”ç”¨å€æ•°
            training_days = int(base_days * days_multiplier)
            
            # è®¡ç®—éœ€è¦çš„Kçº¿æ•°é‡
            interval_minutes = {
                '15m': 15, '2h': 120, '4h': 240
            }
            minutes = interval_minutes.get(timeframe, 60)
            required_klines = int((training_days * 24 * 60) / minutes)
            
            logger.info(f"ğŸ“¥ è·å–{timeframe}æ•°æ®ï¼ˆÃ—{days_multiplier}å€ï¼‰: {required_klines}æ¡Kçº¿ ({training_days}å¤©)")
            
            # åˆ†æ‰¹è·å–
            all_klines = []
            batch_size = 1500
            batches_needed = (required_klines + batch_size - 1) // batch_size
            
            end_time = None
            for batch in range(batches_needed):
                remaining = required_klines - len(all_klines)
                batch_limit = min(batch_size, remaining)
                
                klines = binance_client.get_klines(
                    symbol=symbol,
                    interval=timeframe,
                    limit=batch_limit,
                    end_time=end_time
                )
                
                if not klines:
                    break
                
                all_klines.extend(klines)
                
                if len(klines) < batch_limit:
                    break
                
                end_time = klines[0]['timestamp'] - 1
            
            # è½¬æ¢ä¸ºDataFrameï¼ˆä¸ä¾èµ–reverseï¼Œç›´æ¥ç”¨æ—¶é—´æˆ³æ’åºï¼‰
            df = pd.DataFrame(all_klines)
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            
            # ğŸ”‘ å…³é”®ï¼šä¾èµ–æ—¶é—´æˆ³æ’åºï¼Œè€Œä¸æ˜¯å‡è®¾APIè¿”å›é¡ºåº
            df = df.sort_values('timestamp', ascending=True)  # æ˜ç¡®æŒ‡å®šå‡åºï¼ˆæ—§â†’æ–°ï¼‰
            df = df.drop_duplicates(subset=['timestamp'], keep='last')
            df = df.set_index('timestamp')
            
            logger.info(f"âœ… è·å–æˆåŠŸ: {len(df)}æ¡ï¼ˆÃ—{days_multiplier}å€æ•°æ®ï¼‰")
            
            return df
            
        except Exception as e:
            logger.error(f"å‡†å¤‡å·®å¼‚åŒ–è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            raise
    
    def _prepare_features_labels_reuse(self, df: pd.DataFrame, timeframe: str) -> Tuple[pd.DataFrame, pd.Series]:
        """
        å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå¤ç”¨å·²é€‰æ‹©çš„ç‰¹å¾åˆ—ï¼‰
        
        ç”¨é€”ï¼šä¸ºXGBoostå’ŒCatBoostå‡†å¤‡æ•°æ®æ—¶ï¼Œå¤ç”¨LightGBMå·²é€‰æ‹©çš„ç‰¹å¾åˆ—
        
        Args:
            df: åŒ…å«labelåˆ—çš„DataFrame
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            (X, y): ç‰¹å¾DataFrameå’Œæ ‡ç­¾Series
        """
        try:
            # ä½¿ç”¨å·²é€‰æ‹©çš„ç‰¹å¾åˆ—ï¼ˆLightGBMè®­ç»ƒæ—¶å·²ç¡®å®šï¼‰
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            
            if not feature_columns:
                logger.error(f"{timeframe} ç‰¹å¾åˆ—æœªæ‰¾åˆ°ï¼Œæ— æ³•å¤ç”¨")
                return pd.DataFrame(), pd.Series()
            
            X = df[feature_columns].copy()
            y = df['label'].copy()
            
            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            mask = ~(X.isna().any(axis=1) | y.isna())
            X = X[mask]
            y = y[mask]
            
            
            return X, y
            
        except Exception as e:
            logger.error(f"å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå¤ç”¨ï¼‰å¤±è´¥: {e}")
            return pd.DataFrame(), pd.Series()
    
    async def train_all_timeframes(self) -> Dict[str, Any]:
        """
        è®­ç»ƒæ‰€æœ‰æ—¶é—´æ¡†æ¶çš„é›†æˆæ¨¡å‹
        
        Returns:
            è®­ç»ƒç»“æœå’ŒæŒ‡æ ‡
        """
        try:
            logger.info("ğŸš€ å¼€å§‹Stackingé›†æˆæ¨¡å‹è®­ç»ƒ...")
            if self.enable_informer2 and TORCH_AVAILABLE:
                logger.info(f"âœ¨ å››æ¨¡å‹èåˆ: LightGBM + XGBoost + CatBoost + Informer-2 (GMADLæŸå¤±)")
                logger.info(f"   è¶…å‚æ•°ä¼˜åŒ–: {'å¯ç”¨' if self.enable_hyperparameter_tuning else 'å…³é—­'}")
                logger.info(f"   æ·±åº¦å­¦ä¹ : GPU {'å¯ç”¨' if torch.cuda.is_available() else 'ä¸å¯ç”¨'}")
            else:
                logger.info(f"ä¸‰æ¨¡å‹èåˆ: LightGBM + XGBoost + CatBoost")
            logger.info(f"æ—¶é—´æ¡†æ¶: {settings.TIMEFRAMES}")
            logger.info("")
            
            results = {}
            
            for timeframe in settings.TIMEFRAMES:
                logger.info("=" * 60)
                logger.info(f"ğŸ“Š è®­ç»ƒ {timeframe} é›†æˆæ¨¡å‹...")
                logger.info("=" * 60)
                
                result = await self._train_ensemble_single_timeframe(timeframe)
                results[timeframe] = result
                
                logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {result['accuracy']:.4f}")
                logger.info("")
            
            # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
            avg_accuracy = np.mean([r['accuracy'] for r in results.values()])
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ Stackingé›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
            logger.info(f"æˆåŠŸè®­ç»ƒ: {len(results)}/{len(settings.TIMEFRAMES)} ä¸ªæ—¶é—´æ¡†æ¶")
            logger.info(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
            logger.info("=" * 60)
            logger.info("")
            
            # ğŸ”‘ ä¿å­˜æ¨¡å‹æŒ‡æ ‡åˆ°Redisç¼“å­˜ï¼ˆä¾›health_monitorè¯»å–ï¼‰
            metrics_cache = {
                'accuracy': float(avg_accuracy),
                'timeframes': {tf: float(r['accuracy']) for tf, r in results.items()},
                'training_date': datetime.now().isoformat(),
                'method': 'Stacking Ensemble',
                'models': ['LightGBM', 'XGBoost', 'CatBoost']
            }
            await cache_manager.set_model_metrics(settings.SYMBOL, metrics_cache)
            
            return {
                'results': results,
                'average_accuracy': avg_accuracy,
                'training_date': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    async def _train_ensemble_single_timeframe(self, timeframe: str) -> Dict[str, Any]:
        """
        è®­ç»ƒå•ä¸ªæ—¶é—´æ¡†æ¶çš„Stackingé›†æˆæ¨¡å‹
        
        æ”¹è¿›ï¼šä¸‰ä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼Œå¢åŠ å¤šæ ·æ€§
        
        æµç¨‹:
        1. å‡†å¤‡ä¸‰ä»½ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼ˆä¸åŒå¤©æ•°ï¼‰
        2. è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆLightGBM, XGBoost, CatBoostï¼‰- å„ç”¨ä¸åŒæ•°æ®
        3. ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆåŸºç¡€æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ï¼‰
        4. è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰
        5. è¯„ä¼°é›†æˆæ•ˆæœ
        """
        try:
            # 1ï¸âƒ£ ä¸ºä¸‰ä¸ªæ¨¡å‹å‡†å¤‡ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
            logger.info(f"ğŸ“¥ ä¸ºä¸‰ä¸ªæ¨¡å‹å‡†å¤‡å·®å¼‚åŒ–è®­ç»ƒæ•°æ®...")
            
            # LightGBM: ä½¿ç”¨è¾ƒæ–°æ•°æ®ï¼ˆæ ‡å‡†å¤©æ•°ï¼‰
            data_lgb = await self._prepare_training_data_for_timeframe(timeframe)
            logger.info(f"âœ… LightGBMæ•°æ®: {len(data_lgb)}æ¡ï¼ˆæ ‡å‡†ï¼‰")
            
            # XGBoost: ä½¿ç”¨æ›´å¤šæ•°æ®ï¼ˆ+50%å¤©æ•°ï¼‰
            data_xgb = await self._prepare_diverse_training_data(timeframe, days_multiplier=1.5)
            logger.info(f"âœ… XGBoostæ•°æ®: {len(data_xgb)}æ¡ï¼ˆ+50%å¤©æ•°ï¼‰")
            
            # CatBoost: ä½¿ç”¨æœ€å¤šæ•°æ®ï¼ˆ+100%å¤©æ•°ï¼‰
            data_cat = await self._prepare_diverse_training_data(timeframe, days_multiplier=2.0)
            logger.info(f"âœ… CatBoostæ•°æ®: {len(data_cat)}æ¡ï¼ˆ+100%å¤©æ•°ï¼‰")
            
            # 2ï¸âƒ£ å¤„ç†ä¸‰ä»½æ•°æ®ï¼ˆç‰¹å¾å·¥ç¨‹ + æ ‡ç­¾ + ç‰¹å¾é€‰æ‹©ï¼‰
            logger.info(f"ğŸ”§ å¤„ç†ä¸‰ä»½è®­ç»ƒæ•°æ®...")
            
            # å¤„ç†LightGBMæ•°æ®
            data_lgb = self.feature_engineer.create_features(data_lgb)
            data_lgb = self._create_labels(data_lgb, timeframe=timeframe)
            X_lgb, y_lgb = self._prepare_features_labels(data_lgb, timeframe)
            X_lgb_scaled = self._scale_features(X_lgb, timeframe, fit=True)
            
            # å¤„ç†XGBoostæ•°æ®ï¼ˆå¤ç”¨åŒä¸€ä¸ªscalerï¼‰
            data_xgb = self.feature_engineer.create_features(data_xgb)
            data_xgb = self._create_labels(data_xgb, timeframe=timeframe)
            X_xgb, y_xgb = self._prepare_features_labels_reuse(data_xgb, timeframe)
            X_xgb_scaled = self._scale_features(X_xgb, timeframe, fit=False)
            
            # å¤„ç†CatBoostæ•°æ®ï¼ˆå¤ç”¨åŒä¸€ä¸ªscalerï¼‰
            data_cat = self.feature_engineer.create_features(data_cat)
            data_cat = self._create_labels(data_cat, timeframe=timeframe)
            X_cat, y_cat = self._prepare_features_labels_reuse(data_cat, timeframe)
            X_cat_scaled = self._scale_features(X_cat, timeframe, fit=False)
            
            logger.info(f"âœ… ä¸‰ä»½æ•°æ®å¤„ç†å®Œæˆ: LGB={len(X_lgb)}, XGB={len(X_xgb)}, CAT={len(X_cat)}")
            
            # 3ï¸âƒ£ æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆä½¿ç”¨æœ€çŸ­çš„æ•°æ®é•¿åº¦ä½œä¸ºéªŒè¯é›†åŸºå‡†ï¼‰
            min_len = min(len(X_lgb_scaled), len(X_xgb_scaled), len(X_cat_scaled))
            split_idx = int(min_len * 0.8)
            
            # ğŸ”‘ åˆ†å‰²æ•°æ®ï¼ˆå–æœ€æ–°çš„æ•°æ®ï¼Œä¿è¯æ—¶é—´å¯¹é½ï¼‰
            if isinstance(X_lgb_scaled, np.ndarray):
                X_lgb_train, X_lgb_val = X_lgb_scaled[-min_len:][:split_idx], X_lgb_scaled[-min_len:][split_idx:]
                X_xgb_train, X_xgb_val = X_xgb_scaled[-min_len:][:split_idx], X_xgb_scaled[-min_len:][split_idx:]
                X_cat_train, X_cat_val = X_cat_scaled[-min_len:][:split_idx], X_cat_scaled[-min_len:][split_idx:]
            else:
                X_lgb_train, X_lgb_val = X_lgb_scaled.iloc[-min_len:][:split_idx], X_lgb_scaled.iloc[-min_len:][split_idx:]
                X_xgb_train, X_xgb_val = X_xgb_scaled.iloc[-min_len:][:split_idx], X_xgb_scaled.iloc[-min_len:][split_idx:]
                X_cat_train, X_cat_val = X_cat_scaled.iloc[-min_len:][:split_idx], X_cat_scaled.iloc[-min_len:][split_idx:]
            
            y_lgb_train, y_lgb_val = y_lgb.iloc[-min_len:][:split_idx], y_lgb.iloc[-min_len:][split_idx:]
            y_xgb_train, y_xgb_val = y_xgb.iloc[-min_len:][:split_idx], y_xgb.iloc[-min_len:][split_idx:]
            y_cat_train, y_cat_val = y_cat.iloc[-min_len:][:split_idx], y_cat.iloc[-min_len:][split_idx:]
            
            logger.info(f"ğŸ“Š {timeframe} æ•°æ®åˆ†å‰²: è®­ç»ƒ{len(X_lgb_train)}æ¡ï¼ˆå¯¹é½åï¼‰, éªŒè¯{len(X_lgb_val)}æ¡")
            
            # 4ï¸âƒ£ è®­ç»ƒStackingé›†æˆæ¨¡å‹ï¼ˆä½¿ç”¨å·®å¼‚åŒ–æ•°æ®ï¼‰
            logger.info(f"ğŸš‚ å¼€å§‹è®­ç»ƒ {timeframe} Stackingé›†æˆï¼ˆå·®å¼‚åŒ–æ•°æ®ï¼‰...")
            ensemble_result = self._train_stacking_diverse(
                X_lgb_train, y_lgb_train, X_lgb_val, y_lgb_val,
                X_xgb_train, y_xgb_train, X_xgb_val, y_xgb_val,
                X_cat_train, y_cat_train, X_cat_val, y_cat_val,
                timeframe
            )
            
            # 8ï¸âƒ£ ä¿å­˜é›†æˆæ¨¡å‹
            self._save_ensemble_models(timeframe)
            
            logger.info(f"â±ï¸ {timeframe} è®­ç»ƒè€—æ—¶: {ensemble_result['training_time']:.2f}ç§’")
            
            return ensemble_result
            
        except Exception as e:
            logger.error(f"âŒ {timeframe} é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def _train_stacking_diverse(
        self,
        X_lgb_train, y_lgb_train, X_lgb_val, y_lgb_val,
        X_xgb_train, y_xgb_train, X_xgb_val, y_xgb_val,
        X_cat_train, y_cat_train, X_cat_val, y_cat_val,
        timeframe: str
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å·®å¼‚åŒ–æ•°æ®è®­ç»ƒStackingé›†æˆæ¨¡å‹
        
        Args:
            X_lgb_train, y_lgb_train: LightGBMè®­ç»ƒæ•°æ®
            X_lgb_val, y_lgb_val: LightGBMéªŒè¯æ•°æ®
            X_xgb_train, y_xgb_train: XGBoostè®­ç»ƒæ•°æ®
            X_xgb_val, y_xgb_val: XGBoostéªŒè¯æ•°æ®
            X_cat_train, y_cat_train: CatBoostè®­ç»ƒæ•°æ®
            X_cat_val, y_cat_val: CatBoostéªŒè¯æ•°æ®
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        import time
        start_time = time.time()
        
        try:
            # ğŸ”§ Optunaè¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            lgb_params_optimized = None
            xgb_params_optimized = None
            cat_params_optimized = None
            inf_params_optimized = None
            
            if self.enable_hyperparameter_tuning:
                if self.optimize_all_models:
                    logger.info(f"ğŸ”§ å¯åŠ¨è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–ï¼ˆOptunaï¼‰- ä¼˜åŒ–å…¨éƒ¨3ä¸ªä¼ ç»Ÿæ¨¡å‹...")
                else:
                    logger.info(f"ğŸ”§ å¯åŠ¨è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–ï¼ˆOptunaï¼‰- ä»…ä¼˜åŒ–LightGBM...")
                logger.info(f"   GPUåŠ é€Ÿ: {'å¯ç”¨' if self.use_gpu else 'å…³é—­'}")
                logger.info(f"   æ¯æ¨¡å‹è¯•éªŒ: {self.optuna_n_trials}æ¬¡, è¶…æ—¶: {self.optuna_timeout}ç§’")
                
                # ä¼˜åŒ–LightGBM
                logger.info(f"   ğŸ”§ [1/{'3' if self.optimize_all_models else '1'}] ä¼˜åŒ–LightGBM...")
                lgb_optimizer = HyperparameterOptimizer(
                    X=X_lgb_train.values if isinstance(X_lgb_train, pd.DataFrame) else X_lgb_train,
                    y=y_lgb_train,
                    timeframe=timeframe,
                    model_type="lightgbm",
                    use_gpu=self.use_gpu
                )
                lgb_params_optimized = lgb_optimizer.optimize(
                    n_trials=self.optuna_n_trials,
                    timeout=self.optuna_timeout,
                    show_progress=False  # å…³é—­è¿›åº¦æ¡ï¼ˆé¿å…æ··ä¹±ï¼‰
                )
                
                # ä¼˜åŒ–XGBoostï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.optimize_all_models:
                    logger.info(f"   ğŸ”§ [2/3] ä¼˜åŒ–XGBoost...")
                    xgb_optimizer = HyperparameterOptimizer(
                        X=X_xgb_train.values if isinstance(X_xgb_train, pd.DataFrame) else X_xgb_train,
                        y=y_xgb_train,
                        timeframe=timeframe,
                        model_type="xgboost",
                        use_gpu=self.use_gpu
                    )
                    xgb_params_optimized = xgb_optimizer.optimize(
                        n_trials=self.optuna_n_trials,
                        timeout=self.optuna_timeout,
                        show_progress=False
                    )
                
                # ä¼˜åŒ–CatBoostï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.optimize_all_models:
                    logger.info(f"   ğŸ”§ [3/3] ä¼˜åŒ–CatBoost...")
                    cat_optimizer = HyperparameterOptimizer(
                        X=X_cat_train.values if isinstance(X_cat_train, pd.DataFrame) else X_cat_train,
                        y=y_cat_train,
                        timeframe=timeframe,
                        model_type="catboost",
                        use_gpu=self.use_gpu
                    )
                    cat_params_optimized = cat_optimizer.optimize(
                        n_trials=self.optuna_n_trials,
                        timeout=self.optuna_timeout,
                        show_progress=False
                    )
                
                logger.info(f"âœ… ä¼ ç»Ÿæ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
                if lgb_params_optimized:
                    logger.info(f"   LightGBMæœ€ä½³CV: {lgb_optimizer.best_score:.4f}")
                if xgb_params_optimized:
                    logger.info(f"   XGBoostæœ€ä½³CV:  {xgb_optimizer.best_score:.4f}")
                if cat_params_optimized:
                    logger.info(f"   CatBoostæœ€ä½³CV: {cat_optimizer.best_score:.4f}")
            
            # ğŸ¤– Informer-2è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_informer2 and self.optimize_informer2 and TORCH_AVAILABLE:
                logger.info(f"ğŸ¤– å¯åŠ¨Informer-2è¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰...")
                logger.info(f"   GPUåŠ é€Ÿ: {'å¯ç”¨' if self.use_gpu else 'å…³é—­'}")
                logger.info(f"   è¯•éªŒæ¬¡æ•°: {self.informer_n_trials}æ¬¡, è¶…æ—¶: {self.informer_timeout}ç§’")
                
                inf_optimizer = HyperparameterOptimizer(
                    X=X_lgb_train.values if isinstance(X_lgb_train, pd.DataFrame) else X_lgb_train,
                    y=y_lgb_train,
                    timeframe=timeframe,
                    model_type="informer2",
                    use_gpu=self.use_gpu
                )
                inf_params_optimized = inf_optimizer.optimize(
                    n_trials=self.informer_n_trials,
                    timeout=self.informer_timeout,
                    show_progress=False
                )
                logger.info(f"âœ… Informer-2è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ: æœ€ä½³CVå‡†ç¡®ç‡={inf_optimizer.best_score:.4f}")
            
            # 1ï¸âƒ£ è®­ç»ƒå››ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆå„ç”¨è‡ªå·±çš„æ•°æ®ï¼‰
            logger.info(f"ğŸš‚ è®­ç»ƒLightGBMï¼ˆ360å¤©æ•°æ®ï¼‰...")
            lgb_model = self._train_lightgbm(X_lgb_train, y_lgb_train, timeframe, custom_params=lgb_params_optimized)
            
            logger.info(f"ğŸš‚ è®­ç»ƒXGBoostï¼ˆ540å¤©æ•°æ®ï¼‰...")
            xgb_model = self._train_xgboost(X_xgb_train, y_xgb_train, timeframe, custom_params=xgb_params_optimized)
            
            logger.info(f"ğŸš‚ è®­ç»ƒCatBoostï¼ˆ720å¤©æ•°æ®ï¼‰...")
            cat_model = self._train_catboost(X_cat_train, y_cat_train, timeframe, custom_params=cat_params_optimized)
            
            # ğŸ¤– è®­ç»ƒInformer-2ï¼ˆæ·±åº¦å­¦ä¹  + GMADLæŸå¤±ï¼‰
            inf_model = None
            if self.enable_informer2 and TORCH_AVAILABLE:
                logger.info(f"ğŸ¤– è®­ç»ƒInformer-2ï¼ˆæ·±åº¦å­¦ä¹  + GMADLæŸå¤±ï¼‰...")
                inf_model = self._train_informer2(X_lgb_train, y_lgb_train, timeframe, custom_params=inf_params_optimized)
            
            # 2ï¸âƒ£ ç”ŸæˆéªŒè¯é›†çš„é¢„æµ‹æ¦‚ç‡ï¼ˆå…ƒç‰¹å¾ï¼‰
            logger.info(f"ğŸ“Š ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆåŸºäºå¯¹é½çš„éªŒè¯é›†ï¼‰...")
            
            # ä½¿ç”¨å„è‡ªçš„éªŒè¯é›†ç”Ÿæˆé¢„æµ‹
            lgb_pred_proba = lgb_model.predict_proba(X_lgb_val)
            xgb_pred_proba = xgb_model.predict_proba(X_xgb_val)
            cat_pred_proba = cat_model.predict_proba(X_cat_val)
            
            # Informer-2é¢„æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if inf_model is not None:
                inf_pred_proba = inf_model.predict_proba(X_lgb_val)
            
            logger.info(f"æ¦‚ç‡å½¢çŠ¶: lgb={lgb_pred_proba.shape}, xgb={xgb_pred_proba.shape}, cat={cat_pred_proba.shape}")
            
            # ğŸ”‘ éªŒè¯å½¢çŠ¶ä¸€è‡´æ€§
            assert lgb_pred_proba.shape == xgb_pred_proba.shape == cat_pred_proba.shape, \
                f"æ¦‚ç‡æ•°ç»„å½¢çŠ¶ä¸ä¸€è‡´: {lgb_pred_proba.shape} vs {xgb_pred_proba.shape} vs {cat_pred_proba.shape}"
            
            # è·å–é¢„æµ‹ç±»åˆ«
            lgb_pred_raw = lgb_model.predict(X_lgb_val)
            xgb_pred_raw = xgb_model.predict(X_xgb_val)
            cat_pred_raw = cat_model.predict(X_cat_val)
            
            # ğŸ”‘ ç»Ÿä¸€è½¬æ¢ä¸º1Dæ•°ç»„ï¼ˆCatBoostè¿”å›2Dï¼Œéœ€è¦ravelï¼‰
            lgb_pred = lgb_pred_raw.ravel()
            xgb_pred = xgb_pred_raw.ravel()
            cat_pred = cat_pred_raw.ravel()
            
            # ğŸ”‘ ä¸¥æ ¼éªŒè¯é¢„æµ‹æ•°ç»„å½¢çŠ¶
            expected_shape = (len(y_lgb_val),)
            assert lgb_pred.shape == expected_shape, f"lgb_predå½¢çŠ¶é”™è¯¯: {lgb_pred.shape} != {expected_shape}"
            assert xgb_pred.shape == expected_shape, f"xgb_predå½¢çŠ¶é”™è¯¯: {xgb_pred.shape} != {expected_shape}"
            assert cat_pred.shape == expected_shape, f"cat_predå½¢çŠ¶é”™è¯¯: {cat_pred.shape} != {expected_shape}"
            
            logger.info(f"é¢„æµ‹ç±»åˆ«å½¢çŠ¶éªŒè¯é€šè¿‡: {lgb_pred.shape} (å·²ç»Ÿä¸€ä¸º1Dæ•°ç»„)")
            
            # ğŸ†• å¢å¼ºå…ƒç‰¹å¾ï¼ˆæå‡å…ƒå­¦ä¹ å™¨å†³ç­–èƒ½åŠ›ï¼‰
            logger.info(f"ç”Ÿæˆå¢å¼ºå…ƒç‰¹å¾...")
            
            # 1. æ¨¡å‹ä¸€è‡´æ€§ï¼ˆ3ä¸ªæ¨¡å‹é¢„æµ‹æ˜¯å¦ä¸€è‡´ï¼‰
            # ğŸ”‘ å·²ç¡®è®¤éƒ½æ˜¯1Dæ•°ç»„ï¼Œç›´æ¥æ¯”è¾ƒ
            agreement_bool = (lgb_pred == xgb_pred) & (xgb_pred == cat_pred)  # (6757,) boolean
            agreement = agreement_bool.astype(float).reshape(-1, 1)  # (6757, 1)
            
            # éªŒè¯ç»´åº¦
            assert agreement.shape == (len(y_lgb_val), 1), f"agreementå½¢çŠ¶é”™è¯¯: {agreement.shape}"
            logger.debug(f"âœ“ agreement: {agreement.shape}")
            
            # 2. æœ€å¤§æ¦‚ç‡ï¼ˆæ¯ä¸ªæ¨¡å‹çš„æœ€é«˜ç½®ä¿¡åº¦ï¼‰
            lgb_max_prob = lgb_pred_proba.max(axis=1).reshape(-1, 1)
            xgb_max_prob = xgb_pred_proba.max(axis=1).reshape(-1, 1)
            cat_max_prob = cat_pred_proba.max(axis=1).reshape(-1, 1)
            assert lgb_max_prob.shape == (len(y_lgb_val), 1), f"lgb_max_probå½¢çŠ¶é”™è¯¯: {lgb_max_prob.shape}"
            logger.debug(f"âœ“ max_prob: {lgb_max_prob.shape}")
            
            # 3. æ¦‚ç‡ç†µï¼ˆä¸ç¡®å®šæ€§ï¼Œç†µè¶Šé«˜è¶Šä¸ç¡®å®šï¼‰
            from scipy.special import entr
            lgb_entropy = entr(lgb_pred_proba).sum(axis=1).reshape(-1, 1)
            xgb_entropy = entr(xgb_pred_proba).sum(axis=1).reshape(-1, 1)
            cat_entropy = entr(cat_pred_proba).sum(axis=1).reshape(-1, 1)
            assert lgb_entropy.shape == (len(y_lgb_val), 1), f"lgb_entropyå½¢çŠ¶é”™è¯¯: {lgb_entropy.shape}"
            logger.debug(f"âœ“ entropy: {lgb_entropy.shape}")
            
            # Informer-2çš„å¢å¼ºç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if inf_model is not None:
                inf_max_prob = inf_pred_proba.max(axis=1).reshape(-1, 1)
                inf_entropy = entr(inf_pred_proba).sum(axis=1).reshape(-1, 1)
                logger.debug(f"âœ“ inf_max_prob: {inf_max_prob.shape}, inf_entropy: {inf_entropy.shape}")
            
            # 4. å¹³å‡æ¦‚ç‡ï¼ˆä¸‰ä¸ªæˆ–å››ä¸ªæ¨¡å‹çš„å¹³å‡é¢„æµ‹æ¦‚ç‡ï¼‰
            if inf_model is not None:
                avg_proba = (lgb_pred_proba + xgb_pred_proba + cat_pred_proba + inf_pred_proba) / 4
            else:
                avg_proba = (lgb_pred_proba + xgb_pred_proba + cat_pred_proba) / 3
            assert avg_proba.shape == lgb_pred_proba.shape, f"avg_probaå½¢çŠ¶é”™è¯¯: {avg_proba.shape}"
            logger.debug(f"âœ“ avg_proba: {avg_proba.shape}")
            
            # 5. æ¦‚ç‡æ ‡å‡†å·®ï¼ˆæ¨¡å‹é—´çš„é¢„æµ‹å·®å¼‚ï¼‰
            if inf_model is not None:
                prob_std = np.std(np.stack([lgb_pred_proba, xgb_pred_proba, cat_pred_proba, inf_pred_proba]), axis=0)
            else:
                prob_std = np.std(np.stack([lgb_pred_proba, xgb_pred_proba, cat_pred_proba]), axis=0)
            prob_std_max = prob_std.max(axis=1).reshape(-1, 1)
            assert prob_std_max.shape == (len(y_lgb_val), 1), f"prob_std_maxå½¢çŠ¶é”™è¯¯: {prob_std_max.shape}"
            logger.debug(f"âœ“ prob_std_max: {prob_std_max.shape}")
            
            # ğŸ”‘ æ‹¼æ¥æ‰€æœ‰å…ƒç‰¹å¾ï¼ˆä¸¥æ ¼éªŒè¯æ¯ä¸€æ­¥ï¼‰
            logger.info(f"å¼€å§‹æ‹¼æ¥å…ƒç‰¹å¾...")
            
            # é€æ­¥æ‹¼æ¥å¹¶éªŒè¯
            if inf_model is not None:
                # åŒ…å«Informer-2ï¼ˆ23ä¸ªç‰¹å¾ï¼‰
                meta_list = [
                    lgb_pred_proba,      # (N, 3)
                    xgb_pred_proba,      # (N, 3)
                    cat_pred_proba,      # (N, 3)
                    inf_pred_proba,      # (N, 3) â† æ–°å¢
                    agreement,           # (N, 1)
                    lgb_max_prob,        # (N, 1)
                    xgb_max_prob,        # (N, 1)
                    cat_max_prob,        # (N, 1)
                    inf_max_prob,        # (N, 1) â† æ–°å¢
                    lgb_entropy,         # (N, 1)
                    xgb_entropy,         # (N, 1)
                    cat_entropy,         # (N, 1)
                    inf_entropy,         # (N, 1) â† æ–°å¢
                    avg_proba,           # (N, 3)
                    prob_std_max         # (N, 1)
                ]
                expected_features = 23  # 3+3+3+3+1+1+1+1+1+1+1+1+1+3+1
            else:
                # ä»…ä¼ ç»Ÿæ¨¡å‹ï¼ˆ20ä¸ªç‰¹å¾ï¼‰
                meta_list = [
                    lgb_pred_proba,      # (N, 3)
                    xgb_pred_proba,      # (N, 3)
                    cat_pred_proba,      # (N, 3)
                    agreement,           # (N, 1)
                    lgb_max_prob,        # (N, 1)
                    xgb_max_prob,        # (N, 1)
                    cat_max_prob,        # (N, 1)
                    lgb_entropy,         # (N, 1)
                    xgb_entropy,         # (N, 1)
                    cat_entropy,         # (N, 1)
                    avg_proba,           # (N, 3)
                    prob_std_max         # (N, 1)
                ]
                expected_features = 20  # 3+3+3+1+1+1+1+1+1+1+3+1
            
            # éªŒè¯æ‰€æœ‰æ•°ç»„çš„ç¬¬0ç»´åº¦éƒ½ç›¸åŒ
            expected_rows = len(y_lgb_val)
            for i, arr in enumerate(meta_list):
                assert arr.shape[0] == expected_rows, \
                    f"å…ƒç‰¹å¾{i}ç¬¬0ç»´åº¦é”™è¯¯: {arr.shape[0]} != {expected_rows}, å®Œæ•´å½¢çŠ¶: {arr.shape}"
            
            # æ‹¼æ¥
            meta_features_val = np.hstack(meta_list)
            
            # æœ€ç»ˆéªŒè¯
            assert meta_features_val.shape == (expected_rows, expected_features), \
                f"å…ƒç‰¹å¾æœ€ç»ˆå½¢çŠ¶é”™è¯¯: {meta_features_val.shape} != ({expected_rows}, {expected_features})"
            
            # å…ƒæ ‡ç­¾ï¼ˆä½¿ç”¨LightGBMçš„y_valï¼Œå› ä¸ºéªŒè¯é›†å·²å¯¹é½ï¼‰
            meta_labels_val = y_lgb_val
            
            if inf_model is not None:
                logger.info(f"âœ… å¢å¼ºå…ƒç‰¹å¾ç”Ÿæˆå®Œæˆ: {meta_features_val.shape} (åŸºç¡€12+å¢å¼º11=23ä¸ªï¼Œå«Informer-2)")
            else:
                logger.info(f"âœ… å¢å¼ºå…ƒç‰¹å¾ç”Ÿæˆå®Œæˆ: {meta_features_val.shape} (åŸºç¡€9+å¢å¼º11=20ä¸ª)")
            
            # 3ï¸âƒ£ è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰ - å‡çº§ä¸ºLightGBM + åŠ¨æ€HOLDæƒ©ç½š
            logger.info(f"ğŸ§  è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆLightGBM - æ›´å¼ºå¤§çš„å†³ç­–èƒ½åŠ›ï¼‰...")
            
            # ğŸ”‘ æ£€æŸ¥HOLDæ¯”ä¾‹ï¼ŒåŠ¨æ€è°ƒæ•´æƒ©ç½šç³»æ•°
            from sklearn.utils.class_weight import compute_sample_weight
            hold_ratio = (meta_labels_val == 1).sum() / len(meta_labels_val)
            
            # ğŸ”‘ æ ¹æ®HOLDæ¯”ä¾‹åŠ¨æ€è°ƒæ•´æƒ©ç½šï¼ˆå¹³è¡¡ç­–ç•¥ï¼‰
            if hold_ratio > 0.60:  # HOLDå æ¯”>60%ï¼Œé‡æƒ©ç½š
                meta_hold_penalty_weight = 0.45
            elif hold_ratio > 0.50:  # HOLDå æ¯”>50%ï¼Œä¸­ç­‰
                meta_hold_penalty_weight = 0.55
            elif hold_ratio > 0.40:  # HOLDå æ¯”>40%ï¼Œè½»åº¦
                meta_hold_penalty_weight = 0.65
            else:  # HOLDå æ¯”<=40%ï¼Œæ­£å¸¸
                meta_hold_penalty_weight = 0.75
            
            logger.info(f"   HOLDå æ¯”: {hold_ratio*100:.1f}% â†’ æƒ©ç½šç³»æ•°: {meta_hold_penalty_weight}")
            
            meta_class_weights = compute_sample_weight('balanced', meta_labels_val)
            meta_hold_penalty = np.where(meta_labels_val == 1, meta_hold_penalty_weight, 1.0)
            meta_sample_weights = meta_class_weights * meta_hold_penalty
            
            import lightgbm as lgb
            # ğŸ”‘ å…ƒå­¦ä¹ å™¨ï¼šæç®€é…ç½®é˜²æ­¢è¿‡æ‹Ÿåˆ
            meta_learner = lgb.LGBMClassifier(
                n_estimators=50,     # å‡å°‘æ ‘æ•°é‡ 100â†’50
                max_depth=3,         # æ›´æµ…çš„æ ‘ 4â†’3
                learning_rate=0.15,  # æé«˜å­¦ä¹ ç‡ 0.1â†’0.15ï¼ˆå°‘é‡æ ‘ï¼‰
                num_leaves=7,        # å¤§å¹…å‡å°‘å¶å­ 15â†’7
                min_child_samples=30,  # å¢åŠ æœ€å°æ ·æœ¬ 20â†’30
                subsample=0.7,       # é™ä½é‡‡æ · 0.8â†’0.7
                colsample_bytree=0.7,  # é™ä½ç‰¹å¾é‡‡æ · 0.8â†’0.7
                reg_alpha=0.3,       # åŠ å¼ºL1æ­£åˆ™ 0.1â†’0.3
                reg_lambda=0.3,      # åŠ å¼ºL2æ­£åˆ™ 0.1â†’0.3
                random_state=42,
                verbose=-1
            )
            meta_learner.fit(meta_features_val, meta_labels_val, sample_weight=meta_sample_weights)
            
            logger.info(f"âœ… å…ƒå­¦ä¹ å™¨è®­ç»ƒå®Œæˆï¼ˆåŠ¨æ€HOLDæƒ©ç½š={meta_hold_penalty_weight}ï¼‰")
            
            # 4ï¸âƒ£ ä¿å­˜æ¨¡å‹åˆ°å­—å…¸
            if timeframe not in self.ensemble_models:
                self.ensemble_models[timeframe] = {}
            
            self.ensemble_models[timeframe]['lightgbm'] = lgb_model
            self.ensemble_models[timeframe]['xgboost'] = xgb_model
            self.ensemble_models[timeframe]['catboost'] = cat_model
            self.ensemble_models[timeframe]['meta_learner'] = meta_learner
            
            # 5ï¸âƒ£ è¯„ä¼°é›†æˆæ¨¡å‹ - ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            logger.info(f"ğŸ“Š {timeframe} æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¯„ä¼°...")
            
            from sklearn.model_selection import TimeSeriesSplit
            from sklearn.metrics import accuracy_score, precision_recall_fscore_support
            
            # ğŸ†• æ—¶é—´åºåˆ—5æŠ˜äº¤å‰éªŒè¯ï¼ˆæ›´å¯é çš„è¯„ä¼°ï¼‰
            tscv = TimeSeriesSplit(n_splits=5)
            cv_scores = []
            
            # å¯¹éªŒè¯é›†è¿›è¡Œäº¤å‰éªŒè¯
            for fold, (train_idx, test_idx) in enumerate(tscv.split(meta_features_val), 1):
                meta_train, meta_test = meta_features_val[train_idx], meta_features_val[test_idx]
                y_train, y_test = meta_labels_val.iloc[train_idx], meta_labels_val.iloc[test_idx]
                
                # è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆæ¯ä¸ªfoldï¼‰- ä¸æœ€ç»ˆæ¨¡å‹å®Œå…¨ä¸€è‡´çš„é…ç½®
                fold_meta = lgb.LGBMClassifier(
                    n_estimators=50, max_depth=3, learning_rate=0.15,
                    num_leaves=7, min_child_samples=30, subsample=0.7,
                    colsample_bytree=0.7, reg_alpha=0.3, reg_lambda=0.3,
                    random_state=42, verbose=-1
                )
                
                # ğŸ”‘ HOLDæƒ©ç½šï¼ˆä¸æœ€ç»ˆæ¨¡å‹ä¸€è‡´ï¼Œä½¿ç”¨ç›¸åŒçš„åŠ¨æ€ç­–ç•¥ï¼‰
                fold_weights = compute_sample_weight('balanced', y_train)
                fold_hold_ratio = (y_train == 1).sum() / len(y_train)
                
                # åŠ¨æ€æƒ©ç½šï¼ˆå¹³è¡¡ç­–ç•¥ï¼Œä¸æœ€ç»ˆæ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
                if fold_hold_ratio > 0.60:
                    fold_penalty = 0.45
                elif fold_hold_ratio > 0.50:
                    fold_penalty = 0.55
                elif fold_hold_ratio > 0.40:
                    fold_penalty = 0.65
                else:
                    fold_penalty = 0.75
                
                fold_hold_penalty = np.where(y_train == 1, fold_penalty, 1.0)
                fold_sample_weights = fold_weights * fold_hold_penalty
                
                fold_meta.fit(meta_train, y_train, sample_weight=fold_sample_weights)
                fold_pred = fold_meta.predict(meta_test)
                fold_acc = accuracy_score(y_test, fold_pred)
                cv_scores.append(fold_acc)
                
                logger.debug(f"  Fold {fold}: å‡†ç¡®ç‡={fold_acc:.4f}")
            
            # äº¤å‰éªŒè¯å‡†ç¡®ç‡
            cv_mean = np.mean(cv_scores)
            cv_std = np.std(cv_scores)
            
            logger.info(f"âœ… {timeframe} æ—¶é—´åºåˆ—CVç»“æœ: {cv_mean:.4f} Â± {cv_std:.4f}")
            logger.info(f"   CVåˆ†æ•°: {[f'{s:.4f}' for s in cv_scores]}")
            
            # ä½¿ç”¨å®Œæ•´éªŒè¯é›†è¯„ä¼°æœ€ç»ˆæ¨¡å‹
            ensemble_pred = meta_learner.predict(meta_features_val)
            accuracy = accuracy_score(meta_labels_val, ensemble_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(
                meta_labels_val, ensemble_pred, average='weighted', zero_division=0
            )
            
            logger.info(f"ğŸ“Š {timeframe} æœ€ç»ˆæ¨¡å‹éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f} (CV: {cv_mean:.4f}Â±{cv_std:.4f})")
            
            # 6ï¸âƒ£ è¯„ä¼°å„åŸºç¡€æ¨¡å‹
            lgb_pred = lgb_model.predict(X_lgb_val)
            xgb_pred = xgb_model.predict(X_xgb_val)
            cat_pred = cat_model.predict(X_cat_val)
            
            lgb_acc = accuracy_score(y_lgb_val, lgb_pred)
            xgb_acc = accuracy_score(y_xgb_val, xgb_pred)
            cat_acc = accuracy_score(y_cat_val, cat_pred)
            
            # Informer-2å‡†ç¡®ç‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if inf_model is not None:
                inf_pred = inf_model.predict(X_lgb_val)
                inf_acc = accuracy_score(y_lgb_val, inf_pred)
            else:
                inf_acc = 0.0
            
            training_time = time.time() - start_time
            
            result = {
                'accuracy': cv_mean,  # ğŸ”‘ ä½¿ç”¨CVå‡å€¼ä½œä¸ºä¸»å‡†ç¡®ç‡ï¼ˆæ›´å¯é ï¼‰
                'cv_mean': cv_mean,   # äº¤å‰éªŒè¯å‡å€¼
                'cv_std': cv_std,     # äº¤å‰éªŒè¯æ ‡å‡†å·®
                'cv_scores': cv_scores,  # å„æŠ˜åˆ†æ•°
                'val_accuracy': accuracy,  # éªŒè¯é›†å‡†ç¡®ç‡
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'lgb_accuracy': lgb_acc,
                'xgb_accuracy': xgb_acc,
                'cat_accuracy': cat_acc,
                'inf_accuracy': inf_acc if inf_model else 0.0,
                'training_time': training_time,
                'ensemble_size': len(self.ensemble_models[timeframe]),
                'meta_features_count': meta_features_val.shape[1]  # å…ƒç‰¹å¾æ•°é‡
            }
            
            logger.info(f"âœ… Stackingè®­ç»ƒå®Œæˆï¼ˆå·®å¼‚åŒ–æ•°æ®ï¼‰:")
            logger.info(f"  LightGBM(360å¤©): {lgb_acc:.4f}")
            logger.info(f"  XGBoost(540å¤©):  {xgb_acc:.4f}")
            logger.info(f"  CatBoost(720å¤©): {cat_acc:.4f}")
            if inf_model:
                logger.info(f"  Informer-2:      {inf_acc:.4f} ğŸ¤–")
            logger.info(f"  StackingéªŒè¯é›†:  {accuracy:.4f}")
            logger.info(f"  ğŸ¯ æ—¶é—´åºåˆ—CV:  {cv_mean:.4f} Â± {cv_std:.4f} (5-fold)")
            n_base = 12 if inf_model else 9
            n_enhanced = 11 if not inf_model else 11
            logger.info(f"  ğŸ“Š å…ƒç‰¹å¾: {meta_features_val.shape[1]}ä¸ªï¼ˆåŸºç¡€{n_base}+å¢å¼º{n_enhanced}ï¼‰")
            logger.info(f"  è®­ç»ƒè€—æ—¶: {training_time:.2f}ç§’")
            
            return result
            
        except Exception as e:
            logger.error(f"å·®å¼‚åŒ–Stackingè®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def _train_lightgbm(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str, custom_params: Optional[Dict[str, Any]] = None):
        """
        è®­ç»ƒLightGBMæ¨¡å‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œç»Ÿä¸€ä¸‰æ¨¡å‹è®­ç»ƒä»£ç ä½ç½®ï¼‰
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            timeframe: æ—¶é—´æ¡†æ¶
            custom_params: è‡ªå®šä¹‰å‚æ•°ï¼ˆOptunaä¼˜åŒ–åçš„å‚æ•°ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
        """
        try:
            import lightgbm as lgb
            from sklearn.utils.class_weight import compute_sample_weight
            
            # æ ·æœ¬åŠ æƒï¼ˆç±»åˆ«å¹³è¡¡ Ã— æ—¶é—´è¡°å‡ Ã— HOLDæƒ©ç½šï¼‰
            class_weights = compute_sample_weight('balanced', y_train)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            
            # ğŸ”‘ HOLDç±»åˆ«é™æƒï¼ˆé€‚åº¦æƒ©ç½šç­–ç•¥ï¼‰
            hold_penalty = np.where(y_train == 1, 0.65, 1.0)  # HOLDæƒé‡0.65ï¼ˆé€‚åº¦æƒ©ç½š 0.5â†’0.65ï¼‰
            
            sample_weights = class_weights * time_decay * hold_penalty
            
            logger.info(f"âœ… æ ·æœ¬åŠ æƒå·²å¯ç”¨ï¼šç±»åˆ«å¹³è¡¡ Ã— æ—¶é—´è¡°å‡ Ã— HOLDæƒ©ç½š(0.65)")
            
            # ç¡®å®šæœ€ç»ˆå‚æ•°ï¼ˆä¼˜å…ˆçº§ï¼šcustom_params > timeframe_params > base_paramsï¼‰
            if custom_params:
                params = custom_params
                logger.info(f"ğŸ¯ ä½¿ç”¨Optunaä¼˜åŒ–å‚æ•°")
            else:
                # è·å–æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–å‚æ•°
                timeframe_params = self.lgb_params_by_timeframe.get(timeframe, {})
                # åˆå¹¶åŸºç¡€å‚æ•°å’Œå·®å¼‚åŒ–å‚æ•°
                params = {**self.lgb_params, **timeframe_params}
            
            # ğŸ® å¯ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
            if self.use_gpu:
                params['device'] = 'gpu'
                params['gpu_platform_id'] = 0
                params['gpu_device_id'] = 0
                logger.info(f"ğŸš€ LightGBM GPUåŠ é€Ÿå·²å¯ç”¨")
            
            logger.info(f"ğŸ“Š {timeframe} LightGBMå‚æ•°: num_leaves={params.get('num_leaves')}, "
                       f"reg_alpha={params.get('reg_alpha', 0)}, reg_lambda={params.get('reg_lambda', 0)}")
            
            # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹ï¼ˆparamsä¸­å·²åŒ…å«random_state=42ï¼‰
            model = lgb.LGBMClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights)
            
            return model
            
        except Exception as e:
            logger.error(f"LightGBMè®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def _train_xgboost(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str, custom_params: Optional[Dict[str, Any]] = None):
        """è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰"""
        try:
            import xgboost as xgb
            
            # æ ·æœ¬åŠ æƒï¼ˆä¸LightGBMä¸€è‡´ + HOLDæƒ©ç½šï¼‰
            from sklearn.utils.class_weight import compute_sample_weight
            class_weights = compute_sample_weight('balanced', y_train)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            
            # ğŸ”‘ HOLDç±»åˆ«é™æƒï¼ˆé€‚åº¦æƒ©ç½šï¼Œä¸LightGBMä¸€è‡´ï¼‰
            hold_penalty = np.where(y_train == 1, 0.65, 1.0)
            
            sample_weights = class_weights * time_decay * hold_penalty
            
            # ğŸ”‘ æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–é…ç½®ï¼ˆé˜²æ­¢2h/4hè¿‡æ‹Ÿåˆï¼‰
            if custom_params:
                # ä½¿ç”¨Optunaä¼˜åŒ–çš„å‚æ•°
                params = custom_params.copy()
                logger.info(f"ğŸ¯ ä½¿ç”¨Optunaä¼˜åŒ–å‚æ•°")
            else:
                # ä½¿ç”¨é»˜è®¤å‚æ•°
                if timeframe == '15m':
                    params = {
                        'max_depth': 6,
                        'learning_rate': 0.05,
                        'n_estimators': 300,
                        'reg_alpha': 0.3,
                        'reg_lambda': 0.3
                    }
                elif timeframe == '2h':
                    params = {
                        'max_depth': 4,  # 6â†’4ï¼ˆç®€åŒ–ï¼‰
                        'learning_rate': 0.08,  # 0.05â†’0.08ï¼ˆå°‘é‡æ ‘ï¼‰
                        'n_estimators': 150,  # 300â†’150ï¼ˆå‡åŠï¼‰
                        'reg_alpha': 0.8,  # åŠ å¼ºæ­£åˆ™åŒ–
                        'reg_lambda': 0.8
                    }
                else:  # 4h
                    params = {
                        'max_depth': 3,  # 6â†’3ï¼ˆæç®€ï¼‰
                        'learning_rate': 0.1,  # 0.05â†’0.1
                        'n_estimators': 100,  # 300â†’100ï¼ˆå¤§å¹…å‡å°‘ï¼‰
                        'reg_alpha': 1.0,  # æå¼ºæ­£åˆ™åŒ–
                        'reg_lambda': 1.0
                    }
            
            # é€šç”¨å‚æ•°
            params.update({
                'objective': 'multi:softprob',
                'num_class': 3,
                'eval_metric': 'mlogloss',
                'random_state': 42,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            })
            
            # ğŸ® GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_gpu:
                params['tree_method'] = 'gpu_hist'
                params['gpu_id'] = 0
                logger.info(f"ğŸš€ XGBoost GPUåŠ é€Ÿå·²å¯ç”¨")
            else:
                params['tree_method'] = 'hist'
            
            model = xgb.XGBClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
            
            return model
            
        except Exception as e:
            logger.error(f"XGBoostè®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def _train_catboost(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str, custom_params: Optional[Dict[str, Any]] = None):
        """è®­ç»ƒCatBoostæ¨¡å‹ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰"""
        try:
            from catboost import CatBoostClassifier
            
            # æ ·æœ¬åŠ æƒï¼ˆä¸LightGBMä¸€è‡´ + HOLDæƒ©ç½šï¼‰
            from sklearn.utils.class_weight import compute_sample_weight
            class_weights = compute_sample_weight('balanced', y_train)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            
            # ğŸ”‘ HOLDç±»åˆ«é™æƒï¼ˆé€‚åº¦æƒ©ç½šï¼Œä¸LightGBMä¸€è‡´ï¼‰
            hold_penalty = np.where(y_train == 1, 0.65, 1.0)
            
            sample_weights = class_weights * time_decay * hold_penalty
            
            # ğŸ”‘ æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–é…ç½®ï¼ˆé˜²æ­¢2h/4hè¿‡æ‹Ÿåˆï¼‰
            if custom_params:
                # ä½¿ç”¨Optunaä¼˜åŒ–çš„å‚æ•°
                params = custom_params.copy()
                logger.info(f"ğŸ¯ ä½¿ç”¨Optunaä¼˜åŒ–å‚æ•°")
            else:
                # ä½¿ç”¨é»˜è®¤å‚æ•°
                if timeframe == '15m':
                    params = {
                        'iterations': 300,
                        'learning_rate': 0.05,
                        'depth': 6,
                        'l2_leaf_reg': 3.0
                    }
                elif timeframe == '2h':
                    params = {
                        'iterations': 150,  # 300â†’150ï¼ˆå‡åŠï¼‰
                        'learning_rate': 0.08,
                        'depth': 4,  # 6â†’4ï¼ˆç®€åŒ–ï¼‰
                        'l2_leaf_reg': 5.0  # 3.0â†’5.0ï¼ˆåŠ å¼ºæ­£åˆ™ï¼‰
                    }
                else:  # 4h
                    params = {
                        'iterations': 100,  # 300â†’100ï¼ˆå¤§å¹…å‡å°‘ï¼‰
                        'learning_rate': 0.1,
                        'depth': 3,  # 6â†’3ï¼ˆæç®€ï¼‰
                        'l2_leaf_reg': 8.0  # 3.0â†’8.0ï¼ˆæå¼ºæ­£åˆ™ï¼‰
                    }
            
            # é€šç”¨å‚æ•°
            params.update({
                'loss_function': 'MultiClass',
                'random_seed': 42,
                'verbose': False,
                'bootstrap_type': 'Bernoulli',
                'subsample': 0.8,
                'allow_writing_files': False
            })
            
            # ğŸ® GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_gpu:
                params['task_type'] = 'GPU'
                params['devices'] = '0'
                logger.info(f"ğŸš€ CatBoost GPUåŠ é€Ÿå·²å¯ç”¨")
            
            model = CatBoostClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
            
            return model
            
        except Exception as e:
            logger.error(f"CatBoostè®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def _train_informer2(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str, custom_params: Optional[Dict[str, Any]] = None):
        """
        è®­ç»ƒInformer-2æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆä½¿ç”¨GMADLæŸå¤±å‡½æ•°ï¼‰
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            timeframe: æ—¶é—´æ¡†æ¶
            custom_params: è‡ªå®šä¹‰å‚æ•°ï¼ˆæ¥è‡ªOptunaä¼˜åŒ–ï¼‰
        
        Returns:
            è®­ç»ƒå¥½çš„Informer-2æ¨¡å‹ï¼ˆå…¼å®¹scikit-learnæ¥å£ï¼‰
        """
        if not TORCH_AVAILABLE:
            logger.warning("âš ï¸ PyTorchæœªå®‰è£…ï¼Œè·³è¿‡Informer-2è®­ç»ƒ")
            return None
        
        try:
            import time
            start_time = time.time()
            
            logger.info(f"ğŸ¤– è®­ç»ƒInformer-2ç¥ç»ç½‘ç»œæ¨¡å‹...")
            
            # 1. æ•°æ®å‡†å¤‡ï¼ˆPandas â†’ PyTorchï¼‰
            X_np = X_train.values if isinstance(X_train, pd.DataFrame) else X_train
            y_np = y_train.values if isinstance(y_train, pd.Series) else y_train
            
            X_tensor = torch.FloatTensor(X_np)
            y_tensor = torch.LongTensor(y_np)
            
            # 2. æ£€æµ‹GPU
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            logger.info(f"   è®¾å¤‡: {device} {'ğŸš€ (GPUåŠ é€Ÿ)' if device.type == 'cuda' else 'ğŸ’» (CPU)'}")
            
            # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
            dataset = TensorDataset(X_tensor, y_tensor)
            dataloader = DataLoader(
                dataset,
                batch_size=self.informer_batch_size,
                shuffle=True,
                num_workers=0  # Windowså…¼å®¹
            )
            
            # 4. ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
            if custom_params:
                d_model = custom_params.get('d_model', self.informer_d_model)
                n_heads = custom_params.get('n_heads', self.informer_n_heads)
                n_layers = custom_params.get('n_layers', self.informer_n_layers)
                dropout = custom_params.get('dropout', 0.1)
                epochs = custom_params.get('epochs', self.informer_epochs)
                batch_size = custom_params.get('batch_size', self.informer_batch_size)
                lr = custom_params.get('lr', self.informer_lr)
                alpha = custom_params.get('alpha', 1.0)
                beta = custom_params.get('beta', 0.5)
                logger.info(f"ğŸ¯ ä½¿ç”¨ä¼˜åŒ–å‚æ•°: d_model={d_model}, n_heads={n_heads}, n_layers={n_layers}, epochs={epochs}")
            else:
                d_model = self.informer_d_model
                n_heads = self.informer_n_heads
                n_layers = self.informer_n_layers
                dropout = 0.1
                epochs = self.informer_epochs
                batch_size = self.informer_batch_size
                lr = self.informer_lr
                alpha = 1.0
                beta = 0.5
            
            # 5. åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¿®å¤å‚æ•°åï¼‰
            model = Informer2ForClassification(
                n_features=X_np.shape[1],  # ç‰¹å¾æ•°é‡
                n_classes=3,  # ç±»åˆ«æ•°
                d_model=d_model,
                n_heads=n_heads,
                n_layers=n_layers,
                dropout=dropout,
                use_distilling=True
            ).to(device)
            
            # 6. å®šä¹‰GMADLæŸå¤±å‡½æ•°ï¼ˆå…³é”®åˆ›æ–°ï¼ï¼‰
            criterion = GMADLossWithHOLDPenalty(
                hold_penalty=0.65,  # ä¸å…¶ä»–æ¨¡å‹ä¿æŒä¸€è‡´
                alpha=alpha,  # é²æ£’æ€§å‚æ•°
                beta=beta    # å‡¸æ€§å‚æ•°ï¼ˆè®ºæ–‡æ¨èï¼‰
            )
            
            # 7. å®šä¹‰ä¼˜åŒ–å™¨
            optimizer = torch.optim.Adam(
                model.parameters(),
                lr=lr,
                weight_decay=1e-5  # L2æ­£åˆ™åŒ–
            )
            
            # 8. å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=epochs,
                eta_min=1e-6
            )
            
            # 9. è®­ç»ƒå¾ªç¯
            model.train()
            best_loss = float('inf')
            
            for epoch in range(epochs):
                epoch_loss = 0.0
                correct = 0
                total = 0
                
                for batch_X, batch_y in dataloader:
                    batch_X = batch_X.to(device)
                    batch_y = batch_y.to(device)
                    
                    # å‰å‘ä¼ æ’­
                    logits = model(batch_X)
                    loss = criterion(logits, batch_y)
                    
                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    
                    # ç»Ÿè®¡
                    epoch_loss += loss.item()
                    _, predicted = torch.max(logits, 1)
                    total += batch_y.size(0)
                    correct += (predicted == batch_y).sum().item()
                
                # æ›´æ–°å­¦ä¹ ç‡
                scheduler.step()
                
                # è®¡ç®—å‡†ç¡®ç‡
                epoch_loss /= len(dataloader)
                epoch_acc = 100.0 * correct / total
                
                # æ¯10è½®æˆ–æœ€å1è½®æ‰“å°è¿›åº¦
                if (epoch + 1) % 10 == 0 or epoch == self.informer_epochs - 1:
                    logger.info(f"   Epoch [{epoch+1}/{self.informer_epochs}] "
                               f"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if epoch_loss < best_loss:
                    best_loss = epoch_loss
            
            # 9. åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
            model.eval()
            
            # 10. åŒ…è£…æ¨¡å‹ä»¥å…¼å®¹scikit-learnæ¥å£
            class InformerWrapper:
                """åŒ…è£…Informer-2æ¨¡å‹ï¼Œæä¾›predict_probaæ¥å£"""
                
                def __init__(self, model, device):
                    self.model = model
                    self.device = device
                
                def predict_proba(self, X):
                    """
                    é¢„æµ‹æ¦‚ç‡ï¼ˆå…¼å®¹scikit-learnï¼‰
                    
                    Args:
                        X: NumPyæ•°ç»„æˆ–Pandas DataFrame
                    
                    Returns:
                        æ¦‚ç‡æ•°ç»„ (n_samples, n_classes)
                    """
                    self.model.eval()
                    with torch.no_grad():
                        if isinstance(X, pd.DataFrame):
                            X = X.values
                        X_tensor = torch.FloatTensor(X).to(self.device)
                        probs = self.model.predict_proba(X_tensor)
                        return probs.cpu().numpy()
                
                def predict(self, X):
                    """
                    é¢„æµ‹ç±»åˆ«ï¼ˆå…¼å®¹scikit-learnï¼‰
                    
                    Args:
                        X: NumPyæ•°ç»„æˆ–Pandas DataFrame
                    
                    Returns:
                        é¢„æµ‹ç±»åˆ«æ•°ç»„
                    """
                    probs = self.predict_proba(X)
                    return np.argmax(probs, axis=1)
            
            wrapped_model = InformerWrapper(model, device)
            
            training_time = time.time() - start_time
            logger.info(f"âœ… Informer-2è®­ç»ƒå®Œæˆ: æœ€ä½³Loss={best_loss:.4f}, "
                       f"è€—æ—¶={training_time:.2f}ç§’")
            
            return wrapped_model
            
        except Exception as e:
            logger.error(f"Informer-2è®­ç»ƒå¤±è´¥: {e}")
            logger.warning("âš ï¸ å°†è·³è¿‡Informer-2ï¼Œä»…ä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹")
            return None
    
    async def predict(
        self, 
        data: pd.DataFrame, 
        timeframe: str
    ) -> Dict[str, Any]:
        """
        é›†æˆé¢„æµ‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼‰
        
        Args:
            data: Kçº¿æ•°æ®DataFrame
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        try:
            # æ£€æŸ¥é›†æˆæ¨¡å‹æ˜¯å¦å­˜åœ¨
            if timeframe not in self.ensemble_models:
                logger.warning(f"âš ï¸ {timeframe} é›†æˆæ¨¡å‹æœªè®­ç»ƒï¼Œé™çº§åˆ°å•æ¨¡å‹")
                return await super().predict(data, timeframe)
            
            # ç‰¹å¾å·¥ç¨‹
            processed_data = self.feature_engineer.create_features(data.copy())
            if processed_data.empty:
                return None
            
            # å‡†å¤‡ç‰¹å¾ï¼ˆä½¿ç”¨è¯¥æ—¶é—´æ¡†æ¶çš„ç‰¹å¾åˆ—ï¼‰
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            if not feature_columns:
                logger.error(f"{timeframe} ç‰¹å¾åˆ—æœªæ‰¾åˆ°")
                return None
            
            X = processed_data.iloc[-1:][feature_columns]
            if len(X) == 0:
                return None
            
            # ç‰¹å¾ç¼©æ”¾
            X_scaled = self._scale_features(X, timeframe, fit=False)
            
            # è·å–é›†æˆæ¨¡å‹
            models = self.ensemble_models[timeframe]
            
            # ä¸‰ä¸ªåŸºç¡€æ¨¡å‹é¢„æµ‹ï¼ˆX_scaledå¯èƒ½æ˜¯numpyæ•°ç»„æˆ–DataFrameï¼‰
            if isinstance(X_scaled, np.ndarray):
                X_pred = X_scaled
            else:
                X_pred = X_scaled.iloc[[-1]] if hasattr(X_scaled, 'iloc') else X_scaled
            
            # ğŸ”‘ åŸºç¡€æ¨¡å‹é¢„æµ‹ï¼ˆä½¿ç”¨çŸ­é”®åï¼‰
            lgb_proba = models['lgb'].predict_proba(X_pred)[0]
            xgb_proba = models['xgb'].predict_proba(X_pred)[0]
            cat_proba = models['cat'].predict_proba(X_pred)[0]
            
            lgb_pred = models['lgb'].predict(X_pred)[0]
            xgb_pred = models['xgb'].predict(X_pred)[0]
            cat_pred = models['cat'].predict(X_pred)[0]
            
            # ğŸ¤– Informer-2é¢„æµ‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'inf' in models:
                inf_proba = models['inf'].predict_proba(X_pred)[0]
                inf_pred = models['inf'].predict(X_pred)[0]
            else:
                inf_proba = None
                inf_pred = None
            
            # Stackingé¢„æµ‹ï¼ˆä½¿ç”¨å…ƒå­¦ä¹ å™¨ï¼‰
            if 'meta' in models:
                # ğŸ†• ç”Ÿæˆå¢å¼ºå…ƒç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                from scipy.special import entr
                
                # 1. æ¨¡å‹ä¸€è‡´æ€§
                if inf_proba is not None:
                    agreement = float((lgb_pred == xgb_pred) and (xgb_pred == cat_pred) and (cat_pred == inf_pred))
                else:
                    agreement = float((lgb_pred == xgb_pred) and (xgb_pred == cat_pred))
                
                # 2. æœ€å¤§æ¦‚ç‡
                lgb_max_prob = lgb_proba.max()
                xgb_max_prob = xgb_proba.max()
                cat_max_prob = cat_proba.max()
                
                # 3. æ¦‚ç‡ç†µï¼ˆå•ä¸ªæ ·æœ¬ï¼‰
                lgb_entropy = entr(lgb_proba).sum()
                xgb_entropy = entr(xgb_proba).sum()
                cat_entropy = entr(cat_proba).sum()
                
                # 4. å¹³å‡æ¦‚ç‡
                if inf_proba is not None:
                    inf_max_prob = inf_proba.max()
                    inf_entropy = entr(inf_proba).sum()
                    avg_proba = (lgb_proba + xgb_proba + cat_proba + inf_proba) / 4
                else:
                    avg_proba = (lgb_proba + xgb_proba + cat_proba) / 3
                
                # 5. æ¦‚ç‡æ ‡å‡†å·®
                if inf_proba is not None:
                    prob_std = np.std(np.stack([lgb_proba, xgb_proba, cat_proba, inf_proba]), axis=0)
                else:
                    prob_std = np.std(np.stack([lgb_proba, xgb_proba, cat_proba]), axis=0)
                prob_std_max = prob_std.max()
                
                # ğŸ”‘ æ‹¼æ¥æ‰€æœ‰å…ƒç‰¹å¾ï¼ˆ20ä¸ªæˆ–23ä¸ªï¼‰
                if inf_proba is not None:
                    # åŒ…å«Informer-2ï¼ˆ23ä¸ªç‰¹å¾ï¼‰
                    meta_features = np.hstack([
                        lgb_proba,           # 3ä¸ª
                        xgb_proba,           # 3ä¸ª
                        cat_proba,           # 3ä¸ª
                        inf_proba,           # 3ä¸ª â† Informer-2
                        [agreement],         # 1ä¸ª
                        [lgb_max_prob],      # 1ä¸ª
                        [xgb_max_prob],      # 1ä¸ª
                        [cat_max_prob],      # 1ä¸ª
                        [inf_max_prob],      # 1ä¸ª â† Informer-2
                        [lgb_entropy],       # 1ä¸ª
                        [xgb_entropy],       # 1ä¸ª
                        [cat_entropy],       # 1ä¸ª
                        [inf_entropy],       # 1ä¸ª â† Informer-2
                        avg_proba,           # 3ä¸ª
                        [prob_std_max]       # 1ä¸ª
                    ]).reshape(1, -1)  # (1, 23)
                else:
                    # ä»…ä¼ ç»Ÿæ¨¡å‹ï¼ˆ20ä¸ªç‰¹å¾ï¼‰
                    meta_features = np.hstack([
                        lgb_proba,           # 3ä¸ª
                        xgb_proba,           # 3ä¸ª
                        cat_proba,           # 3ä¸ª
                        [agreement],         # 1ä¸ª
                        [lgb_max_prob],      # 1ä¸ª
                        [xgb_max_prob],      # 1ä¸ª
                        [cat_max_prob],      # 1ä¸ª
                        [lgb_entropy],       # 1ä¸ª
                        [xgb_entropy],       # 1ä¸ª
                        [cat_entropy],       # 1ä¸ª
                        avg_proba,           # 3ä¸ª
                        [prob_std_max]       # 1ä¸ª
                    ]).reshape(1, -1)  # (1, 20)
                
                # å…ƒå­¦ä¹ å™¨é¢„æµ‹
                stacking_proba = models['meta'].predict_proba(meta_features)[0]
                final_pred = stacking_proba.argmax()
                confidence = stacking_proba[final_pred]
                final_probabilities = stacking_proba  # ä½¿ç”¨å…ƒå­¦ä¹ å™¨æ¦‚ç‡
            else:
                # é™çº§ï¼šç®€å•åŠ æƒå¹³å‡ï¼ˆå¦‚æœå…ƒå­¦ä¹ å™¨ä¸å­˜åœ¨ï¼‰
                weights = self.fallback_weights
                ensemble_proba = (
                    lgb_proba * weights['lgb'] +
                    xgb_proba * weights['xgb'] +
                    cat_proba * weights['cat']
                )
                final_pred = ensemble_proba.argmax()
                confidence = ensemble_proba[final_pred]
                final_probabilities = ensemble_proba  # ä½¿ç”¨åŠ æƒå¹³å‡æ¦‚ç‡
            
            # æ˜ å°„åˆ°ä¿¡å·ç±»å‹
            signal_map = {0: 'SHORT', 1: 'HOLD', 2: 'LONG'}
            signal_type = signal_map[final_pred]
            
            # ç®€æ´è®°å½•é¢„æµ‹ç»“æœ
            from app.utils.helpers import format_signal_type
            logger.info(f"ğŸ¯ {timeframe} Stackingé¢„æµ‹: {format_signal_type(signal_type)} "
                       f"(ç½®ä¿¡åº¦={confidence:.4f}, æ¦‚ç‡: ğŸ“‰{final_probabilities[0]:.2f} â¸ï¸{final_probabilities[1]:.2f} ğŸ“ˆ{final_probabilities[2]:.2f})")
            
            # è¿”å›å€¼æ ¼å¼ä¸çˆ¶ç±»ä¸€è‡´
            return {
                'signal_type': signal_type,
                'confidence': float(confidence),
                'probabilities': {
                    'short': float(final_probabilities[0]),
                    'hold': float(final_probabilities[1]),
                    'long': float(final_probabilities[2])
                },
                'timestamp': datetime.now(),
                'model_version': '2.0_stacking_ensemble'
            }
            
        except Exception as e:
            logger.error(f"é›†æˆé¢„æµ‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {}
    
    def _save_ensemble_models(self, timeframe: str):
        """ä¿å­˜é›†æˆæ¨¡å‹"""
        try:
            models = self.ensemble_models[timeframe]
            model_dir = Path(self.model_dir)  # ä½¿ç”¨çˆ¶ç±»çš„model_dir
            model_dir.mkdir(parents=True, exist_ok=True)
            
            # ğŸ”‘ ä¿å­˜æ¨¡å‹ï¼ˆæ”¯æŒInformer-2ï¼‰
            model_mapping = {
                'lgb': 'lgb',
                'xgb': 'xgb',
                'cat': 'cat',
                'meta': 'meta'
            }
            
            saved_count = 0
            for short_name in model_mapping:
                if short_name in models:
                    filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_{short_name}_model.pkl"
                    with open(filepath, 'wb') as f:
                        pickle.dump(models[short_name], f)
                    saved_count += 1
            
            # ä¿å­˜Informer-2ï¼ˆPyTorchæ¨¡å‹ï¼Œä½¿ç”¨torch.saveï¼‰
            if 'inf' in models and TORCH_AVAILABLE:
                inf_filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_inf_model.pt"
                # ä¿å­˜æ•´ä¸ªwrapperå¯¹è±¡ï¼ˆåŒ…å«æ¨¡å‹å’Œdeviceï¼‰
                with open(inf_filepath, 'wb') as f:
                    pickle.dump(models['inf'], f)
                saved_count += 1
                logger.info(f"   âœ… Informer-2æ¨¡å‹å·²ä¿å­˜: {inf_filepath.name}")
            
            # ğŸ”¥ ä¿å­˜scalerå’Œfeaturesï¼ˆå…³é”®ï¼é¢„æµ‹æ—¶éœ€è¦ï¼‰
            if timeframe in self.scalers:
                scaler_path = model_dir / f"{settings.SYMBOL}_{timeframe}_scaler.pkl"
                with open(scaler_path, 'wb') as f:
                    pickle.dump(self.scalers[timeframe], f)
                saved_count += 1
            
            if timeframe in self.feature_columns_dict:
                features_path = model_dir / f"{settings.SYMBOL}_{timeframe}_features.pkl"
                with open(features_path, 'wb') as f:
                    pickle.dump(self.feature_columns_dict[timeframe], f)
                saved_count += 1
            
            if saved_count > 0:
                logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹ä¿å­˜å®Œæˆï¼ˆ{saved_count}ä¸ªæ–‡ä»¶ï¼‰")
            else:
                logger.warning(f"âš ï¸ {timeframe} æ²¡æœ‰æ¨¡å‹è¢«ä¿å­˜ï¼ˆé”®å: {list(models.keys())}ï¼‰")
            
        except Exception as e:
            logger.error(f"ä¿å­˜é›†æˆæ¨¡å‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def _load_ensemble_models(self, timeframe: str) -> bool:
        """åŠ è½½é›†æˆæ¨¡å‹ï¼ˆæ”¯æŒInformer-2ï¼‰"""
        try:
            model_dir = Path(self.model_dir)  # ä½¿ç”¨çˆ¶ç±»çš„model_dir
            models = {}
            
            # ğŸ”‘ åŠ è½½ä¼ ç»Ÿæ¨¡å‹ï¼ˆå¿…éœ€ï¼‰
            model_mapping = {
                'lgb': 'lgb',
                'xgb': 'xgb',
                'cat': 'cat',
                'meta': 'meta'
            }
            
            # æ£€æŸ¥å¿…éœ€æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            for short_name in model_mapping:
                filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_{short_name}_model.pkl"
                if not filepath.exists():
                    logger.warning(f"âš ï¸ {timeframe} {short_name}æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                    return False
            
            # åŠ è½½æ‰€æœ‰ä¼ ç»Ÿæ¨¡å‹
            for short_name in model_mapping:
                filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_{short_name}_model.pkl"
                with open(filepath, 'rb') as f:
                    models[short_name] = pickle.load(f)
            
            # ğŸ¤– åŠ è½½Informer-2æ¨¡å‹ï¼ˆå¯é€‰ï¼Œå¦‚æœå­˜åœ¨ï¼‰
            if TORCH_AVAILABLE:
                inf_filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_inf_model.pt"
                if inf_filepath.exists():
                    with open(inf_filepath, 'rb') as f:
                        models['inf'] = pickle.load(f)
                    logger.info(f"   âœ… Informer-2æ¨¡å‹å·²åŠ è½½")
            
            self.ensemble_models[timeframe] = models
            
            # ğŸ”¥ åŠ è½½scalerå’Œfeaturesï¼ˆå…³é”®ï¼é¢„æµ‹æ—¶éœ€è¦ï¼‰
            scaler_path = model_dir / f"{settings.SYMBOL}_{timeframe}_scaler.pkl"
            if scaler_path.exists():
                with open(scaler_path, 'rb') as f:
                    self.scalers[timeframe] = pickle.load(f)
            
            features_path = model_dir / f"{settings.SYMBOL}_{timeframe}_features.pkl"
            if features_path.exists():
                with open(features_path, 'rb') as f:
                    self.feature_columns_dict[timeframe] = pickle.load(f)
            
            logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹åŠ è½½å®Œæˆï¼ˆ{len(models)}ä¸ªæ¨¡å‹ï¼‰")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½é›†æˆæ¨¡å‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    async def train_model(self, force_retrain: bool = False) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œä½¿ç”¨Stackingé›†æˆï¼‰
        
        Args:
            force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒ
        
        Returns:
            è®­ç»ƒç»“æœå’Œå¹³å‡å‡†ç¡®ç‡
        """
        try:
            # è°ƒç”¨é›†æˆè®­ç»ƒ
            result = await self.train_all_timeframes()
            
            return {
                'accuracy': result['average_accuracy'],
                'timeframes': result['results'],
                'method': 'Stacking Ensemble',
                'models': ['LightGBM', 'XGBoost', 'CatBoost']
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    async def start(self):
        """å¯åŠ¨é›†æˆMLæœåŠ¡"""
        try:
            logger.info("å¯åŠ¨Stackingé›†æˆæœºå™¨å­¦ä¹ æœåŠ¡...")
            
            # å°è¯•åŠ è½½å·²æœ‰é›†æˆæ¨¡å‹
            all_loaded = True
            for timeframe in settings.TIMEFRAMES:
                if not self._load_ensemble_models(timeframe):
                    all_loaded = False
                    break
            
            if all_loaded:
                logger.info("âœ… æ‰€æœ‰é›†æˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°é›†æˆæ¨¡å‹ï¼Œéœ€è¦è®­ç»ƒ")
            
            logger.info("Stackingé›†æˆMLæœåŠ¡å¯åŠ¨å®Œæˆï¼ˆè®­ç»ƒç”±schedulerç®¡ç†ï¼‰")
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆMLæœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
            raise

# å…¨å±€é›†æˆMLæœåŠ¡å®ä¾‹
ensemble_ml_service = EnsembleMLService()

