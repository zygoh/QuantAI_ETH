"""
é›†æˆæœºå™¨å­¦ä¹ æœåŠ¡ - Stackingä¸‰æ¨¡å‹èåˆ
"""
import logging
from typing import Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
import lightgbm as lgb
from datetime import datetime
import pickle
from pathlib import Path

from app.services.ml_service import MLService
from app.core.config import settings

logger = logging.getLogger(__name__)

class EnsembleMLService(MLService):
    """
    é›†æˆæœºå™¨å­¦ä¹ æœåŠ¡ï¼ˆStackingï¼‰
    
    ä½¿ç”¨LightGBM + XGBoost + CatBoostä¸‰æ¨¡å‹Stackingèåˆ
    ç›®æ ‡ï¼šå‡†ç¡®ç‡ä»42.81%æå‡åˆ°50%+
    """
    
    def __init__(self):
        super().__init__()
        
        # é›†æˆæ¨¡å‹å­—å…¸ {timeframe: {lgb, xgb, cat, meta}}
        self.ensemble_models = {}
        
        # é›†æˆæƒé‡ï¼ˆStackingè‡ªåŠ¨å­¦ä¹ ï¼Œè¿™é‡Œä½œä¸ºé™çº§æ–¹æ¡ˆï¼‰
        self.fallback_weights = {
            'lgb': 0.4,
            'xgb': 0.3,
            'cat': 0.3
        }
        
        logger.info("âœ… é›†æˆMLæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆStackingä¸‰æ¨¡å‹èåˆï¼‰")
    
    async def train_all_timeframes(self) -> Dict[str, Any]:
        """
        è®­ç»ƒæ‰€æœ‰æ—¶é—´æ¡†æ¶çš„é›†æˆæ¨¡å‹
        
        Returns:
            è®­ç»ƒç»“æœå’ŒæŒ‡æ ‡
        """
        try:
            logger.info("ğŸš€ å¼€å§‹Stackingé›†æˆæ¨¡å‹è®­ç»ƒ...")
            logger.info(f"ä¸‰æ¨¡å‹èåˆ: LightGBM + XGBoost + CatBoost")
            logger.info(f"æ—¶é—´æ¡†æ¶: {settings.TIMEFRAMES}")
            logger.info("")
            
            results = {}
            
            for timeframe in settings.TIMEFRAMES:
                logger.info("=" * 60)
                logger.info(f"ğŸ“Š è®­ç»ƒ {timeframe} é›†æˆæ¨¡å‹...")
                logger.info("=" * 60)
                
                result = await self._train_ensemble_single_timeframe(timeframe)
                results[timeframe] = result
                
                logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {result['accuracy']:.4f}")
                logger.info("")
            
            # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
            avg_accuracy = np.mean([r['accuracy'] for r in results.values()])
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ Stackingé›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
            logger.info(f"æˆåŠŸè®­ç»ƒ: {len(results)}/{len(settings.TIMEFRAMES)} ä¸ªæ—¶é—´æ¡†æ¶")
            logger.info(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
            logger.info("=" * 60)
            logger.info("")
            
            return {
                'results': results,
                'average_accuracy': avg_accuracy,
                'training_date': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    async def _train_ensemble_single_timeframe(self, timeframe: str) -> Dict[str, Any]:
        """
        è®­ç»ƒå•ä¸ªæ—¶é—´æ¡†æ¶çš„Stackingé›†æˆæ¨¡å‹
        
        æµç¨‹:
        1. å‡†å¤‡æ•°æ®
        2. è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆLightGBM, XGBoost, CatBoostï¼‰
        3. ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆåŸºç¡€æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ï¼‰
        4. è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰
        5. è¯„ä¼°é›†æˆæ•ˆæœ
        """
        try:
            # 1ï¸âƒ£ å‡†å¤‡è®­ç»ƒæ•°æ®
            logger.info(f"ğŸ“¥ è·å– {timeframe} è®­ç»ƒæ•°æ®...")
            data = await self._prepare_training_data_for_timeframe(timeframe)
            logger.info(f"âœ… {timeframe} æ•°æ®è·å–æˆåŠŸ: {len(data)}æ¡")
            
            # 2ï¸âƒ£ ç‰¹å¾å·¥ç¨‹
            data = self.feature_engineer.create_features(data)
            
            # 3ï¸âƒ£ åˆ›å»ºæ ‡ç­¾
            data = self._create_labels(data, timeframe=timeframe)
            
            # 4ï¸âƒ£ å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
            X, y = self._prepare_features_labels(data, timeframe)
            
            # 5ï¸âƒ£ ç‰¹å¾ç¼©æ”¾
            X_scaled = self._scale_features(X, timeframe, fit=True)
            
            # 6ï¸âƒ£ æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆè®­ç»ƒé›†80%ï¼ŒéªŒè¯é›†20%ï¼‰
            split_idx = int(len(X_scaled) * 0.8)
            
            # ğŸ”‘ X_scaledæ˜¯numpyæ•°ç»„ï¼Œyæ˜¯Seriesï¼Œåˆ†åˆ«å¤„ç†
            if isinstance(X_scaled, np.ndarray):
                X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]
            else:
                X_train, X_val = X_scaled.iloc[:split_idx], X_scaled.iloc[split_idx:]
            
            y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
            
            logger.info(f"ğŸ“Š {timeframe} æ•°æ®åˆ†å‰²: è®­ç»ƒ{len(X_train)}æ¡, éªŒè¯{len(X_val)}æ¡")
            
            # 7ï¸âƒ£ è®­ç»ƒStackingé›†æˆæ¨¡å‹
            logger.info(f"ğŸš‚ å¼€å§‹è®­ç»ƒ {timeframe} Stackingé›†æˆ...")
            ensemble_result = self._train_stacking_ensemble(
                X_train, y_train, X_val, y_val, timeframe
            )
            
            # 8ï¸âƒ£ ä¿å­˜é›†æˆæ¨¡å‹
            self._save_ensemble_models(timeframe)
            
            logger.info(f"â±ï¸ {timeframe} è®­ç»ƒè€—æ—¶: {ensemble_result['training_time']:.2f}ç§’")
            
            return ensemble_result
            
        except Exception as e:
            logger.error(f"âŒ {timeframe} é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    def _train_stacking_ensemble(
        self, 
        X_train: pd.DataFrame, 
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        timeframe: str
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒStackingé›†æˆæ¨¡å‹
        
        Stackingæ–¹æ³•:
        1. è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆLightGBM, XGBoost, CatBoostï¼‰
        2. ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆé¢„æµ‹æ¦‚ç‡ï¼‰
        3. è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆLogisticRegressionï¼‰å­¦ä¹ å¦‚ä½•ç»„åˆ
        
        ä¼˜åŠ¿:
        - æ¯”ç®€å•åŠ æƒæ›´æ™ºèƒ½
        - è‡ªåŠ¨å­¦ä¹ æ¯ä¸ªæ¨¡å‹çš„å¼ºé¡¹
        - æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
        """
        import time
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
        
        start_time = time.time()
        
        logger.info(f"ğŸ¯ Stage 1: è®­ç»ƒ3ä¸ªåŸºç¡€æ¨¡å‹...")
        
        # 1. è®­ç»ƒLightGBMï¼ˆåŸºç¡€æ¨¡å‹1ï¼‰
        logger.info(f"  ğŸ“Š è®­ç»ƒLightGBM...")
        lgb_model = self._train_lightgbm(X_train, y_train, timeframe)
        
        # 2. è®­ç»ƒXGBoostï¼ˆåŸºç¡€æ¨¡å‹2ï¼‰
        logger.info(f"  ğŸ“Š è®­ç»ƒXGBoost...")
        xgb_model = self._train_xgboost(X_train, y_train, timeframe)
        
        # 3. è®­ç»ƒCatBoostï¼ˆåŸºç¡€æ¨¡å‹3ï¼‰
        logger.info(f"  ğŸ“Š è®­ç»ƒCatBoost...")
        cat_model = self._train_catboost(X_train, y_train, timeframe)
        
        logger.info(f"âœ… 3ä¸ªåŸºç¡€æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # 4. ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆè®­ç»ƒé›†ï¼‰
        logger.info(f"ğŸ¯ Stage 2: ç”Ÿæˆå…ƒç‰¹å¾...")
        lgb_pred_train = lgb_model.predict_proba(X_train)
        xgb_pred_train = xgb_model.predict_proba(X_train)
        cat_pred_train = cat_model.predict_proba(X_train)
        
        # åˆå¹¶å…ƒç‰¹å¾ï¼ˆ9ç»´ï¼šæ¯ä¸ªæ¨¡å‹3ä¸ªç±»åˆ«æ¦‚ç‡ï¼‰
        meta_features_train = np.hstack([
            lgb_pred_train,
            xgb_pred_train,
            cat_pred_train
        ])
        
        # 5. è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰
        logger.info(f"ğŸ¯ Stage 3: è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰...")
        meta_learner = LogisticRegression(
            multi_class='multinomial',
            solver='lbfgs',
            max_iter=1000,
            random_state=42
        )
        meta_learner.fit(meta_features_train, y_train)
        
        logger.info(f"âœ… å…ƒå­¦ä¹ å™¨è®­ç»ƒå®Œæˆ")
        
        # 6. éªŒè¯é›†è¯„ä¼°
        logger.info(f"ğŸ¯ Stage 4: éªŒè¯é›†è¯„ä¼°...")
        
        # ç”ŸæˆéªŒè¯é›†å…ƒç‰¹å¾
        lgb_pred_val = lgb_model.predict_proba(X_val)
        xgb_pred_val = xgb_model.predict_proba(X_val)
        cat_pred_val = cat_model.predict_proba(X_val)
        
        meta_features_val = np.hstack([
            lgb_pred_val,
            xgb_pred_val,
            cat_pred_val
        ])
        
        # Stackingé¢„æµ‹
        stacking_pred = meta_learner.predict(meta_features_val)
        stacking_proba = meta_learner.predict_proba(meta_features_val)
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_val, stacking_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_val, stacking_pred, average='weighted', zero_division=0
        )
        
        # è®¡ç®—AUCï¼ˆå¤šåˆ†ç±»ä½¿ç”¨OvRï¼‰
        try:
            auc = roc_auc_score(y_val, stacking_proba, multi_class='ovr', average='weighted')
        except:
            auc = 0.5
        
        # å„åŸºç¡€æ¨¡å‹å•ç‹¬å‡†ç¡®ç‡
        lgb_acc = accuracy_score(y_val, lgb_model.predict(X_val))
        xgb_acc = accuracy_score(y_val, xgb_model.predict(X_val))
        cat_acc = accuracy_score(y_val, cat_model.predict(X_val))
        
        training_time = time.time() - start_time
        
        # ä¿å­˜é›†æˆæ¨¡å‹
        self.ensemble_models[timeframe] = {
            'lgb': lgb_model,
            'xgb': xgb_model,
            'cat': cat_model,
            'meta': meta_learner
        }
        
        # æ—¥å¿—è¾“å‡º
        logger.info(f"ğŸ“Š {timeframe} Stackingé›†æˆè¯„ä¼°:")
        logger.info(f"  åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡:")
        logger.info(f"    LightGBM: {lgb_acc:.4f}")
        logger.info(f"    XGBoost:  {xgb_acc:.4f}")
        logger.info(f"    CatBoost: {cat_acc:.4f}")
        logger.info(f"  Stackingå‡†ç¡®ç‡: {accuracy:.4f}")
        logger.info(f"  æå‡: +{(accuracy - max(lgb_acc, xgb_acc, cat_acc))*100:.2f}%")
        logger.info(f"  ç²¾ç¡®ç‡: {precision:.4f}")
        logger.info(f"  å¬å›ç‡: {recall:.4f}")
        logger.info(f"  F1åˆ†æ•°: {f1:.4f}")
        logger.info(f"  AUC: {auc:.4f}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'base_models': {
                'lgb': lgb_acc,
                'xgb': xgb_acc,
                'cat': cat_acc
            },
            'training_time': training_time
        }
    
    def _train_lightgbm(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str):
        """
        è®­ç»ƒLightGBMæ¨¡å‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œç»Ÿä¸€ä¸‰æ¨¡å‹è®­ç»ƒä»£ç ä½ç½®ï¼‰
        
        è¦†ç›–åŸå› ï¼šä¿è¯ä»£ç ç»“æ„ç»Ÿä¸€ï¼Œä¸‰ä¸ªæ¨¡å‹è®­ç»ƒéƒ½åœ¨ensemble_ml_service.py
        """
        try:
            import lightgbm as lgb
            from sklearn.utils.class_weight import compute_sample_weight
            
            # æ ·æœ¬åŠ æƒï¼ˆç±»åˆ«å¹³è¡¡ Ã— æ—¶é—´è¡°å‡ï¼‰
            class_weights = compute_sample_weight('balanced', y_train)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            sample_weights = class_weights * time_decay
            
            # è·å–æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–å‚æ•°
            timeframe_params = self.lgb_params_by_timeframe.get(timeframe, {})
            
            # åˆå¹¶åŸºç¡€å‚æ•°å’Œå·®å¼‚åŒ–å‚æ•°
            params = {**self.base_lgb_params, **timeframe_params}
            
            logger.info(f"ğŸ“Š {timeframe} LightGBMå‚æ•°: num_leaves={params.get('num_leaves')}, "
                       f"reg_alpha={params.get('reg_alpha', 0)}, reg_lambda={params.get('reg_lambda', 0)}")
            
            # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
            model = lgb.LGBMClassifier(**params, random_state=42)
            model.fit(X_train, y_train, sample_weight=sample_weights)
            
            return model
            
        except Exception as e:
            logger.error(f"LightGBMè®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def _train_xgboost(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str):
        """è®­ç»ƒXGBoostæ¨¡å‹"""
        try:
            import xgboost as xgb
            
            # æ ·æœ¬åŠ æƒï¼ˆä¸LightGBMä¸€è‡´ï¼‰
            from sklearn.utils.class_weight import compute_sample_weight
            class_weights = compute_sample_weight('balanced', y_train)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            sample_weights = class_weights * time_decay
            
            params = {
                'max_depth': 6,
                'learning_rate': 0.05,
                'n_estimators': 300,
                'objective': 'multi:softprob',
                'num_class': 3,
                'eval_metric': 'mlogloss',
                'random_state': 42,
                'tree_method': 'hist',
                'reg_alpha': 0.3,
                'reg_lambda': 0.3,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            }
            
            model = xgb.XGBClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
            
            return model
            
        except Exception as e:
            logger.error(f"XGBoostè®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def _train_catboost(self, X_train: pd.DataFrame, y_train: pd.Series, timeframe: str):
        """è®­ç»ƒCatBoostæ¨¡å‹"""
        try:
            from catboost import CatBoostClassifier
            
            # æ ·æœ¬åŠ æƒï¼ˆä¸LightGBMä¸€è‡´ï¼‰
            from sklearn.utils.class_weight import compute_sample_weight
            class_weights = compute_sample_weight('balanced', y_train)
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            sample_weights = class_weights * time_decay
            
            params = {
                'iterations': 300,
                'learning_rate': 0.05,
                'depth': 6,
                'loss_function': 'MultiClass',
                'random_seed': 42,
                'verbose': False,
                'l2_leaf_reg': 3.0,
                'bootstrap_type': 'Bayesian',
                'bagging_temperature': 1.0,
                'subsample': 0.8
            }
            
            model = CatBoostClassifier(**params)
            model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
            
            return model
            
        except Exception as e:
            logger.error(f"CatBoostè®­ç»ƒå¤±è´¥: {e}")
            raise
    
    async def predict(
        self, 
        symbol: str, 
        timeframe: str, 
        use_stacking: bool = True
    ) -> Dict[str, Any]:
        """
        é›†æˆé¢„æµ‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼‰
        
        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´æ¡†æ¶
            use_stacking: æ˜¯å¦ä½¿ç”¨Stackingï¼ˆTrue=å…ƒå­¦ä¹ å™¨ï¼ŒFalse=ç®€å•åŠ æƒï¼‰
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        try:
            # æ£€æŸ¥é›†æˆæ¨¡å‹æ˜¯å¦å­˜åœ¨
            if timeframe not in self.ensemble_models:
                logger.warning(f"âš ï¸ {timeframe} é›†æˆæ¨¡å‹æœªè®­ç»ƒï¼Œé™çº§åˆ°å•æ¨¡å‹")
                return await super().predict(symbol, timeframe)
            
            # å‡†å¤‡é¢„æµ‹æ•°æ®
            data = await self._prepare_prediction_data(symbol, timeframe)
            if data.empty:
                return None
            
            # å‡†å¤‡ç‰¹å¾
            X = self._prepare_features_for_prediction(data, timeframe)
            if len(X) == 0:
                return None
            
            # è·å–é›†æˆæ¨¡å‹
            models = self.ensemble_models[timeframe]
            
            # è·å–æœ€åä¸€è¡Œï¼ˆæœ€æ–°æ•°æ®ï¼‰
            X_latest = X.iloc[[-1]]
            
            # ä¸‰ä¸ªåŸºç¡€æ¨¡å‹é¢„æµ‹
            lgb_proba = models['lgb'].predict_proba(X_latest)[0]
            xgb_proba = models['xgb'].predict_proba(X_latest)[0]
            cat_proba = models['cat'].predict_proba(X_latest)[0]
            
            # Stackingé¢„æµ‹
            if use_stacking and 'meta' in models:
                # ç”Ÿæˆå…ƒç‰¹å¾
                meta_features = np.hstack([lgb_proba, xgb_proba, cat_proba]).reshape(1, -1)
                
                # å…ƒå­¦ä¹ å™¨é¢„æµ‹
                stacking_proba = models['meta'].predict_proba(meta_features)[0]
                final_pred = stacking_proba.argmax()
                confidence = stacking_proba[final_pred]
                
                method = "Stacking"
            else:
                # é™çº§ï¼šç®€å•åŠ æƒå¹³å‡
                weights = self.fallback_weights
                ensemble_proba = (
                    lgb_proba * weights['lgb'] +
                    xgb_proba * weights['xgb'] +
                    cat_proba * weights['cat']
                )
                final_pred = ensemble_proba.argmax()
                confidence = ensemble_proba[final_pred]
                
                method = "Weighted"
            
            # æ˜ å°„åˆ°ä¿¡å·ç±»å‹
            signal_map = {0: 'SHORT', 1: 'HOLD', 2: 'LONG'}
            signal_type = signal_map[final_pred]
            
            return {
                'signal_type': signal_type,
                'confidence': float(confidence),
                'probabilities': {
                    'SHORT': float(lgb_proba[0] + xgb_proba[0] + cat_proba[0]) / 3,
                    'HOLD': float(lgb_proba[1] + xgb_proba[1] + cat_proba[1]) / 3,
                    'LONG': float(lgb_proba[2] + xgb_proba[2] + cat_proba[2]) / 3
                },
                'base_predictions': {
                    'lgb': {'type': signal_map[lgb_proba.argmax()], 'confidence': float(lgb_proba.max())},
                    'xgb': {'type': signal_map[xgb_proba.argmax()], 'confidence': float(xgb_proba.max())},
                    'cat': {'type': signal_map[cat_proba.argmax()], 'confidence': float(cat_proba.max())}
                },
                'method': method,
                'timestamp': datetime.now(),
                'model_version': '2.0_ensemble'
            }
            
        except Exception as e:
            logger.error(f"é›†æˆé¢„æµ‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None
    
    def _save_ensemble_models(self, timeframe: str):
        """ä¿å­˜é›†æˆæ¨¡å‹"""
        try:
            models = self.ensemble_models[timeframe]
            model_dir = Path(self.model_dir)  # ä½¿ç”¨çˆ¶ç±»çš„model_dir
            model_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜4ä¸ªæ¨¡å‹
            for model_name in ['lgb', 'xgb', 'cat', 'meta']:
                if model_name in models:
                    filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_{model_name}_model.pkl"
                    with open(filepath, 'wb') as f:
                        pickle.dump(models[model_name], f)
            
            logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹ä¿å­˜å®Œæˆï¼ˆ4ä¸ªæ¨¡å‹ï¼‰")
            
        except Exception as e:
            logger.error(f"ä¿å­˜é›†æˆæ¨¡å‹å¤±è´¥: {e}")
    
    def _load_ensemble_models(self, timeframe: str) -> bool:
        """åŠ è½½é›†æˆæ¨¡å‹"""
        try:
            model_dir = Path(self.model_dir)  # ä½¿ç”¨çˆ¶ç±»çš„model_dir
            models = {}
            
            # åŠ è½½4ä¸ªæ¨¡å‹
            for model_name in ['lgb', 'xgb', 'cat', 'meta']:
                filepath = model_dir / f"{settings.SYMBOL}_{timeframe}_{model_name}_model.pkl"
                
                if not filepath.exists():
                    logger.warning(f"âš ï¸ {timeframe} {model_name}æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
                    return False
                
                with open(filepath, 'rb') as f:
                    models[model_name] = pickle.load(f)
            
            self.ensemble_models[timeframe] = models
            logger.info(f"âœ… {timeframe} é›†æˆæ¨¡å‹åŠ è½½å®Œæˆï¼ˆ4ä¸ªæ¨¡å‹ï¼‰")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½é›†æˆæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    async def train_model(self, force_retrain: bool = False) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆè¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œä½¿ç”¨Stackingé›†æˆï¼‰
        
        Args:
            force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒ
        
        Returns:
            è®­ç»ƒç»“æœå’Œå¹³å‡å‡†ç¡®ç‡
        """
        try:
            # è°ƒç”¨é›†æˆè®­ç»ƒ
            result = await self.train_all_timeframes()
            
            return {
                'accuracy': result['average_accuracy'],
                'timeframes': result['results'],
                'method': 'Stacking Ensemble',
                'models': ['LightGBM', 'XGBoost', 'CatBoost']
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
    
    async def start(self):
        """å¯åŠ¨é›†æˆMLæœåŠ¡"""
        try:
            logger.info("å¯åŠ¨Stackingé›†æˆæœºå™¨å­¦ä¹ æœåŠ¡...")
            
            # å°è¯•åŠ è½½å·²æœ‰é›†æˆæ¨¡å‹
            all_loaded = True
            for timeframe in settings.TIMEFRAMES:
                if not self._load_ensemble_models(timeframe):
                    all_loaded = False
                    break
            
            if all_loaded:
                logger.info("âœ… æ‰€æœ‰é›†æˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°é›†æˆæ¨¡å‹ï¼Œéœ€è¦è®­ç»ƒ")
            
            logger.info("Stackingé›†æˆMLæœåŠ¡å¯åŠ¨å®Œæˆï¼ˆè®­ç»ƒç”±schedulerç®¡ç†ï¼‰")
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆMLæœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
            raise

# å…¨å±€é›†æˆMLæœåŠ¡å®ä¾‹
ensemble_ml_service = EnsembleMLService()

