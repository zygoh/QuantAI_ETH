"""
Informer-2模型 - 改进版Informer时间序列预测模型
论文: "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting" (AAAI 2021)
改进: Informer-2 (2023更新版本)

核心特性:
1. ProbSparse自注意力（降低复杂度O(L log L)）
2. 自注意力蒸馏（提取关键信息）
3. 生成式Decoder（一次性预测整个序列）

适用场景:
- 长时间序列预测
- 多时间框架融合
- 分类/回归任务
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Optional, Tuple
import logging

logger = logging.getLogger(__name__)


class ProbSparseSelfAttention(nn.Module):
    """
    ProbSparse自注意力机制（Informer核心创新）
    
    核心思想：
    - 只计算Top-K个重要的Query
    - 其他Query使用均值代替
    - 复杂度从O(L^2)降到O(L log L)
    """
    
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        factor: int = 5,
        dropout: float = 0.1
    ):
        """
        初始化ProbSparse注意力
        
        Args:
            d_model: 模型维度
            n_heads: 注意力头数
            factor: 稀疏因子（采样因子）
            dropout: Dropout比率
        """
        super(ProbSparseSelfAttention, self).__init__()
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads
        self.factor = factor
        
        assert d_model % n_heads == 0, "d_model必须能被n_heads整除"
        
        # Q, K, V投影
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        
        # 输出投影
        self.W_O = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def _prob_QK(
        self,
        Q: torch.Tensor,
        K: torch.Tensor,
        sample_k: int,
        n_top: int
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        计算Query的重要性分数（Prob Sparse核心）
        
        Args:
            Q: Query (batch, n_heads, L_Q, d_head)
            K: Key (batch, n_heads, L_K, d_head)
            sample_k: 采样K的数量
            n_top: 选择Top-N个Query
        
        Returns:
            Q_reduce: 筛选后的Query
            M_top: Top-N的Query索引
        """
        B, H, L_Q, d = Q.shape
        _, _, L_K, _ = K.shape
        
        # 1. 计算Query的Sparsity Measurement
        # 随机采样K个Key
        K_sample = K[:, :, torch.randperm(L_K)[:sample_k], :]  # (B, H, sample_k, d)
        
        # 计算Q与采样K的注意力分数
        Q_K = torch.matmul(Q, K_sample.transpose(-2, -1))  # (B, H, L_Q, sample_k)
        
        # 2. 计算Query的重要性（Sparsity Score）
        # M(q_i) = max(Q·K) - mean(Q·K)
        M = Q_K.max(dim=-1)[0] - Q_K.mean(dim=-1)  # (B, H, L_Q)
        
        # 3. 选择Top-N个重要的Query
        M_top = M.topk(n_top, dim=-1)[1]  # (B, H, n_top)
        
        # 4. 提取Top-N的Query
        # 扩展索引维度以匹配Q的形状
        M_top_expanded = M_top.unsqueeze(-1).expand(-1, -1, -1, d)  # (B, H, n_top, d)
        Q_reduce = torch.gather(Q, dim=2, index=M_top_expanded)  # (B, H, n_top, d)
        
        return Q_reduce, M_top
    
    def forward(
        self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        前向传播
        
        Args:
            queries: (batch, seq_len, d_model)
            keys: (batch, seq_len, d_model)
            values: (batch, seq_len, d_model)
            attn_mask: 注意力掩码
        
        Returns:
            output: (batch, seq_len, d_model)
            attn: 注意力权重
        """
        B, L_Q, _ = queries.shape
        _, L_K, _ = keys.shape
        
        # 1. Q, K, V投影并分多头
        Q = self.W_Q(queries).view(B, L_Q, self.n_heads, self.d_head).transpose(1, 2)
        K = self.W_K(keys).view(B, L_K, self.n_heads, self.d_head).transpose(1, 2)
        V = self.W_V(values).view(B, L_K, self.n_heads, self.d_head).transpose(1, 2)
        
        # 2. ProbSparse注意力计算
        # 采样参数
        U_part = self.factor * int(np.ceil(np.log(L_K)))  # 采样K的数量
        u = self.factor * int(np.ceil(np.log(L_Q)))  # Top-u个Query
        
        U_part = min(U_part, L_K)
        u = min(u, L_Q)
        
        # 筛选重要Query
        Q_reduce, M_top = self._prob_QK(Q, K, sample_k=U_part, n_top=u)
        
        # 3. 计算筛选后Query的注意力
        scale = 1.0 / math.sqrt(self.d_head)
        scores = torch.matmul(Q_reduce, K.transpose(-2, -1)) * scale  # (B, H, u, L_K)
        
        if attn_mask is not None:
            scores = scores.masked_fill(attn_mask == 0, -1e9)
        
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        
        # 4. 加权求和Value
        context = torch.matmul(attn, V)  # (B, H, u, d_head)
        
        # 5. 填充非Top-u的Query（使用V的均值）
        # 创建输出张量
        output = torch.zeros(B, self.n_heads, L_Q, self.d_head, device=Q.device)
        
        # 填充Top-u位置
        M_top_expanded = M_top.unsqueeze(-1).expand(-1, -1, -1, self.d_head)
        output.scatter_(dim=2, index=M_top_expanded, src=context)
        
        # 填充非Top-u位置（使用V的均值）
        V_mean = V.mean(dim=2, keepdim=True)  # (B, H, 1, d_head)
        mask = torch.ones(B, self.n_heads, L_Q, 1, device=Q.device)
        mask.scatter_(dim=2, index=M_top_expanded[:, :, :, :1], value=0)
        output = output + mask * V_mean
        
        # 6. 合并多头并输出投影
        output = output.transpose(1, 2).contiguous().view(B, L_Q, self.d_model)
        output = self.W_O(output)
        
        return output, attn


class DistillingLayer(nn.Module):
    """
    蒸馏层（Informer核心创新2）
    
    核心思想：
    - 类似MaxPooling，提取序列中的关键信息
    - 逐层减半序列长度
    - 保留重要特征，降低计算量
    """
    
    def __init__(self, kernel_size: int = 3):
        """
        初始化蒸馏层
        
        Args:
            kernel_size: 卷积核大小
        """
        super(DistillingLayer, self).__init__()
        self.conv = nn.Conv1d(
            in_channels=1,
            out_channels=1,
            kernel_size=kernel_size,
            stride=2,
            padding=(kernel_size - 1) // 2
        )
        self.activation = nn.ELU()
        self.pooling = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        前向传播
        
        Args:
            x: (batch, seq_len, d_model)
        
        Returns:
            output: (batch, seq_len//2, d_model)
        """
        B, L, D = x.shape
        
        # 对每个特征维度分别进行蒸馏
        x_reshaped = x.transpose(1, 2)  # (B, D, L)
        outputs = []
        
        for i in range(D):
            feature = x_reshaped[:, i:i+1, :]  # (B, 1, L)
            feature = self.conv(feature)  # (B, 1, L//2)
            feature = self.activation(feature)
            feature = self.pooling(feature)  # (B, 1, L//4)
            outputs.append(feature)
        
        output = torch.cat(outputs, dim=1)  # (B, D, L//4)
        output = output.transpose(1, 2)  # (B, L//4, D)
        
        return output


class InformerEncoderLayer(nn.Module):
    """
    Informer Encoder层
    
    组成：
    1. ProbSparse自注意力
    2. Feed-Forward Network
    3. Layer Normalization
    4. Residual Connection
    """
    
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        d_ff: int,
        factor: int = 5,
        dropout: float = 0.1,
        activation: str = 'gelu'
    ):
        """
        初始化Encoder层
        
        Args:
            d_model: 模型维度
            n_heads: 注意力头数
            d_ff: Feed-Forward维度
            factor: 稀疏因子
            dropout: Dropout比率
            activation: 激活函数
        """
        super(InformerEncoderLayer, self).__init__()
        
        # 1. 自注意力
        self.attention = ProbSparseSelfAttention(
            d_model=d_model,
            n_heads=n_heads,
            factor=factor,
            dropout=dropout
        )
        
        # 2. Feed-Forward Network
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU() if activation == 'gelu' else nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        
        # 3. Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # 4. Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(
        self,
        x: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        前向传播
        
        Args:
            x: (batch, seq_len, d_model)
            attn_mask: 注意力掩码
        
        Returns:
            output: (batch, seq_len, d_model)
            attn: 注意力权重
        """
        # 1. 自注意力 + 残差连接
        attn_output, attn = self.attention(x, x, x, attn_mask)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)
        
        # 2. Feed-Forward + 残差连接
        ff_output = self.ff(x)
        x = x + ff_output
        x = self.norm2(x)
        
        return x, attn


class Informer2ForClassification(nn.Module):
    """
    Informer-2分类模型（完整版，用于交易信号预测）
    
    架构：
    1. 输入嵌入（特征投影）
    2. 多层Encoder + ProbSparse自注意力
    3. 蒸馏层（序列长度压缩）
    4. 全局池化
    5. 分类头（输出LONG/HOLD/SHORT概率）
    
    核心特性：
    - ProbSparse Self-Attention (O(L log L)复杂度)
    - Distilling Layers (关键信息提取)
    - 多层Encoder (完整Informer架构)
    """
    
    def __init__(
        self,
        n_features: int,
        n_classes: int = 3,
        d_model: int = 128,
        n_heads: int = 8,
        n_layers: int = 3,
        d_ff: int = 512,
        factor: int = 5,
        dropout: float = 0.1,
        use_distilling: bool = True
    ):
        """
        初始化Informer-2分类模型（完整版）
        
        Args:
            n_features: 输入特征数量
            n_classes: 类别数（3: LONG/HOLD/SHORT）
            d_model: 模型维度
            n_heads: 注意力头数
            n_layers: Encoder层数（完整多层架构）
            d_ff: Feed-Forward维度
            factor: 稀疏因子
            dropout: Dropout比率
            use_distilling: 是否使用蒸馏层（推荐True）
        """
        super(Informer2ForClassification, self).__init__()
        
        self.n_features = n_features
        self.n_classes = n_classes
        self.d_model = d_model
        self.use_distilling = use_distilling
        
        # 1. 输入投影（特征→模型维度）
        self.input_projection = nn.Linear(n_features, d_model)
        
        # 2. 多层Encoder + 蒸馏层（完整Informer架构）
        self.encoder_layers = nn.ModuleList([
            InformerEncoderLayer(
                d_model=d_model,
                n_heads=n_heads,
                d_ff=d_ff,
                factor=factor,
                dropout=dropout
            ) for _ in range(n_layers)
        ])
        
        # 3. 蒸馏层（如果启用）
        if use_distilling:
            self.distilling_layers = nn.ModuleList([
                DistillingLayer() for _ in range(n_layers - 1)
            ])
        else:
            self.distilling_layers = None
        
        # 4. 分类头
        self.classifier = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, n_classes)
        )
        
        logger.info(f"✅ Informer-2模型初始化完成:")
        logger.info(f"   特征数: {n_features}, 类别数: {n_classes}")
        logger.info(f"   模型维度: {d_model}, 注意力头: {n_heads}")
        logger.info(f"   Encoder层: {n_layers}, 蒸馏: {use_distilling}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        前向传播（完整Informer-2架构）
        
        处理流程：
        1. 输入投影：特征维度转换
        2. 多层Encoder：ProbSparse自注意力处理
        3. 蒸馏层：序列长度压缩，提取关键信息
        4. 全局池化：聚合序列信息
        5. 分类头：输出类别概率
        
        Args:
            x: (batch, seq_len, n_features) - 序列输入
        
        Returns:
            logits: (batch, n_classes) - 分类logits
        """
        # 1. 输入投影：(batch, seq_len, n_features) → (batch, seq_len, d_model)
        x = self.input_projection(x)
        
        # 2. 多层Encoder + 蒸馏层处理
        for i, encoder_layer in enumerate(self.encoder_layers):
            # Encoder处理
            x, _ = encoder_layer(x)  # (batch, seq_len, d_model)
            
            # 蒸馏层处理（除了最后一层）
            if self.use_distilling and self.distilling_layers and i < len(self.encoder_layers) - 1:
                x = self.distilling_layers[i](x)  # (batch, seq_len//4, d_model)
        
        # 3. 全局池化（聚合序列信息）
        x = x.mean(dim=1)  # (batch, d_model)
        
        # 4. 分类
        logits = self.classifier(x)  # (batch, n_classes)
        
        return logits
    
    def predict_proba(self, x: torch.Tensor) -> torch.Tensor:
        """
        预测类别概率
        
        Args:
            x: (batch, seq_len, n_features) - 序列输入
        
        Returns:
            probs: (batch, n_classes)
        """
        logits = self.forward(x)
        probs = F.softmax(logits, dim=-1)
        return probs

