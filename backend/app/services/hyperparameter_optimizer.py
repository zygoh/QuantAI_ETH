"""
è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–å™¨ - ä½¿ç”¨Optuna
"""

import logging
import gc
from typing import Dict, Any, Optional
import numpy as np
import pandas as pd
import optuna
from optuna.samplers import TPESampler
import lightgbm as lgb
import xgboost as xgb
import catboost as cb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score
from sklearn.utils.class_weight import compute_sample_weight
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, TensorDataset
from app.services.informer2_model import Informer2ForClassification
from app.services.gmadl_loss import create_trade_loss
from app.core.config import settings

logger = logging.getLogger(__name__)


class HyperparameterOptimizer:
    """
    è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–å™¨
    
    ä½¿ç”¨Optunaçš„TPEï¼ˆTree-structured Parzen Estimatorï¼‰ç®—æ³•
    è‡ªåŠ¨æœç´¢æœ€ä½³è¶…å‚æ•°ç»„åˆ
    """
    
    def __init__(
        self,
        X: np.ndarray,
        y: pd.Series,
        timeframe: str,
        model_type: str = "lightgbm",
        use_gpu: bool = True
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            X: ç‰¹å¾æ•°æ®ï¼ˆå·²ç¼©æ”¾ï¼‰
            y: æ ‡ç­¾æ•°æ®
        timeframe: æ—¶é—´æ¡†æ¶ï¼ˆ3m/5m/15mï¼‰
            model_type: æ¨¡å‹ç±»å‹ï¼ˆlightgbm/xgboost/catboostï¼‰
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
        """
        self.X = X
        self.y = y
        self.timeframe = timeframe
        self.model_type = model_type
        self.use_gpu = use_gpu
        self.best_params: Optional[Dict[str, Any]] = None
        self.best_score: float = 0.0
        
        # HOLDæƒ©ç½šç³»æ•°ï¼ˆä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        self.hold_penalty = 0.65
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–è¶…å‚æ•°ä¼˜åŒ–å™¨: {timeframe} - {model_type}")
        if len(X.shape) == 3:
            logger.info(f"   æ ·æœ¬æ•°: {len(X)}, åºåˆ—é•¿åº¦: {X.shape[1]}, ç‰¹å¾æ•°: {X.shape[2]}")
        else:
            logger.info(f"   æ ·æœ¬æ•°: {len(X)}, ç‰¹å¾æ•°: {X.shape[1]}")
        logger.info(f"   GPUåŠ é€Ÿ: {'å¯ç”¨' if use_gpu else 'å…³é—­'}")
    
    def clear_gpu_memory(self):
        """
        ç»Ÿä¸€GPUå†…å­˜æ¸…ç†æ–¹æ³•
        
        åŠŸèƒ½ï¼š
        - æ¸…ç©ºPyTorchç¼“å­˜
        - åŒæ­¥GPUæ“ä½œ
        - å¼ºåˆ¶åƒåœ¾å›æ”¶
        - è®°å½•æ¸…ç†çŠ¶æ€
        """
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()
            
            # è®°å½•GPUå†…å­˜çŠ¶æ€
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            gpu_used = torch.cuda.memory_allocated(0)
            gpu_free = gpu_memory - gpu_used
            logger.debug(f"ğŸ§¹ GPUå†…å­˜å·²æ¸…ç† (ä½¿ç”¨: {gpu_used/1024**3:.1f}GB, å¯ç”¨: {gpu_free/1024**3:.1f}GB)")
        else:
            logger.debug("ğŸ§¹ CPUæ¨¡å¼ï¼Œæ— éœ€æ¸…ç†GPUå†…å­˜")
    
    def monitor_gpu_memory(self):
        """
        ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        
        Returns:
            Dict: GPUå†…å­˜çŠ¶æ€ä¿¡æ¯
        """
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            gpu_used = torch.cuda.memory_allocated(0)
            gpu_free = gpu_memory - gpu_used
            gpu_reserved = torch.cuda.memory_reserved(0)
            
            return {
                'total': gpu_memory,
                'used': gpu_used,
                'free': gpu_free,
                'reserved': gpu_reserved,
                'usage_percent': (gpu_used / gpu_memory) * 100
            }
        else:
            return {'error': 'GPUä¸å¯ç”¨'}
    
    def _get_lightgbm_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
        """
        LightGBMæœç´¢ç©ºé—´
        
        æ ¹æ®æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–é…ç½®æœç´¢èŒƒå›´
        """
        # åŸºç¡€å‚æ•°
        base_params = {}
        
        if self.timeframe == "15m":
            # 15m: æ ·æœ¬å¤šï¼Œå¯ä»¥å¤æ‚ä¸€äº›
            base_params = {
                'n_estimators': trial.suggest_int('n_estimators', 200, 500),
                'max_depth': trial.suggest_int('max_depth', 6, 12),
                'num_leaves': trial.suggest_int('num_leaves', 63, 127),
                'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.1, log=True),
                'min_child_samples': trial.suggest_int('min_child_samples', 20, 50),
                'subsample': trial.suggest_float('subsample', 0.6, 0.9),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.5),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.5),
                'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.1),
                'random_state': 42,
                'verbose': -1,
                'force_col_wise': True
            }
        else:
            # 3m/5m ç®€åŒ–æœç´¢ï¼ˆä¸15måŒºåˆ†ï¼‰
            base_params = {
                'n_estimators': trial.suggest_int('n_estimators', 120, 320),
                'max_depth': trial.suggest_int('max_depth', 4, 8),
                'num_leaves': trial.suggest_int('num_leaves', 31, 63),
                'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.12, log=True),
                'min_child_samples': trial.suggest_int('min_child_samples', 30, 70),
                'subsample': trial.suggest_float('subsample', 0.6, 0.85),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.85),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.3, 1.0),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.3, 1.0),
                'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.2),
                'random_state': 42,
                'verbose': -1,
                'force_col_wise': True
            }
        
        # ğŸ® GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gpu:
            base_params['device'] = 'gpu'
            base_params['gpu_platform_id'] = 0
            base_params['gpu_device_id'] = 0
        
        return base_params
    
    def _get_xgboost_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
        """XGBoostæœç´¢ç©ºé—´"""
        if self.timeframe == "15m":
            base_params = {
                'n_estimators': trial.suggest_int('n_estimators', 200, 500),
                'max_depth': trial.suggest_int('max_depth', 4, 8),
                'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.1, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 0.9),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.5),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.5),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'random_state': 42,
                'verbosity': 0
            }
        else:
            # 3m/5m ç®€åŒ–
            base_params = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 250),
                'max_depth': trial.suggest_int('max_depth', 2, 5),
                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 0.8),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.5, 1.2),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 1.2),
                'min_child_weight': trial.suggest_int('min_child_weight', 3, 10),
                'random_state': 42,
                'verbosity': 0
            }
        
        # ğŸ® GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gpu:
            base_params['tree_method'] = 'hist'  # æ–°ç‰ˆæœ¬ä½¿ç”¨ hist
            base_params['device'] = 'cuda'  # ä½¿ç”¨ device å‚æ•°æŒ‡å®š GPU
        else:
            base_params['tree_method'] = 'hist'
        
        return base_params
    
    def _get_catboost_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
        """CatBoostæœç´¢ç©ºé—´"""
        if self.timeframe == "15m":
            base_params = {
                'iterations': trial.suggest_int('iterations', 200, 500),
                'depth': trial.suggest_int('depth', 4, 8),
                'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.1, log=True),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 5.0),
                'border_count': trial.suggest_int('border_count', 32, 128),
                'random_state': 42,
                'verbose': False
            }
        else:
            # 3m/5m ç®€åŒ–
            base_params = {
                'iterations': trial.suggest_int('iterations', 100, 250),
                'depth': trial.suggest_int('depth', 2, 5),
                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2, log=True),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 3.0, 10.0),
                'border_count': trial.suggest_int('border_count', 32, 64),
                'random_state': 42,
                'verbose': False
            }
        
        # ğŸ® GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gpu:
            base_params['task_type'] = 'GPU'
            base_params['devices'] = '0'
        
        return base_params
    
    def _get_informer2_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
        """Informer-2æœç´¢ç©ºé—´ï¼ˆåŸºäºTransformerç†è®ºçš„æœ€ä½³å®è·µ + ç²¾ç¡®å¤æ‚åº¦åŒ¹é…ï¼‰"""
        
        # ğŸ”‘ åºåˆ—é•¿åº¦é…ç½®ï¼ˆä¸ensemble_ml_service.pyä¿æŒä¸€è‡´ï¼‰
        # ğŸ¯ ä¼˜åŒ–ï¼šå‡å°‘åºåˆ—é•¿åº¦ä»¥é™ä½å†…å­˜å ç”¨ï¼ˆå‡å°‘80-90%ï¼‰
        seq_len_config = {
            '3m': 96,   # 96 Ã— 3åˆ†é’Ÿ = 4.8å°æ—¶ï¼ˆè¶³å¤ŸçŸ­æœŸæ¨¡å¼è¯†åˆ«ï¼‰
            '5m': 96,   # 96 Ã— 5åˆ†é’Ÿ = 8å°æ—¶ï¼ˆä¸»æ—¶é—´æ¡†æ¶ï¼‰
            '15m': 64   # 64 Ã— 15åˆ†é’Ÿ = 16å°æ—¶ï¼ˆè¶‹åŠ¿ç¡®è®¤ï¼‰
        }
        
        seq_len = seq_len_config.get(self.timeframe, 96)
        
        # ğŸ¯ åŸºäºTransformerç†è®ºçš„æœ€ä½³å®è·µ
        # 1. d_modelä¸åºåˆ—é•¿åº¦çš„å…³ç³»ï¼šd_model â‰ˆ sqrt(seq_len) * 8-16
        # 2. n_headsä¸d_modelçš„å…³ç³»ï¼šn_heads = d_model / 64 (æ ‡å‡†æ¯”ä¾‹)
        # 3. n_layersä¸åºåˆ—é•¿åº¦çš„å…³ç³»ï¼šn_layers â‰ˆ log2(seq_len) + 1
        
        if self.timeframe == "15m":
            # 15m: çŸ­åºåˆ—(64)ï¼Œç²¾ç¡®å¤æ‚åº¦åŒ¹é…
            # d_model = sqrt(64) * 12 â‰ˆ 96 â†’ 128
            # n_heads = 128 / 64 = 2 â†’ 4,8 (æ¸è¿›å¼æœç´¢)
            # n_layers = log2(64) + 1 â‰ˆ 7 â†’ 2,3 (æ¸è¿›å¼æœç´¢)
            base_params = {
                'd_model': trial.suggest_categorical('d_model', [128, 256]),      # ç²¾ç¡®åŒ¹é…
                'n_heads': trial.suggest_categorical('n_heads', [4, 8]),          # ç²¾ç¡®åŒ¹é…
                'n_layers': trial.suggest_int('n_layers', 2, 3),  # ç²¾ç¡®åŒ¹é…
                'epochs': trial.suggest_int('epochs', 20, 40),
                'batch_size': trial.suggest_categorical('batch_size', [128, 256, 512]),
                'lr': trial.suggest_float('lr', 0.0005, 0.005, log=True),
                'dropout': trial.suggest_float('dropout', 0.05, 0.2),
                'alpha': trial.suggest_float('alpha', 0.5, 2.0),  # GMADLå‚æ•°
                'beta': trial.suggest_float('beta', 0.3, 0.7)    # GMADLå‚æ•°
            }
        else:
            # 3m/5mï¼šä¸­åºåˆ—(96)
            base_params = {
                'd_model': trial.suggest_categorical('d_model', [64, 128]),
                'n_heads': trial.suggest_categorical('n_heads', [2, 4, 8]),
                'n_layers': trial.suggest_int('n_layers', 1, 2),
                'epochs': trial.suggest_int('epochs', 10, 30),
                'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),
                'lr': trial.suggest_float('lr', 0.0008, 0.006, log=True),
                'dropout': trial.suggest_float('dropout', 0.1, 0.3),
                'alpha': trial.suggest_float('alpha', 0.8, 1.8),
                'beta': trial.suggest_float('beta', 0.4, 0.6)
            }
        
        # æ·»åŠ åºåˆ—é•¿åº¦ä¿¡æ¯åˆ°å‚æ•°ä¸­ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        base_params['seq_len'] = seq_len
        
        return base_params
    
    def objective(self, trial: optuna.Trial) -> float:
        """
        ä¼˜åŒ–ç›®æ ‡å‡½æ•°
        
        ä½¿ç”¨5æŠ˜æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¯„ä¼°è¶…å‚æ•°ç»„åˆ
        
        Args:
            trial: Optunaè¯•éªŒå¯¹è±¡
        
        Returns:
            è´Ÿçš„CVå¹³å‡å‡†ç¡®ç‡ï¼ˆOptunaé»˜è®¤æœ€å°åŒ–ï¼‰
        """
        # è·å–æœç´¢ç©ºé—´
        if self.model_type == "lightgbm":
            params = self._get_lightgbm_search_space(trial)
        elif self.model_type == "xgboost":
            params = self._get_xgboost_search_space(trial)
        elif self.model_type == "catboost":
            params = self._get_catboost_search_space(trial)
        elif self.model_type == "informer2":
            params = self._get_informer2_search_space(trial)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
        
        # æ—¶é—´åºåˆ—5æŠ˜äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=5)
        cv_scores = []
        fold_fail_count = 0
        
        # ğŸ”‘ ä¿®å¤ï¼šå¯¹äº3Dåºåˆ—è¾“å…¥ï¼Œéœ€è¦åŸºäºæ ·æœ¬æ•°é‡è€Œä¸æ˜¯ç‰¹å¾è¿›è¡Œåˆ†å‰²
        n_samples = len(self.X) if isinstance(self.X, np.ndarray) else self.X.shape[0]
        
        for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(np.arange(n_samples))):
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿ç´¢å¼•è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆå…¼å®¹å†…å­˜æ˜ å°„ï¼‰
            train_idx = np.asarray(train_idx)
            val_idx = np.asarray(val_idx)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¯¹äºå†…å­˜æ˜ å°„æ•°ç»„ï¼Œéœ€è¦å¤åˆ¶æ•°æ®åˆ°å†…å­˜
            if hasattr(self.X, 'filename') and self.X.filename:
                # å†…å­˜æ˜ å°„æ•°ç»„ï¼šå¤åˆ¶åˆ°å†…å­˜
                X_train = np.array(self.X[train_idx], dtype=np.float32)
                X_val = np.array(self.X[val_idx], dtype=np.float32)
            else:
                # æ™®é€šæ•°ç»„ï¼šç›´æ¥åˆ‡ç‰‡
                X_train, X_val = self.X[train_idx], self.X[val_idx]
            
            # ğŸ”‘ ä¿®å¤ï¼šç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆå…¼å®¹pandas Serieså’Œnumpyæ•°ç»„ï¼‰
            if isinstance(self.y, pd.Series):
                y_train = self.y.iloc[train_idx].values
                y_val = self.y.iloc[val_idx].values
            elif isinstance(self.y, np.ndarray):
                if hasattr(self.y, 'filename') and self.y.filename:
                    # å†…å­˜æ˜ å°„æ•°ç»„ï¼šå¤åˆ¶åˆ°å†…å­˜
                    y_train = np.array(self.y[train_idx], dtype=np.int64)
                    y_val = np.array(self.y[val_idx], dtype=np.int64)
                else:
                    y_train, y_val = self.y[train_idx], self.y[val_idx]
            else:
                # å…¶ä»–ç±»å‹ï¼šå°è¯•è½¬æ¢
                y_train = np.asarray(self.y)[train_idx]
                y_val = np.asarray(self.y)[val_idx]
            
            # è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆæœ‰æ•ˆæ ·æœ¬æ•° Ã— æ—¶é—´è¡°å‡ Ã— HOLDæƒ©ç½šï¼‰
            try:
                from app.services.ml_service import MLService
                temp_svc = MLService()
                class_weights = temp_svc._compute_effective_sample_weights(y_train, self.timeframe)
            except Exception:
                class_weights = compute_sample_weight('balanced', y_train)
            # âœ… æ·»åŠ æ—¶é—´è¡°å‡æƒé‡ï¼ˆä¸åŸºç¡€æ¨¡å‹è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            # HOLDæƒ©ç½šè‡ªé€‚åº”
            hold_ratio_tmp = float((y_train == 1).sum()) / max(len(y_train), 1)
            if self.timeframe == '3m':
                hold_weight_tmp = float(max(0.35, min(0.70, 0.80 - 0.6 * hold_ratio_tmp)))
            else:
                hold_weight_tmp = float(max(0.50, min(0.75, 0.85 - 0.5 * hold_ratio_tmp)))
            hold_penalty_weights = np.where(y_train == 1, hold_weight_tmp, 1.0)
            sample_weights = class_weights * time_decay * hold_penalty_weights
            
            # è®­ç»ƒæ¨¡å‹
            try:
                # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒå‰æ¸…ç†
                self.clear_gpu_memory()
                
                if self.model_type == "lightgbm":
                    try:
                        model = lgb.LGBMClassifier(**params)
                        model.fit(X_train, y_train, sample_weight=sample_weights)
                        
                        # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒåæ¸…ç†
                        self.clear_gpu_memory()
                            
                    except Exception as e:
                        logger.error(f"âŒ LightGBMè®­ç»ƒå¤±è´¥: {e}")
                        # é™çº§åˆ°CPU
                        params['device'] = 'cpu'
                        model = lgb.LGBMClassifier(**params)
                        model.fit(X_train, y_train, sample_weight=sample_weights)
                        self.clear_gpu_memory()
                
                elif self.model_type == "xgboost":
                    try:
                        model = xgb.XGBClassifier(**params)
                        model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
                        
                        # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒåæ¸…ç†
                        self.clear_gpu_memory()
                            
                    except Exception as e:
                        logger.error(f"âŒ XGBoostè®­ç»ƒå¤±è´¥: {e}")
                        # é™çº§åˆ°CPU
                        params['tree_method'] = 'hist'
                        params['device'] = 'cpu'
                        model = xgb.XGBClassifier(**params)
                        model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
                        self.clear_gpu_memory()
                
                elif self.model_type == "catboost":
                    # æ£€æŸ¥GPUå†…å­˜å¯ç”¨æ€§
                    if torch.cuda.is_available():
                        gpu_status = self.monitor_gpu_memory()
                        if gpu_status.get('free', 0) > 500 * 1024**2:  # 500MB
                            params['task_type'] = 'GPU'
                            params['devices'] = '0'
                            logger.debug(f"ğŸš€ CatBoostä½¿ç”¨GPUè®­ç»ƒ (å¯ç”¨å†…å­˜: {gpu_status['free']/1024**3:.1f}GB)")
                        else:
                            params['task_type'] = 'CPU'
                            logger.warning(f"âš ï¸ GPUå†…å­˜ä¸è¶³({gpu_status['free']/1024**3:.1f}GB)ï¼Œåˆ‡æ¢åˆ°CPU")
                    else:
                        params['task_type'] = 'CPU'
                        logger.debug("ğŸ”„ GPUä¸å¯ç”¨ï¼ŒCatBoostä½¿ç”¨CPUè®­ç»ƒ")
                    
                    try:
                        model = cb.CatBoostClassifier(**params)
                        model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
                        
                        # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒåæ¸…ç†
                        self.clear_gpu_memory()
                            
                    except Exception as e:
                        logger.error(f"âŒ CatBoost GPUè®­ç»ƒå¤±è´¥: {e}")
                        # é™çº§åˆ°CPU
                        params['task_type'] = 'CPU'
                        model = cb.CatBoostClassifier(**params)
                        model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)
                        self.clear_gpu_memory()
                
                elif self.model_type == "informer2":
                    # Informer-2éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆæ·±åº¦å­¦ä¹ æ¨¡å‹ + åºåˆ—è¾“å…¥ï¼‰
                    # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒå‰æ¸…ç†
                    self.clear_gpu_memory()
                    
                    # ğŸ”‘ æ£€æŸ¥è¾“å…¥ç»´åº¦ï¼ˆ2Dæˆ–3Dï¼‰
                    if len(X_train.shape) == 2:
                        # 2Dè¾“å…¥ï¼šéœ€è¦æ„é€ åºåˆ—ï¼ˆè¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä½œä¸ºé™çº§å¤„ç†ï¼‰
                        logger.warning(f"âš ï¸ Informer-2æ”¶åˆ°2Dè¾“å…¥ï¼Œå°†è·³è¿‡æ­¤fold")
                        cv_scores.append(0.0)
                        fold_fail_count += 1
                        continue
                    
                    # 3Dåºåˆ—è¾“å…¥ï¼š(n_samples, seq_len, n_features)
                    n_features = X_train.shape[2]
                    
                    # è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
                    device = torch.device('cuda:0' if self.use_gpu and torch.cuda.is_available() else 'cpu')
                    
                    # ğŸ”¥ å†…å­˜ä¼˜åŒ–ï¼šç¡®ä¿è¾“å…¥æ•°æ®ä¸ºfloat32ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰
                    if X_train.dtype != np.float32:
                        logger.debug(f"   è½¬æ¢X_trainä¸ºfloat32ï¼ˆåŸç±»å‹: {X_train.dtype}ï¼‰")
                        X_train = X_train.astype(np.float32)
                    if X_val.dtype != np.float32:
                        logger.debug(f"   è½¬æ¢X_valä¸ºfloat32ï¼ˆåŸç±»å‹: {X_val.dtype}ï¼‰")
                        X_val = X_val.astype(np.float32)
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç»Ÿä¸€è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆç¡®ä¿æ˜¯è¿ç»­å†…å­˜ï¼‰
                    if not isinstance(y_train, np.ndarray):
                        y_train_np = np.asarray(y_train, dtype=np.int64)
                    else:
                        y_train_np = y_train.astype(np.int64) if y_train.dtype != np.int64 else y_train
                    
                    if not isinstance(y_val, np.ndarray):
                        y_val_np = np.asarray(y_val, dtype=np.int64)
                    else:
                        y_val_np = y_val.astype(np.int64) if y_val.dtype != np.int64 else y_val
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆ›å»ºå¼ é‡ç”¨äºå†…å­˜ç›‘æ§ï¼ˆä½†ä¸ç”¨äºè®­ç»ƒï¼‰
                    # DataLoaderä¼šè‡ªåŠ¨å¤„ç†æ•°æ®è½¬æ¢
                    train_memory_mb = (X_train.nbytes + y_train_np.nbytes) / (1024 ** 2)
                    logger.debug(f"   è®­ç»ƒé›†å†…å­˜: {train_memory_mb:.1f} MB")
                    
                    # ğŸš€ æ¢¯åº¦ç´¯ç§¯é…ç½®ï¼ˆè§£å†³GPU OOMé—®é¢˜ï¼‰
                    effective_batch_size = params['batch_size']
                    actual_batch_size = max(8, params['batch_size'] // 8)
                    accumulation_steps = effective_batch_size // actual_batch_size
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®æ˜¯è¿ç»­çš„numpyæ•°ç»„ï¼ˆé¿å…å†…å­˜æ˜ å°„é—®é¢˜ï¼‰
                    if not X_train.flags['C_CONTIGUOUS']:
                        logger.debug(f"   è½¬æ¢X_trainä¸ºè¿ç»­æ•°ç»„")
                        X_train = np.ascontiguousarray(X_train)
                    if not y_train_np.flags['C_CONTIGUOUS']:
                        logger.debug(f"   è½¬æ¢y_trainä¸ºè¿ç»­æ•°ç»„")
                        y_train_np = np.ascontiguousarray(y_train_np)
                    
                    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨æ›´å°çš„ç‰©ç†æ‰¹æ¬¡ï¼‰
                    class NumpyTimeSeriesDataset(Dataset):
                        def __init__(self, X_np, y_np):
                            # ç¡®ä¿æ•°æ®æ˜¯è¿ç»­çš„numpyæ•°ç»„
                            self.X_np = np.ascontiguousarray(X_np) if not X_np.flags['C_CONTIGUOUS'] else X_np
                            self.y_np = np.ascontiguousarray(y_np) if not y_np.flags['C_CONTIGUOUS'] else y_np
                        def __len__(self):
                            return len(self.y_np)
                        def __getitem__(self, idx):
                            return (
                                torch.from_numpy(self.X_np[idx].copy()).to(dtype=torch.float32),
                                torch.tensor(self.y_np[idx], dtype=torch.long)
                            )

                    train_dataset = NumpyTimeSeriesDataset(X_train, y_train_np)
                    train_loader = DataLoader(
                        train_dataset,
                        batch_size=actual_batch_size,
                        shuffle=True,
                        num_workers=0,
                        pin_memory=True if device.type == 'cuda' else False
                    )
                    
                    # åˆ›å»ºæ¨¡å‹ï¼ˆæ”¯æŒåºåˆ—è¾“å…¥ + æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰
                    model = Informer2ForClassification(
                        n_features=n_features,  # ç‰¹å¾æ•°é‡ï¼ˆä»åºåˆ—çš„æœ€åä¸€ç»´è·å–ï¼‰
                        n_classes=3,  # ç±»åˆ«æ•°
                        d_model=params['d_model'],
                        n_heads=params['n_heads'],
                        n_layers=params['n_layers'],
                        dropout=params['dropout'],
                        use_distilling=True,  # å¯ç”¨è’¸é¦å±‚ï¼ˆå®Œæ•´Informeræ¶æ„ï¼‰
                        use_gradient_checkpointing=True  # ğŸ”¥ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœ50-70%å†…å­˜ï¼‰
                    ).to(device)
                    
                    # å®šä¹‰æŸå¤±å‡½æ•°ï¼ˆä¸è®­ç»ƒæµç¨‹ä¿æŒä¸€è‡´ï¼‰
                    hold_ratio_opt = float((y_train_np == 1).sum()) / max(len(y_train_np), 1)
                    if self.timeframe == '3m':
                        hold_penalty_nn = float(max(0.35, min(0.70, 0.80 - 0.6 * hold_ratio_opt)))
                    else:
                        hold_penalty_nn = float(max(0.50, min(0.75, 0.85 - 0.5 * hold_ratio_opt)))

                    criterion = create_trade_loss(
                        use_gmadl=settings.USE_GMADL_LOSS,
                        hold_penalty=hold_penalty_nn,
                        alpha=params.get('alpha', settings.GMADL_ALPHA),
                        beta=params.get('beta', settings.GMADL_BETA)
                    )

                    if settings.USE_GMADL_LOSS:
                        logger.debug(
                            f"   æŸå¤±å‡½æ•°: GMADL + HOLDæƒ©ç½š (alpha={params.get('alpha', settings.GMADL_ALPHA):.2f}, beta={params.get('beta', settings.GMADL_BETA):.2f})"
                        )
                    else:
                        logger.debug("   æŸå¤±å‡½æ•°: äº¤å‰ç†µ + HOLDæƒ©ç½š (ç¨³å®šæ¨¡å¼)")
                    
                    # ğŸ”¥ å°è¯•ä½¿ç”¨8-bit Adamä¼˜åŒ–å™¨ï¼ˆèŠ‚çœ75%ä¼˜åŒ–å™¨å†…å­˜ï¼‰
                    optimizer_created = False
                    if self.use_gpu and device.type == 'cuda':
                        try:
                            import bitsandbytes as bnb
                            optimizer = bnb.optim.Adam8bit(
                                model.parameters(),
                                lr=params['lr'],
                                betas=(0.9, 0.999)
                            )
                            optimizer_created = True
                        except (ImportError, Exception):
                            pass
                    
                    if not optimizer_created:
                        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])
                    
                    # ğŸš€ æ··åˆç²¾åº¦è®­ç»ƒï¼ˆä½¿ç”¨æ–°çš„torch.amp API + æ¿€è¿›ä¼˜åŒ–ï¼‰
                    use_amp = device.type == 'cuda' and torch.cuda.is_available()
                    if settings.USE_GMADL_LOSS and use_amp:
                        logger.debug("   âš ï¸ GMADLå¼€å¯ â†’ Optunaè¯•éªŒç¦ç”¨AMPæ”¹ç”¨FP32è®­ç»ƒ")
                        use_amp = False
                    if use_amp:
                        # ğŸ”¥ æ¿€è¿›æ··åˆç²¾åº¦ä¼˜åŒ–
                        scaler = torch.amp.GradScaler('cuda', init_scale=2.**16)
                        torch.backends.cuda.matmul.allow_tf32 = True
                        torch.backends.cudnn.allow_tf32 = True
                    else:
                        scaler = None
                    
                    # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦ï¼‰
                    model.train()
                    for epoch in range(params['epochs']):
                        optimizer.zero_grad()
                        
                        for i, (batch_X, batch_y) in enumerate(train_loader):
                            # ğŸ¯ æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                            # å°†æ‰¹æ¬¡ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
                            batch_X = batch_X.to(device, non_blocking=True)
                            batch_y = batch_y.to(device, non_blocking=True)
                            if use_amp:
                                with torch.amp.autocast('cuda'):
                                    outputs = model(batch_X)
                                    # ç»Ÿä¸€dtypeä¸lossè¾“å…¥ï¼šlogitsç”¨float32ï¼Œtargetsç”¨long
                                    loss = criterion(outputs.float(), batch_y.long()) / accumulation_steps
                            else:
                                outputs = model(batch_X)
                                loss = criterion(outputs.float(), batch_y.long()) / accumulation_steps
                            
                            # ğŸ¯ æ··åˆç²¾åº¦åå‘ä¼ æ’­
                            if use_amp:
                                scaler.scale(loss).backward()
                            else:
                                loss.backward()
                            
                            # ğŸ¯ æ¢¯åº¦ç´¯ç§¯
                            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):
                                if use_amp:
                                    scaler.unscale_(optimizer)
                                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                                    scaler.step(optimizer)
                                    scaler.update()
                                else:
                                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                                    optimizer.step()
                                
                                optimizer.zero_grad()
                                
                                # å®šæœŸæ¸…ç†GPUç¼“å­˜
                                if (i + 1) % (accumulation_steps * 10) == 0 and device.type == 'cuda':
                                    torch.cuda.empty_cache()
                        
                        # æ¯ä¸ªepochç»“æŸåæ¸…ç†GPUç¼“å­˜
                        if device.type == 'cuda':
                            torch.cuda.empty_cache()
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿éªŒè¯æ•°æ®ä¹Ÿæ˜¯è¿ç»­æ•°ç»„
                    if not X_val.flags['C_CONTIGUOUS']:
                        logger.debug(f"   è½¬æ¢X_valä¸ºè¿ç»­æ•°ç»„")
                        X_val = np.ascontiguousarray(X_val)
                    if not y_val_np.flags['C_CONTIGUOUS']:
                        logger.debug(f"   è½¬æ¢y_valä¸ºè¿ç»­æ•°ç»„")
                        y_val_np = np.ascontiguousarray(y_val_np)
                    
                    # è¯„ä¼°æ¨¡å¼
                    model.eval()
                    with torch.no_grad():
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨copy()é¿å…å†…å­˜æ˜ å°„é—®é¢˜
                        X_val_tensor = torch.from_numpy(X_val.copy()).to(device, dtype=torch.float32)
                        val_outputs = model(X_val_tensor)
                        y_pred = torch.argmax(val_outputs, dim=1).cpu().numpy()
                    
                    # ğŸ® ç»Ÿä¸€GPUå†…å­˜ç®¡ç†ï¼šè®­ç»ƒåæ¸…ç†
                    self.clear_gpu_memory()
                    
                    # åˆ é™¤æ¨¡å‹å’Œå¼ é‡é‡Šæ”¾å†…å­˜
                    del model, X_val_tensor
                    self.clear_gpu_memory()
                
                # é¢„æµ‹å¹¶è¯„ä¼°
                if self.model_type != "informer2":
                    y_pred = model.predict(X_val)
                acc = accuracy_score(y_val, y_pred)
                cv_scores.append(acc)
                
            except Exception as e:
                fold_fail_count += 1
                logger.debug(f"Trial {trial.number} Fold {fold_idx+1} å¤±è´¥: {e}")
                # å¤±è´¥çš„trialè¿”å›å¾ˆå·®çš„åˆ†æ•°
                cv_scores.append(0.0)
        
        # è®¡ç®—å¹³å‡CVå‡†ç¡®ç‡
        mean_cv_acc = np.mean(cv_scores)
        if fold_fail_count > 0:
            logger.info(f"   Trial {trial.number} æ±‡æ€»ï¼šå¤±è´¥fold={fold_fail_count}/5ï¼ŒCV={mean_cv_acc:.4f}")
        
        # æ¯10æ¬¡è¯•éªŒæŠ¥å‘Šä¸€æ¬¡è¿›åº¦
        if trial.number % 10 == 0:
            logger.info(f"   Trial {trial.number}: CVå‡†ç¡®ç‡={mean_cv_acc:.4f}")
        
        # è¿”å›è´Ÿå€¼ï¼ˆOptunaæœ€å°åŒ–ç›®æ ‡ï¼‰
        return -mean_cv_acc
    
    def optimize(
        self,
        n_trials: int = 100,
        timeout: int = 1800,
        show_progress: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            n_trials: è¯•éªŒæ¬¡æ•°ï¼ˆé»˜è®¤100æ¬¡ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤30åˆ†é’Ÿï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
        Returns:
            æœ€ä½³å‚æ•°å­—å…¸
        """
        logger.info(f"ğŸš€ å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–: {self.timeframe} - {self.model_type}")
        logger.info(f"   è¯•éªŒæ¬¡æ•°: {n_trials}, è¶…æ—¶: {timeout}ç§’ (~{timeout//60}åˆ†é’Ÿ)")
        
        # åˆ›å»ºstudyï¼ˆé™é»˜æ¨¡å¼ï¼Œé¿å…è¿‡å¤šæ—¥å¿—ï¼‰
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        
        study = optuna.create_study(
            direction='minimize',  # æœ€å°åŒ–è´Ÿå‡†ç¡®ç‡
            sampler=TPESampler(seed=42)
        )
        
        # æ‰§è¡Œä¼˜åŒ–
        try:
            study.optimize(
                self.objective,
                n_trials=n_trials,
                timeout=timeout,
                show_progress_bar=show_progress,
                n_jobs=1  # å•çº¿ç¨‹ï¼ˆé¿å…å¹¶å‘é—®é¢˜ï¼‰
            )
        except KeyboardInterrupt:
            logger.warning("âš ï¸ ä¼˜åŒ–è¢«ç”¨æˆ·ä¸­æ–­")
        
        # ä¿å­˜æœ€ä½³å‚æ•°
        self.best_params = study.best_params
        self.best_score = -study.best_value  # è½¬å›æ­£å‡†ç¡®ç‡
        
        logger.info(f"âœ… {self.timeframe} è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        logger.info(f"   æœ€ä½³CVå‡†ç¡®ç‡: {self.best_score:.4f} ({self.best_score*100:.2f}%)")
        logger.info(f"   æœ€ä½³å‚æ•°: {self.best_params}")
        logger.info(f"   æ€»è¯•éªŒæ¬¡æ•°: {len(study.trials)}")
        
        # æ˜¾ç¤ºå‚æ•°é‡è¦æ€§ï¼ˆTop 5ï¼‰
        try:
            importances = optuna.importance.get_param_importances(study)
            logger.info(f"   å‚æ•°é‡è¦æ€§ï¼ˆTop 5ï¼‰:")
            for i, (param, importance) in enumerate(list(importances.items())[:5]):
                logger.info(f"      {i+1}. {param}: {importance:.4f}")
        except:
            pass
        
        return self.best_params
    
    def get_optimized_params(self) -> Dict[str, Any]:
        """
        è·å–ä¼˜åŒ–åçš„å‚æ•°
        
        Returns:
            æœ€ä½³å‚æ•°å­—å…¸ï¼Œå¦‚æœæœªä¼˜åŒ–åˆ™è¿”å›None
        """
        return self.best_params

