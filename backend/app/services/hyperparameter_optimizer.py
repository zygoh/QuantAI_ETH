"""
è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–å™¨ - ä½¿ç”¨Optuna
"""

import logging
from typing import Dict, Any, Optional
import numpy as np
import pandas as pd
import optuna
from optuna.samplers import TPESampler
import lightgbm as lgb
import xgboost as xgb
import catboost as cb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score
from sklearn.utils.class_weight import compute_sample_weight

logger = logging.getLogger(__name__)


class HyperparameterOptimizer:
    """
    è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–å™¨
    
    ä½¿ç”¨Optunaçš„TPEï¼ˆTree-structured Parzen Estimatorï¼‰ç®—æ³•
    è‡ªåŠ¨æœç´¢æœ€ä½³è¶…å‚æ•°ç»„åˆ
    """
    
    def __init__(
        self,
        X: np.ndarray,
        y: pd.Series,
        timeframe: str,
        model_type: str = "lightgbm",
        use_gpu: bool = True
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            X: ç‰¹å¾æ•°æ®ï¼ˆå·²ç¼©æ”¾ï¼‰
            y: æ ‡ç­¾æ•°æ®
            timeframe: æ—¶é—´æ¡†æ¶ï¼ˆ15m/2h/4hï¼‰
            model_type: æ¨¡å‹ç±»å‹ï¼ˆlightgbm/xgboost/catboostï¼‰
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
        """
        self.X = X
        self.y = y
        self.timeframe = timeframe
        self.model_type = model_type
        self.use_gpu = use_gpu
        self.best_params: Optional[Dict[str, Any]] = None
        self.best_score: float = 0.0
        
        # HOLDæƒ©ç½šç³»æ•°ï¼ˆä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        self.hold_penalty = 0.65
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–è¶…å‚æ•°ä¼˜åŒ–å™¨: {timeframe} - {model_type}")
        if len(X.shape) == 3:
            logger.info(f"   æ ·æœ¬æ•°: {len(X)}, åºåˆ—é•¿åº¦: {X.shape[1]}, ç‰¹å¾æ•°: {X.shape[2]}")
        else:
            logger.info(f"   æ ·æœ¬æ•°: {len(X)}, ç‰¹å¾æ•°: {X.shape[1]}")
        logger.info(f"   GPUåŠ é€Ÿ: {'å¯ç”¨' if use_gpu else 'å…³é—­'}")
    
    def _get_lightgbm_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
        """
        LightGBMæœç´¢ç©ºé—´
        
        æ ¹æ®æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–é…ç½®æœç´¢èŒƒå›´
        """
        # åŸºç¡€å‚æ•°
        base_params = {}
        
        if self.timeframe == "15m":
            # 15m: æ ·æœ¬å¤šï¼Œå¯ä»¥å¤æ‚ä¸€äº›
            base_params = {
                'n_estimators': trial.suggest_int('n_estimators', 200, 500),
                'max_depth': trial.suggest_int('max_depth', 6, 12),
                'num_leaves': trial.suggest_int('num_leaves', 63, 127),
                'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.1, log=True),
                'min_child_samples': trial.suggest_int('min_child_samples', 20, 50),
                'subsample': trial.suggest_float('subsample', 0.6, 0.9),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.5),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.5),
                'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.1),
                'random_state': 42,
                'verbose': -1,
                'force_col_wise': True
            }
        
        elif self.timeframe == "2h":
            # 2h: æ ·æœ¬ä¸­ç­‰ï¼Œç®€åŒ–æ¨¡å‹
            base_params = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 300),
                'max_depth': trial.suggest_int('max_depth', 3, 6),
                'num_leaves': trial.suggest_int('num_leaves', 11, 31),
                'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.15, log=True),
                'min_child_samples': trial.suggest_int('min_child_samples', 40, 80),
                'subsample': trial.suggest_float('subsample', 0.6, 0.85),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.85),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.5, 1.2),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 1.2),
                'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.2),
                'random_state': 42,
                'verbose': -1,
                'force_col_wise': True
            }
        
        else:  # 4h
            # 4h: æ ·æœ¬å°‘ï¼Œæç®€æ¨¡å‹
            base_params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 200),
                'max_depth': trial.suggest_int('max_depth', 2, 5),
                'num_leaves': trial.suggest_int('num_leaves', 7, 21),
                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2, log=True),
                'min_child_samples': trial.suggest_int('min_child_samples', 50, 100),
                'subsample': trial.suggest_float('subsample', 0.6, 0.8),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.8, 1.5),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.8, 1.5),
                'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.3),
                'random_state': 42,
                'verbose': -1,
                'force_col_wise': True
            }
        
        # ğŸ® GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gpu:
            base_params['device'] = 'gpu'
            base_params['gpu_platform_id'] = 0
            base_params['gpu_device_id'] = 0
        
        return base_params
    
    def _get_xgboost_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
        """XGBoostæœç´¢ç©ºé—´"""
        if self.timeframe == "15m":
            base_params = {
                'n_estimators': trial.suggest_int('n_estimators', 200, 500),
                'max_depth': trial.suggest_int('max_depth', 4, 8),
                'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.1, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 0.9),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.5),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.5),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'random_state': 42,
                'verbosity': 0
            }
        else:
            # 2h/4hç®€åŒ–
            base_params = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 250),
                'max_depth': trial.suggest_int('max_depth', 2, 5),
                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 0.8),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.5, 1.2),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 1.2),
                'min_child_weight': trial.suggest_int('min_child_weight', 3, 10),
                'random_state': 42,
                'verbosity': 0
            }
        
        # ğŸ® GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gpu:
            base_params['tree_method'] = 'gpu_hist'
            base_params['gpu_id'] = 0
        else:
            base_params['tree_method'] = 'hist'
        
        return base_params
    
    def _get_catboost_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
        """CatBoostæœç´¢ç©ºé—´"""
        if self.timeframe == "15m":
            base_params = {
                'iterations': trial.suggest_int('iterations', 200, 500),
                'depth': trial.suggest_int('depth', 4, 8),
                'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.1, log=True),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 5.0),
                'border_count': trial.suggest_int('border_count', 32, 128),
                'random_state': 42,
                'verbose': False
            }
        else:
            # 2h/4hç®€åŒ–
            base_params = {
                'iterations': trial.suggest_int('iterations', 100, 250),
                'depth': trial.suggest_int('depth', 2, 5),
                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2, log=True),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 3.0, 10.0),
                'border_count': trial.suggest_int('border_count', 32, 64),
                'random_state': 42,
                'verbose': False
            }
        
        # ğŸ® GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gpu:
            base_params['task_type'] = 'GPU'
            base_params['devices'] = '0'
        
        return base_params
    
    def _get_informer2_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
        """Informer-2æœç´¢ç©ºé—´ï¼ˆåŸºäºTransformerç†è®ºçš„æœ€ä½³å®è·µ + ç²¾ç¡®å¤æ‚åº¦åŒ¹é…ï¼‰"""
        
        # ğŸ”‘ åºåˆ—é•¿åº¦é…ç½®ï¼ˆä¸ensemble_ml_service.pyä¿æŒä¸€è‡´ï¼‰
        seq_len_config = {
            '15m': 96,   # 96 Ã— 15åˆ†é’Ÿ = 24å°æ—¶
            '2h': 48,    # 48 Ã— 2å°æ—¶ = 4å¤©
            '4h': 24     # 24 Ã— 4å°æ—¶ = 4å¤©
        }
        
        seq_len = seq_len_config.get(self.timeframe, 96)
        
        # ğŸ¯ åŸºäºTransformerç†è®ºçš„æœ€ä½³å®è·µ
        # 1. d_modelä¸åºåˆ—é•¿åº¦çš„å…³ç³»ï¼šd_model â‰ˆ sqrt(seq_len) * 8-16
        # 2. n_headsä¸d_modelçš„å…³ç³»ï¼šn_heads = d_model / 64 (æ ‡å‡†æ¯”ä¾‹)
        # 3. n_layersä¸åºåˆ—é•¿åº¦çš„å…³ç³»ï¼šn_layers â‰ˆ log2(seq_len) + 1
        
        if self.timeframe == "15m":
            # 15m: é•¿åºåˆ—(96)ï¼Œç²¾ç¡®å¤æ‚åº¦åŒ¹é…
            # d_model = sqrt(96) * 12 â‰ˆ 118 â†’ 128
            # n_heads = 128 / 64 = 2 â†’ 4,8,16 (æ¸è¿›å¼æœç´¢)
            # n_layers = log2(96) + 1 â‰ˆ 7 â†’ 2,3,4 (æ¸è¿›å¼æœç´¢)
            base_params = {
                'd_model': trial.suggest_categorical('d_model', [128, 256]),      # ç²¾ç¡®åŒ¹é…
                'n_heads': trial.suggest_categorical('n_heads', [4, 8]),          # ç²¾ç¡®åŒ¹é…
                'n_layers': trial.suggest_int('n_layers', 2, 3),  # ç²¾ç¡®åŒ¹é…
                'epochs': trial.suggest_int('epochs', 20, 40),
                'batch_size': trial.suggest_categorical('batch_size', [128, 256, 512]),
                'lr': trial.suggest_float('lr', 0.0005, 0.005, log=True),
                'dropout': trial.suggest_float('dropout', 0.05, 0.2),
                'alpha': trial.suggest_float('alpha', 0.5, 2.0),  # GMADLå‚æ•°
                'beta': trial.suggest_float('beta', 0.3, 0.7)    # GMADLå‚æ•°
            }
        elif self.timeframe == "2h":
            # 2h: ä¸­ç­‰åºåˆ—(48)ï¼Œç²¾ç¡®å¤æ‚åº¦åŒ¹é…
            # d_model = sqrt(48) * 12 â‰ˆ 83 â†’ 64,128
            # n_heads = 64/128 / 64 = 1/2 â†’ 2,4,8 (æ¸è¿›å¼æœç´¢)
            # n_layers = log2(48) + 1 â‰ˆ 6 â†’ 1,2,3 (æ¸è¿›å¼æœç´¢)
            base_params = {
                'd_model': trial.suggest_categorical('d_model', [64, 128]),       # ç²¾ç¡®åŒ¹é…
                'n_heads': trial.suggest_categorical('n_heads', [2, 4, 8]),       # ç²¾ç¡®åŒ¹é…
                'n_layers': trial.suggest_int('n_layers', 1, 3),  # ç²¾ç¡®åŒ¹é…
                'epochs': trial.suggest_int('epochs', 15, 30),
                'batch_size': trial.suggest_categorical('batch_size', [128, 256]),
                'lr': trial.suggest_float('lr', 0.001, 0.005, log=True),
                'dropout': trial.suggest_float('dropout', 0.1, 0.3),
                'alpha': trial.suggest_float('alpha', 0.8, 1.5),
                'beta': trial.suggest_float('beta', 0.4, 0.6)
            }
        else:  # 4h
            # 4h: çŸ­åºåˆ—(24)ï¼Œç²¾ç¡®å¤æ‚åº¦åŒ¹é…
            # d_model = sqrt(24) * 12 â‰ˆ 59 â†’ 64
            # n_heads = 64 / 64 = 1 â†’ 2,4 (æ¸è¿›å¼æœç´¢)
            # n_layers = log2(24) + 1 â‰ˆ 5 â†’ 1,2 (æ¸è¿›å¼æœç´¢)
            base_params = {
                'd_model': trial.suggest_categorical('d_model', [64]),            # ç²¾ç¡®åŒ¹é…
                'n_heads': trial.suggest_categorical('n_heads', [2, 4]),          # ç²¾ç¡®åŒ¹é…
                'n_layers': trial.suggest_int('n_layers', 1, 2),  # ç²¾ç¡®åŒ¹é…
                'epochs': trial.suggest_int('epochs', 10, 25),
                'batch_size': trial.suggest_categorical('batch_size', [128, 256]),
                'lr': trial.suggest_float('lr', 0.002, 0.01, log=True),
                'dropout': trial.suggest_float('dropout', 0.15, 0.35),
                'alpha': trial.suggest_float('alpha', 1.0, 2.0),
                'beta': trial.suggest_float('beta', 0.5, 0.7)
            }
        
        # æ·»åŠ åºåˆ—é•¿åº¦ä¿¡æ¯åˆ°å‚æ•°ä¸­ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        base_params['seq_len'] = seq_len
        
        return base_params
    
    def objective(self, trial: optuna.Trial) -> float:
        """
        ä¼˜åŒ–ç›®æ ‡å‡½æ•°
        
        ä½¿ç”¨5æŠ˜æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¯„ä¼°è¶…å‚æ•°ç»„åˆ
        
        Args:
            trial: Optunaè¯•éªŒå¯¹è±¡
        
        Returns:
            è´Ÿçš„CVå¹³å‡å‡†ç¡®ç‡ï¼ˆOptunaé»˜è®¤æœ€å°åŒ–ï¼‰
        """
        # è·å–æœç´¢ç©ºé—´
        if self.model_type == "lightgbm":
            params = self._get_lightgbm_search_space(trial)
        elif self.model_type == "xgboost":
            params = self._get_xgboost_search_space(trial)
        elif self.model_type == "catboost":
            params = self._get_catboost_search_space(trial)
        elif self.model_type == "informer2":
            params = self._get_informer2_search_space(trial)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
        
        # æ—¶é—´åºåˆ—5æŠ˜äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=5)
        cv_scores = []
        
        # ğŸ”‘ ä¿®å¤ï¼šå¯¹äº3Dåºåˆ—è¾“å…¥ï¼Œéœ€è¦åŸºäºæ ·æœ¬æ•°é‡è€Œä¸æ˜¯ç‰¹å¾è¿›è¡Œåˆ†å‰²
        n_samples = len(self.X) if isinstance(self.X, np.ndarray) else self.X.shape[0]
        
        for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(np.arange(n_samples))):
            X_train, X_val = self.X[train_idx], self.X[val_idx]
            # ğŸ”‘ ä¿®å¤ï¼šå…¼å®¹ numpy æ•°ç»„å’Œ pandas Series
            if isinstance(self.y, np.ndarray):
                y_train, y_val = self.y[train_idx], self.y[val_idx]
            else:
                y_train, y_val = self.y.iloc[train_idx], self.y.iloc[val_idx]
            
            # è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆç±»åˆ«å¹³è¡¡ Ã— æ—¶é—´è¡°å‡ Ã— HOLDæƒ©ç½šï¼‰
            class_weights = compute_sample_weight('balanced', y_train)
            # âœ… æ·»åŠ æ—¶é—´è¡°å‡æƒé‡ï¼ˆä¸åŸºç¡€æ¨¡å‹è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
            time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
            hold_penalty_weights = np.where(y_train == 1, self.hold_penalty, 1.0)
            sample_weights = class_weights * time_decay * hold_penalty_weights
            
            # è®­ç»ƒæ¨¡å‹
            try:
                if self.model_type == "lightgbm":
                    model = lgb.LGBMClassifier(**params)
                    model.fit(X_train, y_train, sample_weight=sample_weights)
                
                elif self.model_type == "xgboost":
                    model = xgb.XGBClassifier(**params)
                    model.fit(X_train, y_train, sample_weight=sample_weights)
                
                elif self.model_type == "catboost":
                    model = cb.CatBoostClassifier(**params)
                    model.fit(X_train, y_train, sample_weight=sample_weights)
                
                elif self.model_type == "informer2":
                    # Informer-2éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆæ·±åº¦å­¦ä¹ æ¨¡å‹ + åºåˆ—è¾“å…¥ï¼‰
                    from app.services.informer2_model import Informer2ForClassification
                    from app.services.gmadl_loss import GMADLossWithHOLDPenalty
                    import torch
                    import torch.nn as nn
                    from torch.utils.data import DataLoader, TensorDataset
                    
                    # ğŸ”‘ æ£€æŸ¥è¾“å…¥ç»´åº¦ï¼ˆ2Dæˆ–3Dï¼‰
                    if len(X_train.shape) == 2:
                        # 2Dè¾“å…¥ï¼šéœ€è¦æ„é€ åºåˆ—ï¼ˆè¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä½œä¸ºé™çº§å¤„ç†ï¼‰
                        logger.warning(f"âš ï¸ Informer-2æ”¶åˆ°2Dè¾“å…¥ï¼Œå°†è·³è¿‡æ­¤fold")
                        cv_scores.append(0.0)
                        continue
                    
                    # 3Dåºåˆ—è¾“å…¥ï¼š(n_samples, seq_len, n_features)
                    n_features = X_train.shape[2]
                    
                    # è½¬æ¢ä¸ºPyTorchå¼ é‡
                    device = torch.device('cuda:0' if self.use_gpu and torch.cuda.is_available() else 'cpu')
                    X_train_tensor = torch.FloatTensor(X_train).to(device)
                    # âœ… å…¼å®¹pandas Serieså’Œnumpy ndarray
                    y_train_np = y_train.values if hasattr(y_train, 'values') else y_train
                    y_val_np = y_val.values if hasattr(y_val, 'values') else y_val
                    y_train_tensor = torch.LongTensor(y_train_np).to(device)
                    X_val_tensor = torch.FloatTensor(X_val).to(device)
                    y_val_tensor = torch.LongTensor(y_val_np).to(device)
                    
                    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
                    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, num_workers=0)
                    
                    # åˆ›å»ºæ¨¡å‹ï¼ˆæ”¯æŒåºåˆ—è¾“å…¥ï¼‰
                    model = Informer2ForClassification(
                        n_features=n_features,  # ç‰¹å¾æ•°é‡ï¼ˆä»åºåˆ—çš„æœ€åä¸€ç»´è·å–ï¼‰
                        n_classes=3,  # ç±»åˆ«æ•°
                        d_model=params['d_model'],
                        n_heads=params['n_heads'],
                        n_layers=params['n_layers'],
                        dropout=params['dropout'],
                        use_distilling=True  # å¯ç”¨è’¸é¦å±‚ï¼ˆå®Œæ•´Informeræ¶æ„ï¼‰
                    ).to(device)
                    
                    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
                    criterion = GMADLossWithHOLDPenalty(
                        hold_penalty=self.hold_penalty,
                        alpha=params['alpha'],
                        beta=params['beta']
                    )
                    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])
                    
                    # è®­ç»ƒæ¨¡å‹
                    model.train()
                    for epoch in range(params['epochs']):
                        for batch_X, batch_y in train_loader:
                            optimizer.zero_grad()
                            outputs = model(batch_X)
                            loss = criterion(outputs, batch_y)
                            loss.backward()
                            optimizer.step()
                    
                    # è¯„ä¼°æ¨¡å¼
                    model.eval()
                    with torch.no_grad():
                        val_outputs = model(X_val_tensor)
                        y_pred = torch.argmax(val_outputs, dim=1).cpu().numpy()
                
                # é¢„æµ‹å¹¶è¯„ä¼°
                if self.model_type != "informer2":
                    y_pred = model.predict(X_val)
                acc = accuracy_score(y_val, y_pred)
                cv_scores.append(acc)
                
            except Exception as e:
                logger.warning(f"Trial {trial.number} Fold {fold_idx+1} å¤±è´¥: {e}")
                # å¤±è´¥çš„trialè¿”å›å¾ˆå·®çš„åˆ†æ•°
                cv_scores.append(0.0)
        
        # è®¡ç®—å¹³å‡CVå‡†ç¡®ç‡
        mean_cv_acc = np.mean(cv_scores)
        
        # æ¯10æ¬¡è¯•éªŒæŠ¥å‘Šä¸€æ¬¡è¿›åº¦
        if trial.number % 10 == 0:
            logger.info(f"   Trial {trial.number}: CVå‡†ç¡®ç‡={mean_cv_acc:.4f}")
        
        # è¿”å›è´Ÿå€¼ï¼ˆOptunaæœ€å°åŒ–ç›®æ ‡ï¼‰
        return -mean_cv_acc
    
    def optimize(
        self,
        n_trials: int = 100,
        timeout: int = 1800,
        show_progress: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            n_trials: è¯•éªŒæ¬¡æ•°ï¼ˆé»˜è®¤100æ¬¡ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤30åˆ†é’Ÿï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
        Returns:
            æœ€ä½³å‚æ•°å­—å…¸
        """
        logger.info(f"ğŸš€ å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–: {self.timeframe} - {self.model_type}")
        logger.info(f"   è¯•éªŒæ¬¡æ•°: {n_trials}, è¶…æ—¶: {timeout}ç§’ (~{timeout//60}åˆ†é’Ÿ)")
        
        # åˆ›å»ºstudyï¼ˆé™é»˜æ¨¡å¼ï¼Œé¿å…è¿‡å¤šæ—¥å¿—ï¼‰
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        
        study = optuna.create_study(
            direction='minimize',  # æœ€å°åŒ–è´Ÿå‡†ç¡®ç‡
            sampler=TPESampler(seed=42)
        )
        
        # æ‰§è¡Œä¼˜åŒ–
        try:
            study.optimize(
                self.objective,
                n_trials=n_trials,
                timeout=timeout,
                show_progress_bar=show_progress,
                n_jobs=1  # å•çº¿ç¨‹ï¼ˆé¿å…å¹¶å‘é—®é¢˜ï¼‰
            )
        except KeyboardInterrupt:
            logger.warning("âš ï¸ ä¼˜åŒ–è¢«ç”¨æˆ·ä¸­æ–­")
        
        # ä¿å­˜æœ€ä½³å‚æ•°
        self.best_params = study.best_params
        self.best_score = -study.best_value  # è½¬å›æ­£å‡†ç¡®ç‡
        
        logger.info(f"âœ… {self.timeframe} è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        logger.info(f"   æœ€ä½³CVå‡†ç¡®ç‡: {self.best_score:.4f} ({self.best_score*100:.2f}%)")
        logger.info(f"   æœ€ä½³å‚æ•°: {self.best_params}")
        logger.info(f"   æ€»è¯•éªŒæ¬¡æ•°: {len(study.trials)}")
        
        # æ˜¾ç¤ºå‚æ•°é‡è¦æ€§ï¼ˆTop 5ï¼‰
        try:
            importances = optuna.importance.get_param_importances(study)
            logger.info(f"   å‚æ•°é‡è¦æ€§ï¼ˆTop 5ï¼‰:")
            for i, (param, importance) in enumerate(list(importances.items())[:5]):
                logger.info(f"      {i+1}. {param}: {importance:.4f}")
        except:
            pass
        
        return self.best_params
    
    def get_optimized_params(self) -> Dict[str, Any]:
        """
        è·å–ä¼˜åŒ–åçš„å‚æ•°
        
        Returns:
            æœ€ä½³å‚æ•°å­—å…¸ï¼Œå¦‚æœæœªä¼˜åŒ–åˆ™è¿”å›None
        """
        return self.best_params

