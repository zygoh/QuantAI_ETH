"""
ä¸¤æ¨¡å‹é›†æˆæœåŠ¡ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼šLightGBM + XGBoostï¼‰
CatBoostå®‰è£…å¤±è´¥æ—¶çš„é™çº§æ–¹æ¡ˆ
"""
import logging
from typing import Dict, Any
import pandas as pd
import numpy as np

from app.services.ensemble_ml_service import EnsembleMLService

logger = logging.getLogger(__name__)

class TwoModelEnsemble(EnsembleMLService):
    """ä¸¤æ¨¡å‹é›†æˆï¼ˆLightGBM + XGBoostï¼‰"""
    
    def _train_stacking_ensemble(
        self, 
        X_train: pd.DataFrame, 
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        timeframe: str
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒä¸¤æ¨¡å‹Stackingï¼ˆè·³è¿‡CatBoostï¼‰
        
        é™çº§æ–¹æ¡ˆï¼šåªç”¨LightGBM + XGBoost
        """
        import time
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
        
        start_time = time.time()
        
        logger.info(f"ğŸ¯ Stage 1: è®­ç»ƒ2ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆCatBoostè·³è¿‡ï¼‰...")
        
        # 1. è®­ç»ƒLightGBM
        logger.info(f"  ğŸ“Š è®­ç»ƒLightGBM...")
        lgb_model = self._train_lightgbm(X_train, y_train, timeframe)
        
        # 2. è®­ç»ƒXGBoost
        logger.info(f"  ğŸ“Š è®­ç»ƒXGBoost...")
        xgb_model = self._train_xgboost(X_train, y_train, timeframe)
        
        logger.info(f"âœ… 2ä¸ªåŸºç¡€æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # 3. ç”Ÿæˆå…ƒç‰¹å¾ï¼ˆ6ç»´ï¼š2ä¸ªæ¨¡å‹ Ã— 3ä¸ªç±»åˆ«ï¼‰
        logger.info(f"ğŸ¯ Stage 2: ç”Ÿæˆå…ƒç‰¹å¾...")
        lgb_pred_train = lgb_model.predict_proba(X_train)
        xgb_pred_train = xgb_model.predict_proba(X_train)
        
        meta_features_train = np.hstack([
            lgb_pred_train,
            xgb_pred_train
        ])
        
        # 4. è®­ç»ƒå…ƒå­¦ä¹ å™¨
        logger.info(f"ğŸ¯ Stage 3: è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆä¸¤æ¨¡å‹Stackingï¼‰...")
        meta_learner = LogisticRegression(
            multi_class='multinomial',
            solver='lbfgs',
            max_iter=1000,
            random_state=42
        )
        meta_learner.fit(meta_features_train, y_train)
        
        logger.info(f"âœ… å…ƒå­¦ä¹ å™¨è®­ç»ƒå®Œæˆ")
        
        # 5. éªŒè¯é›†è¯„ä¼°
        logger.info(f"ğŸ¯ Stage 4: éªŒè¯é›†è¯„ä¼°...")
        
        lgb_pred_val = lgb_model.predict_proba(X_val)
        xgb_pred_val = xgb_model.predict_proba(X_val)
        
        meta_features_val = np.hstack([
            lgb_pred_val,
            xgb_pred_val
        ])
        
        stacking_pred = meta_learner.predict(meta_features_val)
        stacking_proba = meta_learner.predict_proba(meta_features_val)
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_val, stacking_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_val, stacking_pred, average='weighted', zero_division=0
        )
        
        try:
            auc = roc_auc_score(y_val, stacking_proba, multi_class='ovr', average='weighted')
        except:
            auc = 0.5
        
        # å„åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡
        lgb_acc = accuracy_score(y_val, lgb_model.predict(X_val))
        xgb_acc = accuracy_score(y_val, xgb_model.predict(X_val))
        
        training_time = time.time() - start_time
        
        # ä¿å­˜ä¸¤æ¨¡å‹ï¼ˆä¸åŒ…å«catboostï¼‰
        self.ensemble_models[timeframe] = {
            'lgb': lgb_model,
            'xgb': xgb_model,
            'meta': meta_learner
        }
        
        # æ—¥å¿—è¾“å‡º
        logger.info(f"ğŸ“Š {timeframe} ä¸¤æ¨¡å‹Stackingè¯„ä¼°:")
        logger.info(f"  åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡:")
        logger.info(f"    LightGBM: {lgb_acc:.4f}")
        logger.info(f"    XGBoost:  {xgb_acc:.4f}")
        logger.info(f"  Stackingå‡†ç¡®ç‡: {accuracy:.4f}")
        logger.info(f"  æå‡: +{(accuracy - max(lgb_acc, xgb_acc))*100:.2f}%")
        logger.info(f"  ç²¾ç¡®ç‡: {precision:.4f}")
        logger.info(f"  å¬å›ç‡: {recall:.4f}")
        logger.info(f"  F1åˆ†æ•°: {f1:.4f}")
        logger.info(f"  AUC: {auc:.4f}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'base_models': {
                'lgb': lgb_acc,
                'xgb': xgb_acc
            },
            'training_time': training_time
        }
    
    async def predict(
        self, 
        symbol: str, 
        timeframe: str, 
        use_stacking: bool = True
    ) -> Dict[str, Any]:
        """ä¸¤æ¨¡å‹é›†æˆé¢„æµ‹"""
        try:
            if timeframe not in self.ensemble_models:
                logger.warning(f"âš ï¸ {timeframe} é›†æˆæ¨¡å‹æœªè®­ç»ƒï¼Œé™çº§åˆ°å•æ¨¡å‹")
                return await super(EnsembleMLService, self).predict(symbol, timeframe)
            
            # å‡†å¤‡æ•°æ®
            data = await self._prepare_prediction_data(symbol, timeframe)
            if data.empty:
                return None
            
            X = self._prepare_features_for_prediction(data, timeframe)
            if len(X) == 0:
                return None
            
            models = self.ensemble_models[timeframe]
            X_latest = X.iloc[[-1]]
            
            # ä¸¤æ¨¡å‹é¢„æµ‹
            lgb_proba = models['lgb'].predict_proba(X_latest)[0]
            xgb_proba = models['xgb'].predict_proba(X_latest)[0]
            
            # Stackingé¢„æµ‹ï¼ˆ6ç»´å…ƒç‰¹å¾ï¼‰
            if use_stacking and 'meta' in models:
                meta_features = np.hstack([lgb_proba, xgb_proba]).reshape(1, -1)
                stacking_proba = models['meta'].predict_proba(meta_features)[0]
                final_pred = stacking_proba.argmax()
                confidence = stacking_proba[final_pred]
                method = "Stacking(2-Model)"
            else:
                # ç®€å•åŠ æƒ
                ensemble_proba = lgb_proba * 0.6 + xgb_proba * 0.4
                final_pred = ensemble_proba.argmax()
                confidence = ensemble_proba[final_pred]
                method = "Weighted(2-Model)"
            
            signal_map = {0: 'SHORT', 1: 'HOLD', 2: 'LONG'}
            signal_type = signal_map[final_pred]
            
            return {
                'signal_type': signal_type,
                'confidence': float(confidence),
                'probabilities': {
                    'SHORT': float((lgb_proba[0] + xgb_proba[0]) / 2),
                    'HOLD': float((lgb_proba[1] + xgb_proba[1]) / 2),
                    'LONG': float((lgb_proba[2] + xgb_proba[2]) / 2)
                },
                'base_predictions': {
                    'lgb': {'type': signal_map[lgb_proba.argmax()], 'confidence': float(lgb_proba.max())},
                    'xgb': {'type': signal_map[xgb_proba.argmax()], 'confidence': float(xgb_proba.max())}
                },
                'method': method,
                'model_version': '2.0_two_model_ensemble'
            }
            
        except Exception as e:
            logger.error(f"ä¸¤æ¨¡å‹é›†æˆé¢„æµ‹å¤±è´¥: {e}")
            return None

# å…¨å±€å®ä¾‹
two_model_ensemble = TwoModelEnsemble()

