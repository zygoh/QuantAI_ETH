"""
å†å²æ•°æ®è·å–å’Œç®¡ç†
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import pandas as pd

from app.core.config import settings
from app.core.database import postgresql_manager
from app.services.binance_client import binance_client

logger = logging.getLogger(__name__)

class HistoricalDataManager:
    """å†å²æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.batch_size = 1000  # æ¯æ‰¹è·å–çš„æ•°æ®é‡
        self.rate_limit_delay = 0.1  # APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
    
    async def fetch_all_historical_data(self, symbol: str, days: int = 30):
        """è·å–æ‰€æœ‰æ—¶é—´æ¡†æ¶çš„å†å²æ•°æ®"""
        try:
            logger.info(f"å¼€å§‹è·å–å†å²æ•°æ®: {symbol} {days}å¤©")
            
            timeframes = settings.TIMEFRAMES
            
            for interval in timeframes:
                await self.fetch_historical_klines(symbol, interval, days)
                await asyncio.sleep(self.rate_limit_delay)
            
            logger.info(f"å†å²æ•°æ®è·å–å®Œæˆ: {symbol}")
            
        except Exception as e:
            logger.error(f"è·å–å†å²æ•°æ®å¤±è´¥: {e}")
            raise
    
    async def fetch_historical_klines(
        self, 
        symbol: str, 
        interval: str, 
        days: int = 30
    ):
        """è·å–æŒ‡å®šæ—¶é—´æ¡†æ¶çš„å†å²Kçº¿æ•°æ®"""
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆåªè·å–å·²å®Œæˆçš„Kçº¿ï¼‰
            now = datetime.now()
            interval_minutes = self._get_interval_minutes(interval)
            
            # âœ… è®¡ç®—æœ€åä¸€æ ¹å·²å®ŒæˆKçº¿çš„å¼€å§‹æ—¶é—´
            # ä¾‹å¦‚ï¼šå½“å‰16:17ï¼Œ15åˆ†é’ŸKçº¿ï¼Œæ­£åœ¨è¿›è¡Œçš„æ˜¯16:15-16:30ï¼Œæœ€åå®Œæˆçš„æ˜¯16:00-16:15
            current_minute = now.hour * 60 + now.minute
            current_period_start = (current_minute // interval_minutes) * interval_minutes
            # å‡å»ä¸€ä¸ªå‘¨æœŸå¾—åˆ°æœ€åå·²å®ŒæˆKçº¿çš„å¼€å§‹æ—¶é—´
            last_completed_start = current_period_start - interval_minutes
            
            # å¤„ç†è·¨å¤©çš„æƒ…å†µ
            if last_completed_start < 0:
                # å¦‚æœæ˜¯è´Ÿæ•°ï¼Œè¯´æ˜è·¨å¤©äº†ï¼Œä»å‰ä¸€å¤©ç®—
                end_time = (now - timedelta(days=1)).replace(
                    hour=23, minute=(1440 + last_completed_start) % 60, second=0, microsecond=0
                )
            else:
                end_time = now.replace(
                    minute=last_completed_start % 60, 
                    hour=last_completed_start // 60,
                    second=0, microsecond=0
                )
            
            start_time = end_time - timedelta(days=days)
            
            # è®¡ç®—éœ€è¦è·å–çš„æ‰¹æ¬¡
            total_klines = int((days * 24 * 60) / interval_minutes)
            batches = (total_klines + self.batch_size - 1) // self.batch_size
            
            logger.info(f"è·å–å†å²Kçº¿: {symbol} {interval} {days}å¤© {batches}æ‰¹æ¬¡ï¼ˆæˆªæ­¢åˆ°{end_time.strftime('%H:%M')}ï¼‰")
            
            all_klines = []
            current_end_time = int(end_time.timestamp() * 1000)
            
            for batch in range(batches):
                try:
                    # è·å–ä¸€æ‰¹æ•°æ®
                    klines = binance_client.get_klines(
                        symbol=symbol,
                        interval=interval,
                        limit=self.batch_size,
                        end_time=current_end_time
                    )
                    
                    if not klines:
                        break
                    
                    # æ·»åŠ åˆ°æ€»åˆ—è¡¨
                    all_klines.extend(klines)
                    
                    # æ›´æ–°æ—¶é—´èŒƒå›´
                    current_end_time = klines[0]['timestamp'] - 1
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»è·å–åˆ°è¶³å¤Ÿçš„æ•°æ®
                    if klines[0]['timestamp'] < int(start_time.timestamp() * 1000):
                        break
                    
                    logger.debug(f"æ‰¹æ¬¡ {batch + 1}/{batches} å®Œæˆ: {len(klines)}æ¡")
                    
                    # APIé™æµ
                    await asyncio.sleep(self.rate_limit_delay)
                    
                except Exception as e:
                    logger.error(f"è·å–æ‰¹æ¬¡æ•°æ®å¤±è´¥: {e}")
                    continue
            
            # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
            filtered_klines = [
                kline for kline in all_klines
                if kline['timestamp'] >= int(start_time.timestamp() * 1000)
            ]
            
            # æŒ‰æ—¶é—´æ’åº
            filtered_klines.sort(key=lambda x: x['timestamp'])
            
            # æ‰¹é‡å†™å…¥æ•°æ®åº“
            if filtered_klines:
                await self._batch_write_klines(symbol, interval, filtered_klines)
            
            logger.info(f"å†å²Kçº¿è·å–å®Œæˆ: {symbol} {interval} {len(filtered_klines)}æ¡")
            
        except Exception as e:
            logger.error(f"è·å–å†å²Kçº¿å¤±è´¥: {e}")
            raise
    
    async def _batch_write_klines(
        self, 
        symbol: str, 
        interval: str, 
        klines: List[Dict[str, Any]]
    ):
        """æ‰¹é‡å†™å…¥Kçº¿æ•°æ®ï¼ˆä¼˜åŒ–ï¼šä¸€æ¬¡æ€§å†™å…¥ï¼Œé¿å…å¾ªç¯è°ƒç”¨ï¼‰"""
        try:
            # ğŸ”¥ ä¼˜åŒ–ï¼šç»™æ‰€æœ‰æ•°æ®æ·»åŠ metadataåä¸€æ¬¡æ€§å†™å…¥
            klines_with_meta = [
                {**kline, 'symbol': symbol, 'interval': interval}
                for kline in klines
            ]
            
            # ä¸€æ¬¡æ€§å†™å…¥ï¼ˆå†…éƒ¨ä¼šè‡ªåŠ¨åˆ†æ‰¹ï¼‰
            await postgresql_manager.write_kline_data(klines_with_meta)
            
            logger.debug(f"æ‰¹é‡å†™å…¥å®Œæˆ: {symbol} {interval} {len(klines)}æ¡")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å†™å…¥Kçº¿æ•°æ®å¤±è´¥: {e}")
            raise
    
    def _get_interval_minutes(self, interval: str) -> int:
        """è·å–æ—¶é—´é—´éš”çš„åˆ†é’Ÿæ•°"""
        interval_map = {
            '1m': 1,
            '3m': 3,
            '5m': 5,
            '15m': 15,
            '30m': 30,
            '1h': 60,
            '2h': 120,
            '4h': 240,
            '6h': 360,
            '8h': 480,
            '12h': 720,
            '1d': 1440,
            '3d': 4320,
            '1w': 10080,
            '1M': 43200
        }
        return interval_map.get(interval, 60)
    
    async def update_recent_data(self, symbol: str, hours: int = 24):
        """æ›´æ–°æœ€è¿‘çš„æ•°æ®"""
        try:
            logger.info(f"æ›´æ–°æœ€è¿‘æ•°æ®: {symbol} {hours}å°æ—¶")
            
            timeframes = settings.TIMEFRAMES
            
            for interval in timeframes:
                await self._update_recent_klines(symbol, interval, hours)
                await asyncio.sleep(self.rate_limit_delay)
            
            logger.info(f"æœ€è¿‘æ•°æ®æ›´æ–°å®Œæˆ: {symbol}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°æœ€è¿‘æ•°æ®å¤±è´¥: {e}")
    
    async def _update_recent_klines(
        self, 
        symbol: str, 
        interval: str, 
        hours: int = 24
    ):
        """æ›´æ–°æœ€è¿‘çš„Kçº¿æ•°æ®ï¼ˆåªè·å–å·²å®Œæˆçš„Kçº¿ï¼‰"""
        try:
            # è®¡ç®—éœ€è¦çš„æ•°æ®é‡
            interval_minutes = self._get_interval_minutes(interval)
            limit = min(int((hours * 60) / interval_minutes), 1000)
            
            # âœ… è®¡ç®—æœ€åä¸€æ ¹å·²å®ŒæˆKçº¿çš„å¼€å§‹æ—¶é—´
            now = datetime.now()
            current_minute = now.hour * 60 + now.minute
            current_period_start = (current_minute // interval_minutes) * interval_minutes
            # å‡å»ä¸€ä¸ªå‘¨æœŸå¾—åˆ°æœ€åå·²å®ŒæˆKçº¿çš„å¼€å§‹æ—¶é—´
            last_completed_start = current_period_start - interval_minutes
            
            # å¤„ç†è·¨å¤©çš„æƒ…å†µ
            if last_completed_start < 0:
                end_time = (now - timedelta(days=1)).replace(
                    hour=23, minute=(1440 + last_completed_start) % 60, second=0, microsecond=0
                )
            else:
                end_time = now.replace(
                    minute=last_completed_start % 60, 
                    hour=last_completed_start // 60,
                    second=0, microsecond=0
                )
            
            end_time_ms = int(end_time.timestamp() * 1000)
            
            # è·å–æœ€æ–°æ•°æ®ï¼ˆåªåˆ°æœ€åå·²å®Œæˆçš„Kçº¿ï¼‰
            klines = binance_client.get_klines(
                symbol=symbol,
                interval=interval,
                limit=limit,
                end_time=end_time_ms  # âœ… åªè·å–å·²å®Œæˆçš„Kçº¿
            )
            
            if klines:
                # ç»™æ¯æ¡æ•°æ®æ·»åŠ  symbol å’Œ interval
                klines_with_meta = [
                    {**kline, 'symbol': symbol, 'interval': interval}
                    for kline in klines
                ]
                await postgresql_manager.write_kline_data(klines_with_meta)
                logger.debug(f"æ›´æ–°Kçº¿æ•°æ®: {symbol} {interval} {len(klines)}æ¡")
            
        except Exception as e:
            logger.error(f"æ›´æ–°Kçº¿æ•°æ®å¤±è´¥: {e}")
    
    async def validate_data_integrity(self, symbol: str, interval: str, days: int = 7):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        try:
            end_time = datetime.now()
            start_time = end_time - timedelta(days=days)
            
            # ä»æ•°æ®åº“æŸ¥è¯¢æ•°æ®
            df = await postgresql_manager.query_kline_data(
                symbol, interval, start_time, end_time
            )
            
            if df.empty:
                logger.warning(f"æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®: {symbol} {interval}")
                return False
            
            # æ£€æŸ¥æ•°æ®è¿ç»­æ€§
            interval_minutes = self._get_interval_minutes(interval)
            expected_count = int((days * 24 * 60) / interval_minutes)
            actual_count = len(df)
            
            completeness = actual_count / expected_count
            
            logger.info(f"æ•°æ®å®Œæ•´æ€§: {symbol} {interval} {completeness:.2%} ({actual_count}/{expected_count})")
            
            # å¦‚æœå®Œæ•´æ€§ä½äº90%ï¼Œå»ºè®®é‡æ–°è·å–
            if completeness < 0.9:
                logger.warning(f"æ•°æ®å®Œæ•´æ€§è¾ƒä½ï¼Œå»ºè®®é‡æ–°è·å–: {symbol} {interval}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"éªŒè¯æ•°æ®å®Œæ•´æ€§å¤±è´¥: {e}")
            return False
    
    async def get_data_summary(self, symbol: str) -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦"""
        try:
            summary = {
                'symbol': symbol,
                'timeframes': {},
                'total_records': 0,
                'date_range': {}
            }
            
            for interval in settings.TIMEFRAMES:
                # æŸ¥è¯¢æœ€è¿‘7å¤©çš„æ•°æ®
                end_time = datetime.now()
                start_time = end_time - timedelta(days=7)
                
                df = await postgresql_manager.query_kline_data(
                    symbol, interval, start_time, end_time
                )
                
                if not df.empty:
                    summary['timeframes'][interval] = {
                        'count': len(df),
                        'start_time': df['timestamp'].min().isoformat(),
                        'end_time': df['timestamp'].max().isoformat(),
                        'latest_price': float(df['close'].iloc[-1])
                    }
                    summary['total_records'] += len(df)
                else:
                    summary['timeframes'][interval] = {
                        'count': 0,
                        'start_time': None,
                        'end_time': None,
                        'latest_price': None
                    }
            
            return summary
            
        except Exception as e:
            logger.error(f"è·å–æ•°æ®æ‘˜è¦å¤±è´¥: {e}")
            return {}
    
    async def cleanup_old_data(self, days: int = 30):
        """æ¸…ç†æ—§æ•°æ®"""
        try:
            await postgresql_manager.cleanup_old_data(days)
            logger.info(f"æ¸…ç†äº†{days}å¤©å‰çš„æ—§æ•°æ®")
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")

# å…¨å±€å†å²æ•°æ®ç®¡ç†å™¨å®ä¾‹
historical_data_manager = HistoricalDataManager()