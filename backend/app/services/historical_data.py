"""
å†å²æ•°æ®è·å–å’Œç®¡ç†
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import pandas as pd

from app.core.config import settings
from app.core.database import postgresql_manager
from app.exchange.exchange_factory import ExchangeFactory

logger = logging.getLogger(__name__)

class HistoricalDataManager:
    """å†å²æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.batch_size = 1000  # æ¯æ‰¹è·å–çš„æ•°æ®é‡
        self.rate_limit_delay = 0.1  # APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
        # ğŸ”‘ è·å–äº¤æ˜“æ‰€å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨å·¥å‚æ¨¡å¼ï¼Œæ”¯æŒå¤šäº¤æ˜“æ‰€ï¼‰
        self.exchange_client = ExchangeFactory.get_current_client()
    
    async def fetch_all_historical_data(self, symbol: str, days: int = 30):
        """è·å–æ‰€æœ‰æ—¶é—´æ¡†æ¶çš„å†å²æ•°æ®"""
        try:
            logger.info(f"å¼€å§‹è·å–å†å²æ•°æ®: {symbol} {days}å¤©")
            
            timeframes = settings.TIMEFRAMES
            
            for interval in timeframes:
                await self.fetch_historical_klines(symbol, interval, days)
                await asyncio.sleep(self.rate_limit_delay)
            
            logger.info(f"å†å²æ•°æ®è·å–å®Œæˆ: {symbol}")
            
        except Exception as e:
            logger.error(f"è·å–å†å²æ•°æ®å¤±è´¥: {e}")
            raise
    
    async def fetch_historical_klines(
        self, 
        symbol: str, 
        interval: str, 
        days: int = 30
    ):
        """è·å–æŒ‡å®šæ—¶é—´æ¡†æ¶çš„å†å²Kçº¿æ•°æ®"""
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆåªè·å–å·²å®Œæˆçš„Kçº¿ï¼‰
            now = datetime.now()
            interval_minutes = self._get_interval_minutes(interval)
            
            # âœ… è®¡ç®—æœ€åä¸€æ ¹å·²å®ŒæˆKçº¿çš„å¼€å§‹æ—¶é—´
            # ä¾‹å¦‚ï¼šå½“å‰16:17ï¼Œ15åˆ†é’ŸKçº¿ï¼Œæ­£åœ¨è¿›è¡Œçš„æ˜¯16:15-16:30ï¼Œæœ€åå®Œæˆçš„æ˜¯16:00-16:15
            current_minute = now.hour * 60 + now.minute
            current_period_start = (current_minute // interval_minutes) * interval_minutes
            # å‡å»ä¸€ä¸ªå‘¨æœŸå¾—åˆ°æœ€åå·²å®ŒæˆKçº¿çš„å¼€å§‹æ—¶é—´
            last_completed_start = current_period_start - interval_minutes
            
            # å¤„ç†è·¨å¤©çš„æƒ…å†µ
            if last_completed_start < 0:
                # å¦‚æœæ˜¯è´Ÿæ•°ï¼Œè¯´æ˜è·¨å¤©äº†ï¼Œä»å‰ä¸€å¤©ç®—
                end_time = (now - timedelta(days=1)).replace(
                    hour=23, minute=(1440 + last_completed_start) % 60, second=0, microsecond=0
                )
            else:
                end_time = now.replace(
                    minute=last_completed_start % 60, 
                    hour=last_completed_start // 60,
                    second=0, microsecond=0
                )
            
            start_time = end_time - timedelta(days=days)
            
            # è®¡ç®—éœ€è¦è·å–çš„æ‰¹æ¬¡
            total_klines = int((days * 24 * 60) / interval_minutes)
            batches = (total_klines + self.batch_size - 1) // self.batch_size
            
            logger.info(f"è·å–å†å²Kçº¿: {symbol} {interval} {days}å¤© {batches}æ‰¹æ¬¡ï¼ˆæˆªæ­¢åˆ°{end_time.strftime('%H:%M')}ï¼‰")
            
            all_klines = []
            current_end_time = int(end_time.timestamp() * 1000)
            
            # âœ… ç»Ÿä¸€ä½¿ç”¨åˆ†é¡µæ–¹æ³•ï¼ˆè‡ªåŠ¨å¤„ç†è¶…è¿‡1500çš„æƒ…å†µï¼‰
            all_klines = self.exchange_client.get_klines_paginated(
                        symbol=symbol,
                        interval=interval,
                limit=total_klines,
                end_time=current_end_time,
                rate_limit_delay=self.rate_limit_delay
            )
            
            # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„æ•°æ®ï¼ˆå› ä¸ºåˆ†é¡µæ–¹æ³•å¯èƒ½è·å–äº†è¶…å‡ºèŒƒå›´çš„æ•°æ®ï¼‰
            start_time_ms = int(start_time.timestamp() * 1000)
            filtered_klines = [
                kline for kline in all_klines
                if kline['timestamp'] >= start_time_ms
            ]
            
            # æŒ‰æ—¶é—´æ’åº
            filtered_klines.sort(key=lambda x: x['timestamp'])
            
            # æ‰¹é‡å†™å…¥æ•°æ®åº“
            if filtered_klines:
                await self._batch_write_klines(symbol, interval, filtered_klines)
            
            logger.info(f"å†å²Kçº¿è·å–å®Œæˆ: {symbol} {interval} {len(filtered_klines)}æ¡")
            
        except Exception as e:
            logger.error(f"è·å–å†å²Kçº¿å¤±è´¥: {e}")
            raise
    
    async def _batch_write_klines(
        self, 
        symbol: str, 
        interval: str, 
        klines: List[Dict[str, Any]]
    ):
        """æ‰¹é‡å†™å…¥Kçº¿æ•°æ®ï¼ˆä¼˜åŒ–ï¼šä¸€æ¬¡æ€§å†™å…¥ï¼Œé¿å…å¾ªç¯è°ƒç”¨ï¼‰"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šå°†UnifiedKlineDataå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
            from dataclasses import asdict
            from app.exchange.base_exchange_client import UnifiedKlineData
            
            klines_with_meta = []
            for kline in klines:
                if isinstance(kline, UnifiedKlineData):
                    # è½¬æ¢ä¸ºå­—å…¸
                    kline_dict = asdict(kline)
                elif isinstance(kline, dict):
                    # å·²ç»æ˜¯å­—å…¸
                    kline_dict = kline
                else:
                    logger.warning(f"âš ï¸ æœªçŸ¥çš„Kçº¿æ•°æ®ç±»å‹: {type(kline)}")
                    continue
                
                # æ·»åŠ  symbol å’Œ interval
                kline_dict['symbol'] = symbol
                kline_dict['interval'] = interval
                klines_with_meta.append(kline_dict)
            
            # ä¸€æ¬¡æ€§å†™å…¥ï¼ˆå†…éƒ¨ä¼šè‡ªåŠ¨åˆ†æ‰¹ï¼‰
            await postgresql_manager.write_kline_data(klines_with_meta)
            
            logger.debug(f"æ‰¹é‡å†™å…¥å®Œæˆ: {symbol} {interval} {len(klines)}æ¡")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å†™å…¥Kçº¿æ•°æ®å¤±è´¥: {e}")
            raise
    
    def _get_interval_minutes(self, interval: str) -> int:
        """è·å–æ—¶é—´é—´éš”çš„åˆ†é’Ÿæ•°"""
        interval_map = {
            '1m': 1,
            '3m': 3,
            '5m': 5,
            '15m': 15,
            '30m': 30,
            '1h': 60,
            '2h': 120,
            '4h': 240,
            '6h': 360,
            '8h': 480,
            '12h': 720,
            '1d': 1440,
            '3d': 4320,
            '1w': 10080,
            '1M': 43200
        }
        return interval_map.get(interval, 60)
    
    async def update_recent_data(self, symbol: str, hours: int = 24):
        """æ›´æ–°æœ€è¿‘çš„æ•°æ®"""
        try:
            logger.info(f"æ›´æ–°æœ€è¿‘æ•°æ®: {symbol} {hours}å°æ—¶")
            
            timeframes = settings.TIMEFRAMES
            
            for interval in timeframes:
                await self._update_recent_klines(symbol, interval, hours)
                await asyncio.sleep(self.rate_limit_delay)
            
            logger.info(f"æœ€è¿‘æ•°æ®æ›´æ–°å®Œæˆ: {symbol}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°æœ€è¿‘æ•°æ®å¤±è´¥: {e}")
    
    async def _update_recent_klines(
        self, 
        symbol: str, 
        interval: str, 
        hours: int = 24
    ):
        """æ›´æ–°æœ€è¿‘çš„Kçº¿æ•°æ®ï¼ˆåªè·å–å·²å®Œæˆçš„Kçº¿ï¼‰"""
        try:
            # è®¡ç®—éœ€è¦çš„æ•°æ®é‡
            interval_minutes = self._get_interval_minutes(interval)
            limit = min(int((hours * 60) / interval_minutes), 1000)
            
            # âœ… è®¡ç®—æœ€åä¸€æ ¹å·²å®ŒæˆKçº¿çš„å¼€å§‹æ—¶é—´
            now = datetime.now()
            current_minute = now.hour * 60 + now.minute
            current_period_start = (current_minute // interval_minutes) * interval_minutes
            # å‡å»ä¸€ä¸ªå‘¨æœŸå¾—åˆ°æœ€åå·²å®ŒæˆKçº¿çš„å¼€å§‹æ—¶é—´
            last_completed_start = current_period_start - interval_minutes
            
            # å¤„ç†è·¨å¤©çš„æƒ…å†µ
            if last_completed_start < 0:
                end_time = (now - timedelta(days=1)).replace(
                    hour=23, minute=(1440 + last_completed_start) % 60, second=0, microsecond=0
                )
            else:
                end_time = now.replace(
                    minute=last_completed_start % 60, 
                    hour=last_completed_start // 60,
                    second=0, microsecond=0
                )
            
            end_time_ms = int(end_time.timestamp() * 1000)
            
            # âœ… ç»Ÿä¸€ä½¿ç”¨åˆ†é¡µæ–¹æ³•ï¼ˆè‡ªåŠ¨å¤„ç†è¶…è¿‡1500çš„æƒ…å†µï¼‰
            # è·å–æœ€æ–°æ•°æ®ï¼ˆåªåˆ°æœ€åå·²å®Œæˆçš„Kçº¿ï¼‰
            klines = self.exchange_client.get_klines_paginated(
                symbol=symbol,
                interval=interval,
                limit=limit,
                end_time=end_time_ms  # âœ… åªè·å–å·²å®Œæˆçš„Kçº¿
            )
            
            if klines:
                # ğŸ”§ ä¿®å¤ï¼šå°†UnifiedKlineDataå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                from dataclasses import asdict
                from app.exchange.base_exchange_client import UnifiedKlineData
                
                klines_dict = []
                for kline in klines:
                    if isinstance(kline, UnifiedKlineData):
                        # è½¬æ¢ä¸ºå­—å…¸
                        kline_dict = asdict(kline)
                    elif isinstance(kline, dict):
                        # å·²ç»æ˜¯å­—å…¸
                        kline_dict = kline
                    else:
                        logger.warning(f"âš ï¸ æœªçŸ¥çš„Kçº¿æ•°æ®ç±»å‹: {type(kline)}")
                        continue
                    
                    # æ·»åŠ  symbol å’Œ interval
                    kline_dict['symbol'] = symbol
                    kline_dict['interval'] = interval
                    klines_dict.append(kline_dict)
                
                await postgresql_manager.write_kline_data(klines_dict)
                logger.debug(f"æ›´æ–°Kçº¿æ•°æ®: {symbol} {interval} {len(klines)}æ¡")
            
        except Exception as e:
            logger.error(f"æ›´æ–°Kçº¿æ•°æ®å¤±è´¥: {e}")
    
    async def validate_data_integrity(self, symbol: str, interval: str, days: int = 7):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        try:
            end_time = datetime.now()
            start_time = end_time - timedelta(days=days)
            
            # ä»æ•°æ®åº“æŸ¥è¯¢æ•°æ®
            df = await postgresql_manager.query_kline_data(
                symbol, interval, start_time, end_time
            )
            
            if df.empty:
                logger.warning(f"æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®: {symbol} {interval}")
                return False
            
            # æ£€æŸ¥æ•°æ®è¿ç»­æ€§
            interval_minutes = self._get_interval_minutes(interval)
            expected_count = int((days * 24 * 60) / interval_minutes)
            actual_count = len(df)
            
            completeness = actual_count / expected_count
            
            logger.info(f"æ•°æ®å®Œæ•´æ€§: {symbol} {interval} {completeness:.2%} ({actual_count}/{expected_count})")
            
            # å¦‚æœå®Œæ•´æ€§ä½äº90%ï¼Œå»ºè®®é‡æ–°è·å–
            if completeness < 0.9:
                logger.warning(f"æ•°æ®å®Œæ•´æ€§è¾ƒä½ï¼Œå»ºè®®é‡æ–°è·å–: {symbol} {interval}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"éªŒè¯æ•°æ®å®Œæ•´æ€§å¤±è´¥: {e}")
            return False
    
    async def get_data_summary(self, symbol: str) -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦"""
        try:
            summary = {
                'symbol': symbol,
                'timeframes': {},
                'total_records': 0,
                'date_range': {}
            }
            
            for interval in settings.TIMEFRAMES:
                # æŸ¥è¯¢æœ€è¿‘7å¤©çš„æ•°æ®
                end_time = datetime.now()
                start_time = end_time - timedelta(days=7)
                
                df = await postgresql_manager.query_kline_data(
                    symbol, interval, start_time, end_time
                )
                
                if not df.empty:
                    summary['timeframes'][interval] = {
                        'count': len(df),
                        'start_time': df['timestamp'].min().isoformat(),
                        'end_time': df['timestamp'].max().isoformat(),
                        'latest_price': float(df['close'].iloc[-1])
                    }
                    summary['total_records'] += len(df)
                else:
                    summary['timeframes'][interval] = {
                        'count': 0,
                        'start_time': None,
                        'end_time': None,
                        'latest_price': None
                    }
            
            return summary
            
        except Exception as e:
            logger.error(f"è·å–æ•°æ®æ‘˜è¦å¤±è´¥: {e}")
            return {}
    
    async def cleanup_old_data(self, days: int = 30):
        """æ¸…ç†æ—§æ•°æ®"""
        try:
            await postgresql_manager.cleanup_old_data(days)
            logger.info(f"æ¸…ç†äº†{days}å¤©å‰çš„æ—§æ•°æ®")
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")

    async def fetch_historical_derivatives_data(
        self, 
        symbol: str, 
        days: int = 120
    ) -> Dict[str, pd.DataFrame]:
        """
        è·å–å†å²è¡ç”Ÿå“æ•°æ®ï¼ˆèµ„é‡‘è´¹ç‡ã€æŒä»“é‡ã€å¤šç©ºæ¯”ï¼‰
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            days: è·å–å¤©æ•°ï¼ˆé»˜è®¤120å¤©ï¼‰
        
        Returns:
            åŒ…å«å†å²è¡ç”Ÿå“æ•°æ®çš„å­—å…¸ï¼š
            - funding_rate: èµ„é‡‘è´¹ç‡DataFrameï¼ˆtimestamp, funding_rateï¼‰
            - open_interest: æŒä»“é‡DataFrameï¼ˆtimestamp, open_interest_usdï¼‰
            - long_short_ratio: å¤šç©ºæ¯”DataFrameï¼ˆtimestamp, long_short_ratioï¼‰
        """
        try:
            logger.info(f"å¼€å§‹è·å–å†å²è¡ç”Ÿå“æ•°æ®: {symbol} {days}å¤©")
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            from datetime import datetime, timedelta
            end_time = datetime.now()
            start_time = end_time - timedelta(days=days)
            start_time_ms = int(start_time.timestamp() * 1000)
            end_time_ms = int(end_time.timestamp() * 1000)
            
            result = {}
            
            # 1. è·å–å†å²èµ„é‡‘è´¹ç‡ï¼ˆæ¯8å°æ—¶ä¸€æ¬¡ï¼Œ120å¤©éœ€è¦360æ¡ï¼‰
            try:
                funding_history = self.exchange_client.get_historical_funding_rate(
                    symbol=symbol,
                    start_time=start_time_ms,
                    end_time=end_time_ms,
                    limit=360  # 120å¤© = 360ä¸ª8å°æ—¶å‘¨æœŸ
                )
                
                if funding_history:
                    df_funding = pd.DataFrame(funding_history)
                    df_funding['timestamp'] = pd.to_datetime(df_funding['timestamp'], unit='ms')
                    df_funding = df_funding.sort_values('timestamp')
                    result['funding_rate'] = df_funding
                    logger.info(f"âœ… è·å–å†å²èµ„é‡‘è´¹ç‡: {len(df_funding)}æ¡")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°å†å²èµ„é‡‘è´¹ç‡æ•°æ®")
                    result['funding_rate'] = pd.DataFrame(columns=['timestamp', 'funding_rate'])
            except Exception as e:
                logger.error(f"è·å–å†å²èµ„é‡‘è´¹ç‡å¤±è´¥: {e}")
                result['funding_rate'] = pd.DataFrame(columns=['timestamp', 'funding_rate'])
            
            # 2. è·å–å†å²æŒä»“é‡ï¼ˆä½¿ç”¨Rubik APIï¼Œéœ€è¦periodå‚æ•°ï¼‰
            try:
                # æ ¹æ®æ•°æ®è·¨åº¦é€‰æ‹©åˆé€‚çš„period
                # å¦‚æœæ•°æ®è·¨åº¦å¤§ï¼Œä½¿ç”¨è¾ƒé•¿çš„periodï¼ˆå¦‚1Hï¼‰ï¼Œå¦åˆ™ä½¿ç”¨5m
                if days >= 30:
                    period = "1H"  # 30å¤©ä»¥ä¸Šä½¿ç”¨1å°æ—¶å‘¨æœŸ
                elif days >= 7:
                    period = "15m"  # 7-30å¤©ä½¿ç”¨15åˆ†é’Ÿå‘¨æœŸ
                else:
                    period = "5m"  # 7å¤©ä»¥ä¸‹ä½¿ç”¨5åˆ†é’Ÿå‘¨æœŸ
                
                oi_history = self.exchange_client.get_historical_open_interest(
                    symbol=symbol,
                    start_time=start_time_ms,
                    end_time=end_time_ms,
                    period=period  # Rubik APIéœ€è¦periodå‚æ•°
                )
                
                if oi_history:
                    df_oi = pd.DataFrame(oi_history)
                    df_oi['timestamp'] = pd.to_datetime(df_oi['timestamp'], unit='ms')
                    df_oi = df_oi.sort_values('timestamp')
                    result['open_interest'] = df_oi
                    logger.info(f"âœ… è·å–å†å²æŒä»“é‡: {len(df_oi)}æ¡")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°å†å²æŒä»“é‡æ•°æ®ï¼ˆOKXå¯èƒ½ä¸æä¾›å†å²æ•°æ®ï¼‰")
                    result['open_interest'] = pd.DataFrame(columns=['timestamp', 'open_interest_usd'])
            except Exception as e:
                logger.error(f"è·å–å†å²æŒä»“é‡å¤±è´¥: {e}")
                result['open_interest'] = pd.DataFrame(columns=['timestamp', 'open_interest_usd'])
            
            # 3. è·å–å†å²å¤šç©ºæ¯”ï¼ˆä½¿ç”¨Rubik APIï¼Œéœ€è¦periodå‚æ•°ï¼‰
            try:
                # æ ¹æ®æ•°æ®è·¨åº¦é€‰æ‹©åˆé€‚çš„periodï¼ˆä¸æŒä»“é‡ä¿æŒä¸€è‡´ï¼‰
                if days >= 30:
                    period = "1H"  # 30å¤©ä»¥ä¸Šä½¿ç”¨1å°æ—¶å‘¨æœŸ
                elif days >= 7:
                    period = "15m"  # 7-30å¤©ä½¿ç”¨15åˆ†é’Ÿå‘¨æœŸ
                else:
                    period = "5m"  # 7å¤©ä»¥ä¸‹ä½¿ç”¨5åˆ†é’Ÿå‘¨æœŸ
                
                ls_history = self.exchange_client.get_historical_long_short_ratio(
                    symbol=symbol,
                    start_time=start_time_ms,
                    end_time=end_time_ms,
                    period=period  # Rubik APIéœ€è¦periodå‚æ•°
                )
                
                if ls_history:
                    df_ls = pd.DataFrame(ls_history)
                    df_ls['timestamp'] = pd.to_datetime(df_ls['timestamp'], unit='ms')
                    df_ls = df_ls.sort_values('timestamp')
                    result['long_short_ratio'] = df_ls
                    logger.info(f"âœ… è·å–å†å²å¤šç©ºæ¯”: {len(df_ls)}æ¡")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ°å†å²å¤šç©ºæ¯”æ•°æ®ï¼ˆOKXå¯èƒ½ä¸æä¾›å†å²æ•°æ®ï¼‰")
                    result['long_short_ratio'] = pd.DataFrame(columns=['timestamp', 'long_short_ratio'])
            except Exception as e:
                logger.error(f"è·å–å†å²å¤šç©ºæ¯”å¤±è´¥: {e}")
                result['long_short_ratio'] = pd.DataFrame(columns=['timestamp', 'long_short_ratio'])
            
            logger.info(f"å†å²è¡ç”Ÿå“æ•°æ®è·å–å®Œæˆ: {symbol}")
            return result
            
        except Exception as e:
            logger.error(f"è·å–å†å²è¡ç”Ÿå“æ•°æ®å¤±è´¥: {e}")
            return {
                'funding_rate': pd.DataFrame(columns=['timestamp', 'funding_rate']),
                'open_interest': pd.DataFrame(columns=['timestamp', 'open_interest_usd']),
                'long_short_ratio': pd.DataFrame(columns=['timestamp', 'long_short_ratio'])
            }
    
    def merge_derivatives_to_klines(
        self, 
        df_klines: pd.DataFrame, 
        derivatives_data: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """
        å°†å†å²è¡ç”Ÿå“æ•°æ®åˆå¹¶åˆ°Kçº¿æ•°æ®ä¸­ï¼ˆæŒ‰æ—¶é—´æˆ³å¯¹é½ï¼‰
        
        Args:
            df_klines: Kçº¿DataFrameï¼ˆå¿…é¡»åŒ…å«timestampåˆ—ï¼‰
            derivatives_data: å†å²è¡ç”Ÿå“æ•°æ®å­—å…¸
        
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        try:
            if df_klines.empty:
                return df_klines
            
            # ç¡®ä¿timestampæ˜¯datetimeç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(df_klines['timestamp']):
                df_klines['timestamp'] = pd.to_datetime(df_klines['timestamp'])
            
            df_result = df_klines.copy()
            
            # 1. åˆå¹¶èµ„é‡‘è´¹ç‡ï¼ˆå‰å‘å¡«å……ï¼Œå› ä¸ºèµ„é‡‘è´¹ç‡æ¯8å°æ—¶å˜åŒ–ä¸€æ¬¡ï¼‰
            if 'funding_rate' in derivatives_data and not derivatives_data['funding_rate'].empty:
                df_funding = derivatives_data['funding_rate'][['timestamp', 'funding_rate']].copy()
                df_funding = df_funding.sort_values('timestamp')
                
                # ä½¿ç”¨merge_asofè¿›è¡Œæ—¶é—´å¯¹é½ï¼ˆå‰å‘å¡«å……ï¼‰
                df_result = pd.merge_asof(
                    df_result.sort_values('timestamp'),
                    df_funding,
                    on='timestamp',
                    direction='forward'  # å‰å‘å¡«å……ï¼šä½¿ç”¨æœ€è¿‘çš„å†å²å€¼
                )
                logger.debug(f"âœ… åˆå¹¶èµ„é‡‘è´¹ç‡: {df_result['funding_rate'].notna().sum()}/{len(df_result)}è¡Œæœ‰æ•°æ®")
            
            # 2. åˆå¹¶æŒä»“é‡ï¼ˆå‰å‘å¡«å……ï¼‰
            if 'open_interest' in derivatives_data and not derivatives_data['open_interest'].empty:
                df_oi = derivatives_data['open_interest'][['timestamp', 'open_interest_usd']].copy()
                df_oi = df_oi.sort_values('timestamp')
                
                df_result = pd.merge_asof(
                    df_result.sort_values('timestamp'),
                    df_oi,
                    on='timestamp',
                    direction='forward'
                )
                logger.debug(f"âœ… åˆå¹¶æŒä»“é‡: {df_result['open_interest_usd'].notna().sum()}/{len(df_result)}è¡Œæœ‰æ•°æ®")
            
            # 3. åˆå¹¶å¤šç©ºæ¯”ï¼ˆå‰å‘å¡«å……ï¼‰
            if 'long_short_ratio' in derivatives_data and not derivatives_data['long_short_ratio'].empty:
                df_ls = derivatives_data['long_short_ratio'][['timestamp', 'long_short_ratio']].copy()
                df_ls = df_ls.sort_values('timestamp')
                
                df_result = pd.merge_asof(
                    df_result.sort_values('timestamp'),
                    df_ls,
                    on='timestamp',
                    direction='forward'
                )
                logger.debug(f"âœ… åˆå¹¶å¤šç©ºæ¯”: {df_result['long_short_ratio'].notna().sum()}/{len(df_result)}è¡Œæœ‰æ•°æ®")
            
            return df_result
            
        except Exception as e:
            logger.error(f"åˆå¹¶è¡ç”Ÿå“æ•°æ®å¤±è´¥: {e}")
            return df_klines

# å…¨å±€å†å²æ•°æ®ç®¡ç†å™¨å®ä¾‹
historical_data_manager = HistoricalDataManager()