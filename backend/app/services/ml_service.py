"""
æœºå™¨å­¦ä¹ æœåŠ¡
"""
import asyncio
import logging
import pickle
import os
import gc
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import lightgbm as lgb
import joblib

from app.core.config import settings
from app.core.database import postgresql_manager
from app.core.cache import cache_manager
from app.services.feature_engineering import feature_engineer
from app.services.data_service import DataService

logger = logging.getLogger(__name__)

class MLService:
    """æœºå™¨å­¦ä¹ æœåŠ¡ï¼ˆæ”¯æŒå¤šæ—¶é—´æ¡†æ¶ç‹¬ç«‹æ¨¡å‹ï¼‰"""
    
    def __init__(self):
        self.is_running = False
        # å¤šæ—¶é—´æ¡†æ¶æ¨¡å‹ï¼š{'15m': model, '2h': model, '4h': model}
        self.models = {}
        self.scalers = {}
        self.feature_columns_dict = {}
        
        # ğŸ”‘ åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨ï¼ˆä¿®å¤ï¼šå­ç±»éœ€è¦è®¿é—®ï¼‰
        self.feature_engineer = feature_engineer
        self.model_metrics = {}
        self.training_task = None
        self.is_first_training = True  # æ ‡è®°æ˜¯å¦é¦–æ¬¡è®­ç»ƒï¼ˆåªæœ‰é¦–æ¬¡æ‰å†™æ•°æ®åº“ï¼‰
        
        # æ¨¡å‹å‚æ•°
        # LightGBMåŸºç¡€å‚æ•°ï¼ˆæ‰€æœ‰æ—¶é—´æ¡†æ¶å…±äº«ï¼‰
        self.lgb_params = {
            'objective': 'multiclass',
            'num_class': 3,  # 0: ä¸‹è·Œ, 1: æ¨ªç›˜, 2: ä¸Šæ¶¨
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'n_estimators': 500,  # âœ… å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤100â†’500ï¼‰
            'num_leaves': 31,  # é»˜è®¤å€¼ï¼Œè®­ç»ƒæ—¶ä¼šæ ¹æ®æ—¶é—´æ¡†æ¶è°ƒæ•´
            'learning_rate': 0.03,  # âœ… é™ä½å­¦ä¹ ç‡ï¼ˆ0.05â†’0.03ï¼‰ï¼Œæ›´ç¨³å®š
            'feature_fraction': 0.85,  # âœ… ç‰¹å¾é‡‡æ ·ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
            'bagging_fraction': 0.85,  # âœ… æ•°æ®é‡‡æ ·ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': 42,
            'n_jobs': -1,
            'max_depth': 8,  # âœ… é™åˆ¶æ ‘æ·±åº¦ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
            'min_child_samples': 30,  # âœ… å¢åŠ å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬ï¼ˆ20â†’30ï¼‰
            'reg_alpha': 0.3,  # âœ… å¢å¼ºL1æ­£åˆ™åŒ–ï¼ˆ0.1â†’0.3ï¼‰
            'reg_lambda': 0.3,  # âœ… å¢å¼ºL2æ­£åˆ™åŒ–ï¼ˆ0.1â†’0.3ï¼‰
            'min_split_gain': 0.01,  # âœ… æœ€å°åˆ†è£‚å¢ç›Šï¼ˆé˜²æ­¢è¿‡åº¦åˆ†è£‚ï¼‰
            'is_unbalance': True  # è‡ªåŠ¨å¤„ç†ä¸å¹³è¡¡ç±»åˆ«
        }
        
        # âœ… å·®å¼‚åŒ–é…ç½®ï¼šæ ¹æ®æ–°æ•°æ®é‡è°ƒæ•´æ¨¡å‹å¤æ‚åº¦
        self.lgb_params_by_timeframe = {
            '15m': {
                'num_leaves': 110,       # 95â†’110ï¼Œé€‚åº¦å¢åŠ å¤æ‚åº¦
                'min_child_samples': 45,  # 50â†’45ï¼Œç•¥å¾®æ”¾æ¾
                'max_depth': 8,          # 7â†’8ï¼Œå¢åŠ æ·±åº¦
                'reg_alpha': 0.4,        # 0.5â†’0.4ï¼Œç•¥å¾®æ”¾æ¾L1æ­£åˆ™
                'reg_lambda': 0.4        # 0.5â†’0.4ï¼Œç•¥å¾®æ”¾æ¾L2æ­£åˆ™
            },
            '2h': {
                'num_leaves': 63,        # æ•°æ®å¢åŠ (5,184æ¡)ï¼Œå¢åŠ å¤æ‚åº¦
                'min_child_samples': 30,  # æ¯å¶å­çº¦82æ¡æ ·æœ¬
                'max_depth': 9
            },
            '4h': {
                'num_leaves': 47,        # æ•°æ®å¢åŠ (3,456æ¡)ï¼Œå¢åŠ å¤æ‚åº¦
                'min_child_samples': 30,  # æ¯å¶å­çº¦73æ¡æ ·æœ¬
                'max_depth': 8
            }
        }
        
        # GPUé…ç½®
        if settings.USE_GPU:
            self.lgb_params.update({
                'device': 'gpu',
                'gpu_platform_id': 0,
                'gpu_device_id': 0
            })
        
        # æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆæ¯ä¸ªæ—¶é—´æ¡†æ¶ç‹¬ç«‹ï¼‰
        self.model_dir = "models"
        os.makedirs(self.model_dir, exist_ok=True)
    
    def _get_model_paths(self, timeframe: str) -> Dict[str, str]:
        """è·å–æŒ‡å®šæ—¶é—´æ¡†æ¶çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„"""
        return {
            'model': os.path.join(self.model_dir, f"{settings.SYMBOL}_{timeframe}_model.pkl"),
            'scaler': os.path.join(self.model_dir, f"{settings.SYMBOL}_{timeframe}_scaler.pkl"),
            'features': os.path.join(self.model_dir, f"{settings.SYMBOL}_{timeframe}_features.pkl")
        }
    
    async def start(self):
        """å¯åŠ¨æœºå™¨å­¦ä¹ æœåŠ¡"""
        try:
            logger.info("å¯åŠ¨æœºå™¨å­¦ä¹ æœåŠ¡...")
            
            # åŠ è½½å·²æœ‰æ¨¡å‹
            await self._load_model()
            
            # æ³¨æ„ï¼šè®­ç»ƒä»»åŠ¡å·²ç”± scheduler ç»Ÿä¸€ç®¡ç†ï¼ˆæ¯å¤©00:01æ‰§è¡Œï¼‰
            # ä¸å†åœ¨æ­¤å¤„å¯åŠ¨ç‹¬ç«‹çš„è®­ç»ƒå¾ªç¯
            
            self.is_running = True
            logger.info("æœºå™¨å­¦ä¹ æœåŠ¡å¯åŠ¨å®Œæˆï¼ˆè®­ç»ƒç”±schedulerç®¡ç†ï¼‰")
            
        except Exception as e:
            logger.error(f"å¯åŠ¨æœºå™¨å­¦ä¹ æœåŠ¡å¤±è´¥: {e}")
            raise
    
    async def stop(self):
        """åœæ­¢æœºå™¨å­¦ä¹ æœåŠ¡"""
        try:
            logger.info("åœæ­¢æœºå™¨å­¦ä¹ æœåŠ¡...")
            
            self.is_running = False
            
            # å–æ¶ˆè‡ªåŠ¨è®­ç»ƒä»»åŠ¡
            if self.training_task:
                self.training_task.cancel()
                try:
                    await self.training_task
                except asyncio.CancelledError:
                    pass
            
            logger.info("æœºå™¨å­¦ä¹ æœåŠ¡å·²åœæ­¢")
            
        except Exception as e:
            logger.error(f"åœæ­¢æœºå™¨å­¦ä¹ æœåŠ¡å¤±è´¥: {e}")
    
    async def train_model(self, force_retrain: bool = False) -> Dict[str, Any]:
        """è®­ç»ƒæ¨¡å‹ï¼ˆä¸ºæ¯ä¸ªæ—¶é—´æ¡†æ¶è®­ç»ƒç‹¬ç«‹æ¨¡å‹ï¼‰"""
        try:
            logger.info("ğŸš€ å¼€å§‹å¤šæ—¶é—´æ¡†æ¶æ¨¡å‹è®­ç»ƒ...")
            logger.info(f"GPUé…ç½®: USE_GPU={settings.USE_GPU}")
            logger.info(f"æ—¶é—´æ¡†æ¶: {settings.TIMEFRAMES}")
            
            all_metrics = {}
            all_training_data = []  # æ”¶é›†æ‰€æœ‰è®­ç»ƒæ•°æ®
            
            # ä¸ºæ¯ä¸ªæ—¶é—´æ¡†æ¶è®­ç»ƒç‹¬ç«‹æ¨¡å‹
            for timeframe in settings.TIMEFRAMES:
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ“Š è®­ç»ƒ {timeframe} æ—¶é—´æ¡†æ¶æ¨¡å‹...")
                logger.info(f"{'='*60}")
                
                try:
                    # è®­ç»ƒå•ä¸ªæ—¶é—´æ¡†æ¶ï¼ˆè¿”å›metricså’Œtraining_dataï¼‰
                    metrics, training_data = await self._train_single_timeframe(timeframe)
                    all_metrics[timeframe] = metrics
                    all_training_data.append(training_data)
                    logger.info(f"âœ… {timeframe} æ¨¡å‹è®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
                except Exception as e:
                    logger.error(f"âŒ {timeframe} æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    all_metrics[timeframe] = {'success': False, 'error': str(e), 'accuracy': 0.0, 'training_time': 0.0}
            
            # ä¿å­˜æ¨¡å‹ï¼ˆå³ä½¿æœ‰ä¸ªåˆ«æ—¶é—´æ¡†æ¶å¤±è´¥ä¹Ÿä¿å­˜æˆåŠŸçš„ï¼‰
            await self._save_model()
            
            # ğŸ”¥ ç¦ç”¨é¦–æ¬¡è®­ç»ƒæ•°æ®å†™å…¥ï¼ˆèŠ‚çœ2åˆ†é’Ÿï¼‰
            # åŸå› ï¼š
            # 1. æ•°æ®åº“æ•°æ®ä»…ç”¨äºå‰ç«¯å±•ç¤ºï¼Œä¸å½±å“é¢„æµ‹
            # 2. WebSocketç¼“å†²åŒºå·²æœ‰60å¤©æ•°æ®ï¼Œè¶³å¤Ÿé¢„æµ‹
            # 3. å®æ—¶WebSocketæ•°æ®ä¼šæŒç»­å†™å…¥æ•°æ®åº“
            # 4. èŠ‚çœ144ç§’å¯åŠ¨æ—¶é—´ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
            # if self.is_first_training and all_training_data:
            #     try:
            #         await self._save_training_data_to_db(all_training_data)
            #         logger.info("ğŸ’¡ é¦–æ¬¡è®­ç»ƒå®Œæˆï¼Œåç»­è®­ç»ƒå°†ä¸å†å†™å…¥å†å²æ•°æ®")
            #     except Exception as e:
            #         logger.warning(f"ä¿å­˜è®­ç»ƒæ•°æ®åˆ°æ•°æ®åº“å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")
            #     finally:
            #         self.is_first_training = False
            
            logger.info("ğŸ’¡ é¦–æ¬¡è®­ç»ƒå®Œæˆï¼ˆå†å²æ•°æ®å·²ç¦ç”¨å†™å…¥ï¼Œä»…ä¿ç•™å®æ—¶WebSocketæ•°æ®ï¼‰")
            self.is_first_training = False
            
            # æ±‡æ€»æ‰€æœ‰æ¨¡å‹æŒ‡æ ‡
            successful_metrics = [m for m in all_metrics.values() if 'accuracy' in m and not np.isnan(m['accuracy'])]
            avg_accuracy = np.mean([m['accuracy'] for m in successful_metrics]) if successful_metrics else 0.0
            total_training_time = sum([m.get('training_time', 0) for m in all_metrics.values()])
            
            self.model_metrics = {
                'timeframe_metrics': all_metrics,
                'average_accuracy': avg_accuracy,
                'accuracy': avg_accuracy,  # å…¼å®¹æ—§ä»£ç 
                'training_time': total_training_time,
                'version': '2.0',
                'training_date': datetime.now().isoformat()
            }
            
            # ç¼“å­˜æ¨¡å‹æŒ‡æ ‡ï¼ˆä¸è¿‡æœŸï¼‰
            await cache_manager.set_model_metrics(settings.SYMBOL, self.model_metrics, expire=None)
            
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ‰ å¤šæ—¶é—´æ¡†æ¶æ¨¡å‹è®­ç»ƒå®Œæˆ")
            logger.info(f"æˆåŠŸè®­ç»ƒ: {len(successful_metrics)}/{len(settings.TIMEFRAMES)} ä¸ªæ—¶é—´æ¡†æ¶")
            logger.info(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
            logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.2f}ç§’")
            logger.info(f"{'='*60}\n")
            
            return self.model_metrics
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {}
    
    async def _train_single_timeframe(self, timeframe: str) -> tuple:
        """è®­ç»ƒå•ä¸ªæ—¶é—´æ¡†æ¶çš„æ¨¡å‹
        
        Returns:
            tuple: (metrics, training_data_with_timeframe) 
        """
        try:
            import time
            start_time = time.time()
            
            # è·å–è¯¥æ—¶é—´æ¡†æ¶çš„è®­ç»ƒæ•°æ®
            train_data = await self._prepare_training_data_for_timeframe(timeframe)
            
            if train_data.empty:
                raise Exception(f"{timeframe} è®­ç»ƒæ•°æ®ä¸ºç©º")
            
            
            # ä¿å­˜åŸå§‹è®­ç»ƒæ•°æ®ï¼ˆç”¨äºåç»­å†™å…¥æ•°æ®åº“ï¼‰
            train_data_with_timeframe = train_data.copy()
            train_data_with_timeframe['timeframe'] = timeframe
            
            # ç‰¹å¾å·¥ç¨‹
            train_data = feature_engineer.create_features(train_data)
            
            if train_data.empty:
                raise Exception(f"{timeframe} ç‰¹å¾å·¥ç¨‹åæ•°æ®ä¸ºç©º")
            
            # åˆ›å»ºæ ‡ç­¾ï¼ˆä¼ å…¥timeframeä½¿ç”¨å·®å¼‚åŒ–é˜ˆå€¼ï¼‰
            train_data = self._create_labels(train_data, timeframe=timeframe)
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X, y = self._prepare_features_labels(train_data, timeframe)
            
            if len(X) == 0:
                raise Exception(f"{timeframe} ç‰¹å¾æ•°æ®ä¸ºç©º")
            
            # æ•°æ®é¢„å¤„ç†
            X_scaled = self._scale_features(X, timeframe=timeframe, fit=True)
            
            # æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆå‰80%è®­ç»ƒï¼Œå20%éªŒè¯ï¼‰
            split_idx = int(len(X_scaled) * 0.8)
            X_train = X_scaled[:split_idx]
            X_val = X_scaled[split_idx:]
            y_train = y.iloc[:split_idx]
            y_val = y.iloc[split_idx:]
            
            logger.info(f"ğŸ“Š {timeframe} æ—¶é—´åºåˆ—åˆ†å‰²: è®­ç»ƒ{len(X_train)}æ¡, éªŒè¯{len(X_val)}æ¡")
            
            # è®­ç»ƒæ¨¡å‹ï¼ˆä¼ å…¥timeframeä»¥ä½¿ç”¨å·®å¼‚åŒ–å‚æ•°ï¼‰
            model = self._train_lightgbm(X_train, y_train, X_val, y_val, timeframe=timeframe)
            self.models[timeframe] = model
            
            # è¯„ä¼°æ¨¡å‹
            metrics = self._evaluate_model_for_timeframe(X_val, y_val, timeframe)
            
            training_time = time.time() - start_time
            metrics['training_time'] = training_time
            metrics['timeframe'] = timeframe
            
            logger.info(f"â±ï¸ {timeframe} è®­ç»ƒè€—æ—¶: {training_time:.2f}ç§’")
            
            # è¿”å›metricså’Œè®­ç»ƒæ•°æ®
            return metrics, train_data_with_timeframe
            
        except Exception as e:
            logger.error(f"{timeframe} å•æ—¶é—´æ¡†æ¶è®­ç»ƒå¤±è´¥: {e}")
            raise
    
    async def predict(self, data: pd.DataFrame, timeframe: str) -> Dict[str, Any]:
        """æ¨¡å‹é¢„æµ‹ï¼ˆéœ€æŒ‡å®šæ—¶é—´æ¡†æ¶ï¼‰"""
        try:
            
            # æ£€æŸ¥è¯¥æ—¶é—´æ¡†æ¶çš„æ¨¡å‹æ˜¯å¦åŠ è½½
            if timeframe not in self.models or self.models[timeframe] is None:
                logger.warning(f"âš ï¸ {timeframe} æ¨¡å‹æœªåŠ è½½ï¼Œå°è¯•åŠ è½½æ¨¡å‹")
                await self._load_model()
                
                if timeframe not in self.models or self.models[timeframe] is None:
                    raise Exception(f"{timeframe} æ¨¡å‹ä¸å¯ç”¨")
            
            # ç‰¹å¾å·¥ç¨‹
            logger.debug(f"ğŸ“Š {timeframe} ç‰¹å¾å·¥ç¨‹...")
            processed_data = feature_engineer.create_features(data.copy())
            
            if processed_data.empty:
                raise Exception("ç‰¹å¾å·¥ç¨‹åæ•°æ®ä¸ºç©º")
            
            
            # è·å–æœ€æ–°ä¸€è¡Œæ•°æ®ï¼ˆä½¿ç”¨è¯¥æ—¶é—´æ¡†æ¶çš„ç‰¹å¾åˆ—ï¼‰
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            if not feature_columns:
                raise Exception(f"{timeframe} ç‰¹å¾åˆ—æœªæ‰¾åˆ°")
            
            latest_data = processed_data.iloc[-1:][feature_columns]
            
            # âœ… è°ƒè¯•æ—¥å¿—ï¼šéªŒè¯è¾“å…¥æ•°æ®
            if 'close' in processed_data.columns:
                last_3_closes = processed_data['close'].tail(3).tolist()
            
            
            # âœ… è®°å½•å…³é”®ç‰¹å¾å€¼ï¼ˆç”¨äºè¯Šæ–­ï¼‰
            if 'price_change' in latest_data.columns:
                price_change = latest_data['price_change'].iloc[0]
            
            # æ•°æ®é¢„å¤„ç†ï¼ˆä½¿ç”¨è¯¥æ—¶é—´æ¡†æ¶çš„scalerï¼‰
            X_scaled = self._scale_features(latest_data, timeframe=timeframe, fit=False)
            
            # é¢„æµ‹ï¼ˆä½¿ç”¨è¯¥æ—¶é—´æ¡†æ¶çš„æ¨¡å‹ï¼‰
            model = self.models[timeframe]
            probabilities = model.predict_proba(X_scaled)[0]
            prediction = np.argmax(probabilities)
            confidence = np.max(probabilities)
            
            # è½¬æ¢é¢„æµ‹ç»“æœ
            signal_map = {0: 'SHORT', 1: 'HOLD', 2: 'LONG'}
            signal_type = signal_map[prediction]
            
            # ç®€æ´è®°å½•é¢„æµ‹ç»“æœï¼ˆä½¿ç”¨å›¾æ ‡+ä¸­æ–‡ï¼‰
            from app.utils.helpers import format_signal_type
            logger.info(f"ğŸ¯ {timeframe} é¢„æµ‹: {format_signal_type(signal_type)} (ç½®ä¿¡åº¦={confidence:.4f}, æ¦‚ç‡: ğŸ“‰{probabilities[0]:.2f} â¸ï¸{probabilities[1]:.2f} ğŸ“ˆ{probabilities[2]:.2f})")
            
            result = {
                'signal_type': signal_type,
                'confidence': float(confidence),
                'probabilities': {
                    'short': float(probabilities[0]),
                    'hold': float(probabilities[1]),
                    'long': float(probabilities[2])
                },
                'timestamp': datetime.now(),
                'model_version': self.model_metrics.get('version', '1.0')
            }
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ {timeframe} æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}", exc_info=True)
            return {}
    
    async def _prepare_training_data_for_timeframe(self, timeframe: str) -> pd.DataFrame:
        """ä¸ºå•ä¸ªæ—¶é—´æ¡†æ¶å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå·®å¼‚åŒ–è®­ç»ƒå¤©æ•°ï¼‰"""
        try:
            from app.services.binance_client import binance_client
            
            symbol = settings.SYMBOL
            
            # å·®å¼‚åŒ–è®­ç»ƒå¤©æ•°ï¼šå¹³è¡¡æ•°æ®é‡ä¸æ—¶æ•ˆæ€§
            # åŸåˆ™ï¼šæ—¶é—´æ¡†æ¶è¶Šé•¿ï¼Œæ•°æ®è¡°å‡è¶Šå¿«ï¼Œä¸å®œç”¨å¤ªå¤šå†å²æ•°æ®
            training_days_config = {
                '15m': 360,  # çŸ­æœŸï¼š360å¤©ï¼ˆ34,560æ¡ï¼‰â† çŸ­æœŸæ¨¡å¼ç¨³å®šï¼Œå¯ä»¥ç”¨æ›´å¤šæ•°æ®
                '2h': 270,   # ä¸­æœŸï¼š270å¤©ï¼ˆ3,240æ¡ï¼‰â† é€‚åº¦å‡å°‘ï¼Œå¹³è¡¡æ—¶æ•ˆæ€§
                '4h': 360    # é•¿æœŸï¼š360å¤©ï¼ˆ2,160æ¡ï¼‰â† å¤§å¹…å‡å°‘ï¼Œé¿å…è¿‡æ—¶æ•°æ®
            }
            training_days = training_days_config.get(timeframe, 360)
            
            # æ—¶é—´å‘¨æœŸå¯¹åº”çš„åˆ†é’Ÿæ•°
            interval_minutes = {
                '1m': 1, '3m': 3, '5m': 5, '15m': 15, '30m': 30,
                '1h': 60, '2h': 120, '4h': 240, '6h': 360, '8h': 480,
                '12h': 720, '1d': 1440
            }
            
            # æ ¹æ®æ—¶é—´å‘¨æœŸè®¡ç®—éœ€è¦çš„Kçº¿æ•°é‡
            minutes = interval_minutes.get(timeframe, 60)
            required_klines = int((training_days * 24 * 60) / minutes)
            
            logger.info(f"ğŸ“¥ è·å– {timeframe} æ•°æ®: {required_klines}æ¡Kçº¿ ({training_days}å¤©)")
            
            # åˆ†æ‰¹è·å–æ•°æ®ï¼ˆæ¯æ¬¡æœ€å¤š1500æ¡ï¼‰
            all_klines = []
            max_per_request = 1500
            batches_needed = (required_klines + max_per_request - 1) // max_per_request
            
            end_time = None  # æœ€æ–°æ•°æ®
            
            for batch in range(batches_needed):
                batch_limit = min(max_per_request, required_klines - len(all_klines))
                
                if batch_limit <= 0:
                    break
                
                klines = binance_client.get_klines(
                    symbol=symbol,
                    interval=timeframe,
                    limit=batch_limit,
                    end_time=end_time
                )
                
                if klines:
                    all_klines.extend(klines)
                    # è®¾ç½®ä¸‹ä¸€æ‰¹æ¬¡çš„end_timeä¸ºå½“å‰æ‰¹æ¬¡æœ€æ—©çš„æ—¶é—´ - 1ms
                    end_time = klines[0]['timestamp'] - 1
                    await asyncio.sleep(0.1)  # é¿å…APIé™æµ
                else:
                    break
            
            # è½¬æ¢ä¸ºDataFrameï¼ˆä¸ä¾èµ–reverseï¼Œç›´æ¥ç”¨æ—¶é—´æˆ³æ’åºï¼‰
            df = pd.DataFrame(all_klines)
            
            if not df.empty:
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                
                # ğŸ”‘ å…³é”®ï¼šä¾èµ–æ—¶é—´æˆ³æ’åºï¼Œè€Œä¸æ˜¯å‡è®¾APIè¿”å›é¡ºåº
                df = df.sort_values('timestamp', ascending=True)  # æ˜ç¡®æŒ‡å®šå‡åºï¼ˆæ—§â†’æ–°ï¼‰
                
                # âœ… å»é‡ï¼ˆé˜²æ­¢æ‰¹æ¬¡è¾¹ç•Œé‡å¤ï¼‰
                df = df.drop_duplicates(subset=['timestamp'], keep='last')
                
                # è®¾ç½®ç´¢å¼•
                df = df.set_index('timestamp')
                
                logger.info(f"âœ… {timeframe} æ•°æ®è·å–æˆåŠŸ: {len(df)}æ¡")
            else:
                logger.warning(f"âš ï¸ {timeframe} æ•°æ®ä¸ºç©º")
            
            return df
            
        except Exception as e:
            logger.error(f"ä¸º{timeframe}å‡†å¤‡è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    
    async def _save_training_data_to_db(self, all_data: list):
        """å°†è®­ç»ƒæ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä¾›å‰ç«¯å±•ç¤ºï¼‰
        
        ä¼˜åŒ–ï¼šåˆå¹¶æ‰€æœ‰æ—¶é—´æ¡†æ¶ï¼Œä¸€æ¬¡æ€§æ‰¹é‡å†™å…¥ï¼Œå‡å°‘æ•°æ®åº“è¿æ¥å‹åŠ›
        
        Args:
            all_data: List of DataFrames with 'timeframe' column
        """
        try:
            logger.info("ğŸ“¥ å¼€å§‹å°†è®­ç»ƒæ•°æ®å†™å…¥æ•°æ®åº“...")
            
            # ğŸ”¥ ä¼˜åŒ–ï¼šå…ˆæ”¶é›†æ‰€æœ‰æ•°æ®ï¼Œç„¶åä¸€æ¬¡æ€§å†™å…¥
            all_klines = []
            
            for df in all_data:
                if df is None or df.empty:
                    logger.warning("è·³è¿‡ç©ºDataFrame")
                    continue
                
                # æ£€æŸ¥timeframeåˆ—æ˜¯å¦å­˜åœ¨
                if 'timeframe' not in df.columns:
                    logger.warning(f"DataFrameç¼ºå°‘timeframeåˆ—ï¼Œè·³è¿‡: {df.columns.tolist()}")
                    continue
                
                # è·å–æ—¶é—´æ¡†æ¶
                timeframe = df['timeframe'].iloc[0]
                logger.info(f"  å¤„ç† {timeframe} æ•°æ®...")
                
                # ç§»é™¤timeframeåˆ—ï¼Œå‡†å¤‡å†™å…¥
                df_to_save = df.drop('timeframe', axis=1).copy()
                
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
                for idx, row in df_to_save.iterrows():
                    try:
                        kline = {
                            'symbol': settings.SYMBOL,
                            'interval': timeframe,
                            'timestamp': int(idx.timestamp() * 1000),
                            'open': float(row['open']),
                            'high': float(row['high']),
                            'low': float(row['low']),
                            'close': float(row['close']),
                            'volume': float(row['volume']),
                            'close_time': int(idx.timestamp() * 1000),
                            'quote_volume': float(row.get('quote_volume', 0)),
                            'trades': int(row.get('trades', 0)),
                            'taker_buy_base_volume': float(row.get('taker_buy_base_volume', 0)),
                            'taker_buy_quote_volume': float(row.get('taker_buy_quote_volume', 0))
                        }
                        all_klines.append(kline)
                    except Exception as e:
                        logger.warning(f"è·³è¿‡æ— æ•ˆè¡Œ: {e}")
                        continue
                
                logger.info(f"  âœ“ {timeframe} å‡†å¤‡å®Œæˆ: {len(df_to_save)}æ¡")
            
            # ğŸ”¥ ä¸€æ¬¡æ€§å†™å…¥æ‰€æœ‰æ•°æ®ï¼ˆå‡å°‘è¿æ¥æ± å‹åŠ›ï¼‰
            if all_klines:
                logger.info(f"ğŸ“Š å¼€å§‹ä¸€æ¬¡æ€§å†™å…¥æ‰€æœ‰æ•°æ®: {len(all_klines)}æ¡...")
                try:
                    await postgresql_manager.write_kline_data(all_klines)
                    logger.info(f"âœ… è®­ç»ƒæ•°æ®å†™å…¥æ•°æ®åº“å®Œæˆ: æ€»è®¡{len(all_klines)}æ¡")
                except Exception as e:
                    logger.error(f"  âœ— æ•°æ®åº“å†™å…¥å¤±è´¥: {e}")
                    raise
            else:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å†™å…¥")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæ•°æ®å†™å…¥æ•°æ®åº“å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            raise
    
    def _create_labels(self, df: pd.DataFrame, timeframe: str = None) -> pd.DataFrame:
        """åˆ›å»ºæ ‡ç­¾ï¼ˆåŠ¨æ€é˜ˆå€¼ï¼ŒåŸºäºATRæ³¢åŠ¨ç‡è‡ªé€‚åº”è°ƒæ•´ï¼‰
        
        Args:
            df: Kçº¿æ•°æ®
            timeframe: æ—¶é—´æ¡†æ¶ï¼ˆç”¨äºå·®å¼‚åŒ–é˜ˆå€¼é…ç½®ï¼‰
        """
        try:
            # âœ… ä¿®å¤ï¼šåªçœ‹ä¸‹ä¸€æ ¹Kçº¿ï¼ˆä¸æ˜¯æœªæ¥5æ ¹ï¼‰
            df['next_return'] = df['close'].shift(-1) / df['close'] - 1
            
            # ğŸ¯ åŠ¨æ€é˜ˆå€¼ï¼šåŸºäºATRæ³¢åŠ¨ç‡è‡ªé€‚åº”è°ƒæ•´
            # è®¡ç®—ATRï¼ˆ14å‘¨æœŸï¼‰
            if 'atr' not in df.columns or df['atr'].isna().all():
                # å¦‚æœæ²¡æœ‰ATRç‰¹å¾ï¼Œæ‰‹åŠ¨è®¡ç®—
                high_low = df['high'] - df['low']
                high_close = np.abs(df['high'] - df['close'].shift())
                low_close = np.abs(df['low'] - df['close'].shift())
                true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
                atr = true_range.rolling(window=14).mean()
            else:
                atr = df['atr']
            
            # ATRç™¾åˆ†æ¯”ï¼ˆç›¸å¯¹äºä»·æ ¼ï¼‰
            atr_pct = (atr / df['close']).rolling(window=50).mean()  # 50æ ¹Kçº¿å¹³å‡
            
            # åŸºç¡€é˜ˆå€¼é…ç½®ï¼ˆä¸­é¢‘äº¤æ˜“ï¼šçµæ•ç­–ç•¥ï¼‰
            base_threshold_config = {
                '15m': 0.001,   # åŸºç¡€Â±0.1% âœ… ä¸­é¢‘äº¤æ˜“æåº¦çµæ•
                '2h': 0.0035,   # åŸºç¡€Â±0.35%
                '4h': 0.005     # åŸºç¡€Â±0.5%
            }
            
            base_threshold = base_threshold_config.get(timeframe, 0.010)
            
            # åŠ¨æ€è°ƒæ•´ç³»æ•°ï¼ˆåŸºäºATRæ³¢åŠ¨ç‡ï¼‰
            # å¦‚æœæ³¢åŠ¨ç‡é«˜ â†’ æ‰©å¤§é˜ˆå€¼ï¼›æ³¢åŠ¨ç‡ä½ â†’ ç¼©å°é˜ˆå€¼
            median_atr_pct = atr_pct.median()
            
            if pd.isna(median_atr_pct) or median_atr_pct == 0:
                # é™çº§ä¸ºå›ºå®šé˜ˆå€¼
                up_threshold = base_threshold
                down_threshold = -base_threshold
                logger.info(f"âš ï¸ ATRè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å›ºå®šé˜ˆå€¼: Â±{base_threshold*100:.2f}%")
            else:
                # åŠ¨æ€è°ƒæ•´ï¼šATRé«˜æ—¶æ”¾å®½é˜ˆå€¼ï¼ŒATRä½æ—¶æ”¶ç´§é˜ˆå€¼
                # è°ƒæ•´èŒƒå›´ï¼š0.7x ~ 1.3x
                adjustment = np.clip(median_atr_pct / 0.005, 0.7, 1.3)  # 0.5%ä¸ºåŸºå‡†
                
                up_threshold = base_threshold * adjustment
                down_threshold = -base_threshold * adjustment
                
                logger.info(f"ğŸ¯ {timeframe} åŠ¨æ€é˜ˆå€¼: Â±{up_threshold*100:.2f}% "
                          f"(åŸºç¡€={base_threshold*100:.2f}%, ATRè°ƒæ•´={adjustment:.2f}x, "
                          f"ATR%={median_atr_pct*100:.3f}%)")
            
            # åˆ›å»ºåˆ†ç±»æ ‡ç­¾
            conditions = [
                df['next_return'] <= down_threshold,  # ä¸‹è·Œ â†’ SHORT
                (df['next_return'] > down_threshold) & (df['next_return'] < up_threshold),  # æ¨ªç›˜ â†’ HOLD
                df['next_return'] >= up_threshold     # ä¸Šæ¶¨ â†’ LONG
            ]
            
            choices = [0, 1, 2]  # 0: SHORT, 1: HOLD, 2: LONG
            df['label'] = np.select(conditions, choices, default=1)
            
            # ç§»é™¤æœ€å1è¡Œï¼ˆæ²¡æœ‰next_returnï¼‰
            df = df[:-1]
            
            # âœ… è¯¦ç»†çš„æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡ï¼ˆç”Ÿäº§ç¯å¢ƒå¿…é¡»ç›‘æ§ï¼‰
            label_counts = df['label'].value_counts().sort_index()
            total = len(df)
            
            short_count = label_counts.get(0, 0)
            hold_count = label_counts.get(1, 0)
            long_count = label_counts.get(2, 0)
            
            logger.info(f"ğŸ“Š {timeframe} æ ‡ç­¾åˆ†å¸ƒï¼ˆé˜ˆå€¼: Â±{up_threshold*100:.2f}%ï¼‰:")  # æ”¹ä¸º.2fç²¾ç¡®æ˜¾ç¤º
            logger.info(f"  SHORT (0): {short_count:5d}æ¡ ({short_count/total*100:5.1f}%)")
            logger.info(f"  HOLD  (1): {hold_count:5d}æ¡ ({hold_count/total*100:5.1f}%)")
            logger.info(f"  LONG  (2): {long_count:5d}æ¡ ({long_count/total*100:5.1f}%)")
            
            # è­¦å‘Šï¼šç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡
            if hold_count / total > 0.60:
                logger.warning(f"âš ï¸ {timeframe} HOLDç±»åˆ«å æ¯”è¿‡é«˜ ({hold_count/total*100:.1f}%)ï¼Œå»ºè®®æé«˜é˜ˆå€¼")
            
            if short_count / total < 0.20 or long_count / total < 0.20:
                logger.warning(f"âš ï¸ {timeframe} LONG/SHORTç±»åˆ«è¿‡å°‘ï¼Œå»ºè®®é™ä½é˜ˆå€¼")
            
            
            return df
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ ‡ç­¾å¤±è´¥: {e}")
            return df
    
    def _prepare_features_labels(self, df: pd.DataFrame, timeframe: str) -> Tuple[pd.DataFrame, pd.Series]:
        """å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå¤šæ—¶é—´æ¡†æ¶ç‹¬ç«‹ç‰¹å¾ï¼‰
        
        Args:
            df: åŒ…å«labelåˆ—çš„DataFrame
            timeframe: æ—¶é—´æ¡†æ¶ï¼ˆå¿…éœ€ï¼‰
            
        Returns:
            (X, y): ç‰¹å¾DataFrameå’Œæ ‡ç­¾Series
        """
        try:
            # æ’é™¤éç‰¹å¾åˆ—
            exclude_cols = [
                'timestamp', 'datetime', 'open', 'high', 'low', 'close', 
                'volume', 'quote_volume', 'label', 'next_return'
            ]
            
            feature_cols = [col for col in df.columns if col not in exclude_cols]
            
            # ğŸ”‘ å…ˆæå–æ ‡ç­¾ï¼ˆç‰¹å¾é€‰æ‹©éœ€è¦ç”¨åˆ°ï¼‰
            y = df['label'].copy()
            
            # ä¸ºæ¯ä¸ªæ—¶é—´æ¡†æ¶é€‰æ‹©ç‹¬ç«‹çš„é‡è¦ç‰¹å¾ï¼ˆåŸºäºæ¨¡å‹çš„ä¸¤é˜¶æ®µé€‰æ‹©ï¼‰
            if timeframe not in self.feature_columns_dict or not self.feature_columns_dict[timeframe]:
                # ğŸ†• æ™ºèƒ½ç‰¹å¾é€‰æ‹©ï¼šåŸºäºLightGBMé‡è¦æ€§çš„ä¸¤é˜¶æ®µé€‰æ‹©
                selected_features = self._select_features_intelligent(
                    df[feature_cols], 
                    y, 
                    timeframe
                )
                self.feature_columns_dict[timeframe] = selected_features
                logger.info(f"âœ… {timeframe} ç‰¹å¾é€‰æ‹©å®Œæˆ: {len(selected_features)}/{len(feature_cols)} ä¸ªç‰¹å¾")
            
            feature_columns = self.feature_columns_dict[timeframe]
            
            X = df[feature_columns].copy()
            
            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            mask = ~(X.isna().any(axis=1) | y.isna())
            X = X[mask]
            y = y[mask]
            
            logger.info(f"ç‰¹å¾æ•°é‡: {len(feature_columns)}, æ ·æœ¬æ•°é‡: {len(X)}")
            
            return X, y
            
        except Exception as e:
            logger.error(f"å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾å¤±è´¥: {e}")
            return pd.DataFrame(), pd.Series()
    
    def _select_features_intelligent(
        self, 
        X: pd.DataFrame, 
        y: pd.Series, 
        timeframe: str
    ) -> list:
        """
        æ™ºèƒ½ç‰¹å¾é€‰æ‹©ï¼ˆä¸¤é˜¶æ®µ + åŠ¨æ€é¢„ç®—ï¼‰
        
        é˜¶æ®µ1: Filterè¿‡æ»¤é›¶å¢ç›Šç‰¹å¾
        é˜¶æ®µ2: åµŒå…¥å¼é€‰æ‹© + åŠ¨æ€é¢„ç®—
        
        Args:
            X: ç‰¹å¾DataFrame
            y: æ ‡ç­¾Series
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            é€‰ä¸­çš„ç‰¹å¾åˆ—è¡¨
        """
        try:
            from sklearn.feature_selection import SelectFromModel
            
            n_samples = len(X)
            n_feats = len(X.columns)
            ratio = n_samples / n_feats if n_feats > 0 else 0
            
            # 1. åŠ¨æ€é¢„ç®—è®¡ç®—ï¼ˆæ ¹æ®æ ·æœ¬/ç‰¹å¾æ¯”ï¼‰
            # ä¸åŒæ—¶é—´æ¡†æ¶çš„æœ€å°‘æ ·æœ¬æ•°/ç‰¹å¾ç³»æ•°
            # ğŸ¯ è°ƒæ•´ç­–ç•¥ï¼šå…è®¸æ›´å¤šç‰¹å¾ä»¥è¾¾åˆ°50%å‡†ç¡®ç‡ç›®æ ‡
            ratio_map = {
                '15m': 120,  # 150â†’120ï¼Œå…è®¸æ›´å¤šç‰¹å¾ï¼ˆ34360/120=286ä¸ªï¼Œå–150ï¼‰
                '2h': 80,    # ä»150é™ä½â†’å…è®¸æ›´å¤šç‰¹å¾ï¼ˆ3040/80=38ä¸ªï¼‰
                '4h': 50     # ä»100é™ä½â†’å…è®¸æ›´å¤šç‰¹å¾ï¼ˆ1960/50=39ä¸ªï¼‰
            }
            k = ratio_map.get(timeframe, 100)
            # ğŸ†• Kimå»ºè®®2: ä¿åº•8ä¸ªç‰¹å¾ï¼Œå°é¡¶150
            budget = max(8, min(int(n_samples / k), 150))
            
            logger.info(f"ğŸ“Š {timeframe} æ ·æœ¬/ç‰¹å¾æ¯”={ratio:.1f}, åŠ¨æ€é¢„ç®—={budget}ä¸ªç‰¹å¾")
            
            # 2. é˜¶æ®µâ‘ ï¼šFilterè¿‡æ»¤é›¶å¢ç›Šç‰¹å¾
            logger.info(f"ğŸ” é˜¶æ®µ1: Filteré›¶å¢ç›Šç‰¹å¾...")
            lgb_filter = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                random_state=42,
                n_jobs=-1,
                verbose=-1  # é™é»˜æ¨¡å¼
            )
            lgb_filter.fit(X, y)
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            imp = lgb_filter.feature_importances_
            # ğŸ†• Kimå»ºè®®1: ä½¿ç”¨å‡å€¼é˜ˆå€¼ï¼Œè¿‡æ»¤å™ªéŸ³ç‰¹å¾ï¼ˆå¦‚1e-6ï¼‰
            imp_threshold = imp.mean() * 0.1  # å‡å€¼çš„10%
            stage1_mask = imp > imp_threshold
            stage1_cols = X.columns[stage1_mask].tolist()
            
            filtered_count = n_feats - len(stage1_cols)
            logger.info(f"âœ… è¿‡æ»¤äº†{filtered_count}ä¸ªä½é‡è¦æ€§ç‰¹å¾(<{imp_threshold:.6f}), å‰©ä½™{len(stage1_cols)}ä¸ª")
            
            # ğŸ†• Kimå»ºè®®4: é‡Šæ”¾å†…å­˜
            import gc
            del lgb_filter
            gc.collect()
            
            # å¦‚æœè¿‡æ»¤åç‰¹å¾æ•°å·²ç»<=é¢„ç®—ï¼Œç›´æ¥è¿”å›
            if len(stage1_cols) <= budget:
                logger.info(f"âœ… {timeframe} ç‰¹å¾æ•°å·²æ»¡è¶³é¢„ç®—ï¼Œè·³è¿‡é˜¶æ®µ2")
                return stage1_cols
            
            # 3. é˜¶æ®µâ‘¡ï¼šåµŒå…¥å¼é€‰æ‹©ï¼ˆåŸºäºé¢„ç®—ï¼‰
            logger.info(f"ğŸ” é˜¶æ®µ2: åµŒå…¥å¼é€‰æ‹©Top {budget}...")
            selector = SelectFromModel(
                lgb.LGBMClassifier(
                    n_estimators=300,
                    learning_rate=0.05,
                    reg_alpha=0.1,
                    reg_lambda=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    random_state=42,
                    n_jobs=-1,
                    verbose=-1
                ),
                max_features=budget,
                threshold=-np.inf,  # åªå—max_featuresé™åˆ¶
                importance_getter='auto',
                prefit=False  # ğŸ†• Kimå»ºè®®3: æ˜¾å¼å£°æ˜ï¼Œä¿æŒå¯æ§
            )
            
            selector.fit(X[stage1_cols], y)
            stage2_mask = selector.get_support()
            selected_cols = [stage1_cols[i] for i, selected in enumerate(stage2_mask) if selected]
            
            # ğŸ†• Kimå»ºè®®4: é‡Šæ”¾å†…å­˜
            del selector
            gc.collect()
            
            # è®¡ç®—æœ€ç»ˆæ ·æœ¬/ç‰¹å¾æ¯”
            final_ratio = n_samples / len(selected_cols) if len(selected_cols) > 0 else 0
            
            logger.info(f"âœ… {timeframe} ä¸¤é˜¶æ®µç‰¹å¾é€‰æ‹©å®Œæˆ:")
            logger.info(f"   åŸå§‹: {n_feats}ä¸ª â†’ Filter: {len(stage1_cols)}ä¸ª â†’ æœ€ç»ˆ: {len(selected_cols)}ä¸ª")
            logger.info(f"   æ ·æœ¬æ•°: {n_samples}, æ ·æœ¬/ç‰¹å¾æ¯”: {final_ratio:.1f}")
            
            # ğŸ†• Kimå»ºè®®5: è¿‡æ‹Ÿåˆè­¦æˆ’çº¿
            if final_ratio < 50:
                logger.warning(f"âš ï¸ {timeframe} æ ·æœ¬/ç‰¹å¾æ¯” <50ï¼Œå»ºè®®è°ƒå¤§kå€¼æˆ–å¢åŠ å¤–éƒ¨æ­£åˆ™åŒ–")
            elif final_ratio < 100:
                logger.warning(f"âš ï¸ {timeframe} æ ·æœ¬/ç‰¹å¾æ¯” <100ï¼Œæ³¨æ„ç›‘æ§è¿‡æ‹Ÿåˆ")
            
            return selected_cols
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•çš„top_né€‰æ‹©
            logger.warning(f"âš ï¸ é™çº§åˆ°ç®€å•ç‰¹å¾é€‰æ‹©...")
            top_n = {'15m': 100, '2h': 80, '4h': 60}.get(timeframe, 80)
            feature_importance = feature_engineer.get_feature_importance(X)
            selected = list(feature_importance.keys())[:top_n]
            return selected
    
    def _scale_features(self, X: pd.DataFrame, timeframe: str, fit: bool = False) -> np.ndarray:
        """ç‰¹å¾ç¼©æ”¾ï¼ˆå¤šæ—¶é—´æ¡†æ¶ç‹¬ç«‹Scalerï¼‰
        
        Args:
            X: ç‰¹å¾DataFrame
            timeframe: æ—¶é—´æ¡†æ¶ï¼ˆå¿…éœ€ï¼‰
            fit: æ˜¯å¦æ‹Ÿåˆæ–°çš„scaler
            
        Returns:
            ç¼©æ”¾åçš„ç‰¹å¾æ•°ç»„
        """
        try:
            # æ¯ä¸ªæ—¶é—´æ¡†æ¶ç‹¬ç«‹çš„scaler
            if fit or timeframe not in self.scalers or self.scalers[timeframe] is None:
                self.scalers[timeframe] = StandardScaler()
                X_scaled = self.scalers[timeframe].fit_transform(X)
            else:
                X_scaled = self.scalers[timeframe].transform(X)
            
            return X_scaled
            
        except Exception as e:
            logger.error(f"ç‰¹å¾ç¼©æ”¾å¤±è´¥: {e}")
            return X.values
    
    # æ³¨ï¼š_train_lightgbm() æ–¹æ³•å·²ç§»è‡³ ensemble_ml_service.pyï¼ˆç»Ÿä¸€ä¸‰æ¨¡å‹è®­ç»ƒä»£ç ä½ç½®ï¼‰
    # åŸå®ç°å·²è¢«å­ç±»è¦†ç›–ï¼Œæ­¤å¤„åˆ é™¤ä»¥é¿å…ä»£ç å†—ä½™
    
    def _evaluate_model_for_timeframe(self, X_val: np.ndarray, y_val: np.ndarray, timeframe: str) -> Dict[str, Any]:
        """è¯„ä¼°ç‰¹å®šæ—¶é—´æ¡†æ¶çš„æ¨¡å‹"""
        try:
            model = self.models.get(timeframe)
            if not model:
                raise Exception(f"{timeframe} æ¨¡å‹ä¸å­˜åœ¨")
            
            # é¢„æµ‹
            y_pred = model.predict(X_val)
            y_pred_proba = model.predict_proba(X_val)
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)
            
            # å¤šåˆ†ç±»AUC
            try:
                auc = roc_auc_score(y_val, y_pred_proba, multi_class='ovr')
            except:
                auc = 0.0
            
            # ç‰¹å¾é‡è¦æ€§
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            feature_importance = dict(zip(
                feature_columns, 
                model.feature_importances_
            ))
            
            # æŒ‰é‡è¦æ€§æ’åºï¼Œå¹¶è½¬æ¢ä¸ºPythonåŸç”Ÿfloat
            feature_importance = {
                k: float(v) for k, v in sorted(
                    feature_importance.items(), 
                    key=lambda x: x[1], 
                    reverse=True
                )
            }
            
            # âœ… è½¬æ¢æ‰€æœ‰numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼ˆé˜²æ­¢JSONåºåˆ—åŒ–é”™è¯¯ï¼‰
            metrics = {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1),
                'auc': float(auc),
                'feature_importance': feature_importance,
                'timeframe': timeframe,
                'training_time': datetime.now().isoformat(),
                'version': '2.0'  # å¤šæ—¶é—´æ¡†æ¶ç‰ˆæœ¬
            }
            
            logger.info(f"ğŸ“Š {timeframe} æ¨¡å‹è¯„ä¼°:")
            logger.info(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
            logger.info(f"  ç²¾ç¡®ç‡: {precision:.4f}")
            logger.info(f"  å¬å›ç‡: {recall:.4f}")
            logger.info(f"  F1åˆ†æ•°: {f1:.4f}")
            logger.info(f"  AUC: {auc:.4f}")
            
            return metrics
            
        except Exception as e:
            logger.error(f"{timeframe} æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return {}
    
    async def _save_model(self):
        """ä¿å­˜å¤šæ—¶é—´æ¡†æ¶æ¨¡å‹"""
        try:
            saved_count = 0
            for timeframe in settings.TIMEFRAMES:
                if timeframe in self.models:
                    paths = self._get_model_paths(timeframe)
                    
                    # ä¿å­˜æ¨¡å‹
                    joblib.dump(self.models[timeframe], paths['model'])
                    
                    # ä¿å­˜ç¼©æ”¾å™¨
                    if timeframe in self.scalers:
                        joblib.dump(self.scalers[timeframe], paths['scaler'])
                    
                    # ä¿å­˜ç‰¹å¾åˆ—
                    if timeframe in self.feature_columns_dict:
                        with open(paths['features'], 'wb') as f:
                            pickle.dump(self.feature_columns_dict[timeframe], f)
                    
                    saved_count += 1
                    logger.info(f"âœ… {timeframe} æ¨¡å‹ä¿å­˜å®Œæˆ")
            
            logger.info(f"ğŸ‰ æ‰€æœ‰æ¨¡å‹ä¿å­˜å®Œæˆ ({saved_count}ä¸ªæ—¶é—´æ¡†æ¶)")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
    
    async def _load_model(self):
        """åŠ è½½å¤šæ—¶é—´æ¡†æ¶æ¨¡å‹"""
        try:
            loaded_count = 0
            for timeframe in settings.TIMEFRAMES:
                paths = self._get_model_paths(timeframe)
                
                try:
                    # åŠ è½½æ¨¡å‹
                    if os.path.exists(paths['model']):
                        self.models[timeframe] = joblib.load(paths['model'])
                        loaded_count += 1
                    
                    # åŠ è½½ç¼©æ”¾å™¨
                    if os.path.exists(paths['scaler']):
                        self.scalers[timeframe] = joblib.load(paths['scaler'])
                    
                    # åŠ è½½ç‰¹å¾åˆ—
                    if os.path.exists(paths['features']):
                        with open(paths['features'], 'rb') as f:
                            self.feature_columns_dict[timeframe] = pickle.load(f)
                    
                    if timeframe in self.models:
                        feature_count = len(self.feature_columns_dict.get(timeframe, []))
                
                except Exception as e:
                    logger.warning(f"âš ï¸ {timeframe} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            
            if loaded_count > 0:
                logger.info(f"ğŸ‰ æ¨¡å‹åŠ è½½å®Œæˆ ({loaded_count}/{len(settings.TIMEFRAMES)}ä¸ªæ—¶é—´æ¡†æ¶)")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œéœ€è¦è®­ç»ƒ")
            
            # åŠ è½½æ¨¡å‹æŒ‡æ ‡
            cached_metrics = await cache_manager.get_model_metrics(settings.SYMBOL)
            if cached_metrics:
                self.model_metrics = cached_metrics
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    
    # æ³¨æ„ï¼šè‡ªåŠ¨è®­ç»ƒå¾ªç¯å·²ç§»é™¤ï¼Œæ”¹ç”± scheduler ç»Ÿä¸€ç®¡ç†
    # scheduler ä¼šåœ¨æ¯å¤©00:01è°ƒç”¨ train_model() æ–¹æ³•
    
    async def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯ï¼ˆå¤šæ—¶é—´æ¡†æ¶ç‰ˆæœ¬ï¼‰"""
        try:
            # ç»Ÿè®¡å·²åŠ è½½çš„æ¨¡å‹
            loaded_models = {}
            total_features = 0
            
            for timeframe in settings.TIMEFRAMES:
                is_loaded = timeframe in self.models and self.models[timeframe] is not None
                feature_count = len(self.feature_columns_dict.get(timeframe, []))
                
                loaded_models[timeframe] = {
                    'loaded': is_loaded,
                    'feature_count': feature_count,
                    'model_path': self._get_model_paths(timeframe)['model'] if is_loaded else None
                }
                
                if is_loaded:
                    total_features += feature_count
            
            info = {
                'models_loaded': loaded_models,
                'total_models': len(self.models),
                'expected_models': len(settings.TIMEFRAMES),
                'total_features': total_features,
                'metrics': self.model_metrics,
                'last_training': self.model_metrics.get('training_date', 'Unknown'),
                'version': self.model_metrics.get('version', '2.0')
            }
            
            return info
            
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'total_models': 0,
                'expected_models': len(settings.TIMEFRAMES),
                'error': str(e)
            }