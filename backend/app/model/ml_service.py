"""
æœºå™¨å­¦ä¹ æœåŠ¡
"""
import asyncio
import logging
import pickle
import os
import gc
import time
import traceback
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.feature_selection import SelectFromModel
import lightgbm as lgb
import joblib

from app.core.config import settings
from app.core.database import postgresql_manager
from app.core.cache import cache_manager
from app.model.feature_engineering import feature_engineer
from app.services.data_service import DataService
from app.utils.helpers import format_signal_type
from app.exchange.exchange_factory import ExchangeFactory

logger = logging.getLogger(__name__)

class MLService:
    """æœºå™¨å­¦ä¹ æœåŠ¡ï¼ˆæ”¯æŒå¤šæ—¶é—´æ¡†æ¶ç‹¬ç«‹æ¨¡å‹ï¼‰"""
    
    def __init__(self):
        self.is_running = False
        # å¤šæ—¶é—´æ¡†æ¶æ¨¡å‹ï¼š{'3m': model, '5m': model, '15m': model}
        self.models = {}
        self.scalers = {}
        self.feature_columns_dict = {}
        
        # ğŸ”‘ åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨ï¼ˆä¿®å¤ï¼šå­ç±»éœ€è¦è®¿é—®ï¼‰
        self.feature_engineer = feature_engineer
        self.model_metrics = {}
        self.training_task = None
        self.is_first_training = True  # æ ‡è®°æ˜¯å¦é¦–æ¬¡è®­ç»ƒï¼ˆåªæœ‰é¦–æ¬¡æ‰å†™æ•°æ®åº“ï¼‰
        
        # ğŸ”‘ è·å–äº¤æ˜“æ‰€å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨å·¥å‚æ¨¡å¼ï¼Œæ”¯æŒå¤šäº¤æ˜“æ‰€ï¼‰
        self.exchange_client = ExchangeFactory.get_current_client()
        
        # æ¨¡å‹å‚æ•°
        # LightGBMåŸºç¡€å‚æ•°ï¼ˆæ‰€æœ‰æ—¶é—´æ¡†æ¶å…±äº«ï¼‰
        # ğŸ”‘ åŸºç¡€å‚æ•°ï¼ˆä¼šè¢«æ—¶é—´æ¡†æ¶å·®å¼‚åŒ–é…ç½®è¦†ç›–ï¼‰
        self.lgb_params = {
            'objective': 'multiclass',
            'num_class': 3,  # 0: ä¸‹è·Œ, 1: æ¨ªç›˜, 2: ä¸Šæ¶¨
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'n_estimators': 300,  # 500â†’300ï¼ˆå‡å°‘è®­ç»ƒè½®æ•°ï¼Œé˜²è¿‡æ‹Ÿåˆï¼‰
            'num_leaves': 31,  # é»˜è®¤å€¼ï¼Œè®­ç»ƒæ—¶ä¼šæ ¹æ®æ—¶é—´æ¡†æ¶è°ƒæ•´
            'learning_rate': 0.05,  # 0.03â†’0.05ï¼ˆé…åˆå‡å°‘è½®æ•°ï¼‰
            'feature_fraction': 0.8,  # 0.85â†’0.8ï¼ˆæ›´å¼ºç‰¹å¾é‡‡æ ·ï¼‰
            'bagging_fraction': 0.8,  # 0.85â†’0.8ï¼ˆæ›´å¼ºæ•°æ®é‡‡æ ·ï¼‰
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': 42,
            'n_jobs': -1,
            'max_depth': 6,  # 8â†’6ï¼ˆé™ä½æ·±åº¦ï¼‰
            'min_child_samples': 40,  # 30â†’40ï¼ˆå¢åŠ æœ€å°æ ·æœ¬ï¼‰
            'reg_alpha': 0.5,  # 0.3â†’0.5ï¼ˆå¢å¼ºL1æ­£åˆ™åŒ–ï¼‰
            'reg_lambda': 0.5,  # 0.3â†’0.5ï¼ˆå¢å¼ºL2æ­£åˆ™åŒ–ï¼‰
            'min_split_gain': 0.02,  # 0.01â†’0.02ï¼ˆæé«˜åˆ†è£‚é˜ˆå€¼ï¼‰
            'is_unbalance': True  # è‡ªåŠ¨å¤„ç†ä¸å¹³è¡¡ç±»åˆ«
        }

    def _compute_effective_sample_weights(self, y: pd.Series, timeframe: str) -> np.ndarray:
        """ä½¿ç”¨æœ‰æ•ˆæ ·æœ¬æ•°(Effective Number of Samples)è®¡ç®—æ ·æœ¬æƒé‡ï¼Œç¼“è§£æç«¯ç±»åˆ«ä¸å¹³è¡¡ã€‚
        å‚è€ƒ: Class-Balanced Loss Based on Effective Number of Samples (Cui et al., CVPR 2019)

        Args:
            y: æ ‡ç­¾Seriesæˆ–ndarrayï¼Œå–å€¼{0: SHORT, 1: HOLD, 2: LONG}
            timeframe: æ—¶é—´æ¡†æ¶ï¼ˆç”¨äºå¯é€‰çš„æ—¶é—´æ¡†æ¶æ•æ„Ÿè°ƒèŠ‚ï¼‰

        Returns:
            æ¯ä¸ªæ ·æœ¬çš„æƒé‡å‘é‡ï¼ˆä¸yç­‰é•¿ï¼‰
        """
        try:
            y_np = y.values if hasattr(y, 'values') else y
            classes = np.array([0, 1, 2])
            counts = np.array([(y_np == c).sum() for c in classes], dtype=np.float64)
            total = max(int(len(y_np)), 1)

            # é¿å…é›¶è®¡æ•°
            counts = np.maximum(counts, 1.0)

            # betaæŒ‰æ ·æœ¬è§„æ¨¡è‡ªé€‚åº”ï¼Œæ ·æœ¬è¶Šå¤šbetaè¶Šæ¥è¿‘1
            # ä¸ºé˜²æ­¢è¿‡å¼ºæƒé‡ï¼Œè®¾ç½®æ—¶é—´æ¡†æ¶æ•æ„Ÿçš„ä¸Šé™
            base_beta = 0.999
            if timeframe == '3m':
                beta = min(base_beta, 1.0 - 1.0 / (total + 1))
            else:
                beta = min(0.995, 1.0 - 1.0 / (total + 1))

            effective_num = (1.0 - np.power(beta, counts)) / (1.0 - beta)
            class_weights = 1.0 / effective_num
            class_weights = class_weights / class_weights.sum() * len(classes)

            # å°†ç±»åˆ«æƒé‡æ˜ å°„ä¸ºæ ·æœ¬æƒé‡
            weight_map = {c: class_weights[i] for i, c in enumerate(classes)}
            sample_weights = np.array([weight_map[int(label)] for label in y_np], dtype=np.float64)

            return sample_weights
        except Exception:
            logger.error("æœ‰æ•ˆæ ·æœ¬æ•°æƒé‡è®¡ç®—å¤±è´¥ï¼Œé™çº§åˆ°å‡ç­‰æƒé‡")
            return np.ones(len(y))
        
        # âœ… å·®å¼‚åŒ–é…ç½®ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆçš„ä¿å®ˆç­–ç•¥
        self.lgb_params_by_timeframe = {
            '15m': {
                'num_leaves': 110,       # æ ·æœ¬å……è¶³(33k+)ï¼Œä¿æŒè¾ƒé«˜å¤æ‚åº¦
                'min_child_samples': 45,
                'max_depth': 8,
                'reg_alpha': 0.4,
                'reg_lambda': 0.4
            },
            '2h': {
                # ğŸ”‘ å¤§å¹…ç®€åŒ–ï¼š2håªæœ‰3040æ¡æ ·æœ¬ï¼Œä¸¥é‡è¿‡æ‹Ÿåˆ
                'num_leaves': 15,        # 63â†’15ï¼ˆå¤§å¹…é™ä½ï¼‰
                'min_child_samples': 50,  # 30â†’50ï¼ˆæ¯å¶å­çº¦200æ¡ï¼‰
                'max_depth': 5,          # 9â†’5ï¼ˆæµ…å±‚æ ‘ï¼‰
                'reg_alpha': 0.8,        # åŠ å¼ºæ­£åˆ™åŒ–
                'reg_lambda': 0.8        # åŠ å¼ºæ­£åˆ™åŒ–
            },
            '4h': {
                # ğŸ”‘ æç®€é…ç½®ï¼š4håªæœ‰1960æ¡æ ·æœ¬ï¼Œæ›´ä¸¥é‡è¿‡æ‹Ÿåˆ
                'num_leaves': 11,        # 47â†’11ï¼ˆæç®€ï¼‰
                'min_child_samples': 60,  # 30â†’60ï¼ˆæ¯å¶å­çº¦178æ¡ï¼‰
                'max_depth': 4,          # 8â†’4ï¼ˆææµ…ï¼‰
                'reg_alpha': 1.0,        # æå¼ºæ­£åˆ™åŒ–
                'reg_lambda': 1.0        # æå¼ºæ­£åˆ™åŒ–
            }
        }
        
        # GPUé…ç½®
        if settings.USE_GPU:
            self.lgb_params.update({
                'device': 'gpu',
                'gpu_platform_id': 0,
                'gpu_device_id': 0
            })
        
        # æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆæ¯ä¸ªæ—¶é—´æ¡†æ¶ç‹¬ç«‹ï¼‰
        self.model_dir = "models"
        os.makedirs(self.model_dir, exist_ok=True)
    
    def _get_model_paths(self, timeframe: str) -> Dict[str, str]:
        """è·å–æŒ‡å®šæ—¶é—´æ¡†æ¶çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„"""
        # ğŸ”§ ä¿®å¤ï¼šå¤„ç†SYMBOLä¸­çš„/å­—ç¬¦ï¼ˆå¦‚"ETH/USDT"ï¼‰ï¼Œæ›¿æ¢ä¸º_é¿å…è·¯å¾„é—®é¢˜
        # å¿…é¡»ä¸ensemble_ml_serviceä¸­çš„é€»è¾‘ä¿æŒä¸€è‡´
        safe_symbol = settings.SYMBOL.replace('/', '_')
        return {
            'model': os.path.join(self.model_dir, f"{safe_symbol}_{timeframe}_model.pkl"),
            'scaler': os.path.join(self.model_dir, f"{safe_symbol}_{timeframe}_scaler.pkl"),
            'features': os.path.join(self.model_dir, f"{safe_symbol}_{timeframe}_features.pkl")
        }
    
    async def start(self):
        """å¯åŠ¨æœºå™¨å­¦ä¹ æœåŠ¡"""
        try:
            logger.info("å¯åŠ¨æœºå™¨å­¦ä¹ æœåŠ¡...")
            
            # åŠ è½½å·²æœ‰æ¨¡å‹
            await self._load_model()
            
            # æ³¨æ„ï¼šè®­ç»ƒä»»åŠ¡å·²ç”± scheduler ç»Ÿä¸€ç®¡ç†ï¼ˆæ¯å¤©00:01æ‰§è¡Œï¼‰
            # ä¸å†åœ¨æ­¤å¤„å¯åŠ¨ç‹¬ç«‹çš„è®­ç»ƒå¾ªç¯
            
            self.is_running = True
            logger.info("æœºå™¨å­¦ä¹ æœåŠ¡å¯åŠ¨å®Œæˆï¼ˆè®­ç»ƒç”±schedulerç®¡ç†ï¼‰")
            
        except Exception as e:
            logger.error(f"å¯åŠ¨æœºå™¨å­¦ä¹ æœåŠ¡å¤±è´¥: {e}")
            raise
    
    async def stop(self):
        """åœæ­¢æœºå™¨å­¦ä¹ æœåŠ¡"""
        try:
            logger.info("åœæ­¢æœºå™¨å­¦ä¹ æœåŠ¡...")
            
            self.is_running = False
            
            # å–æ¶ˆè‡ªåŠ¨è®­ç»ƒä»»åŠ¡
            if self.training_task:
                self.training_task.cancel()
                try:
                    await self.training_task
                except asyncio.CancelledError:
                    pass
            
            logger.info("æœºå™¨å­¦ä¹ æœåŠ¡å·²åœæ­¢")
            
        except Exception as e:
            logger.error(f"åœæ­¢æœºå™¨å­¦ä¹ æœåŠ¡å¤±è´¥: {e}")
    
    async def train_model(self, force_retrain: bool = False) -> Dict[str, Any]:
        """è®­ç»ƒæ¨¡å‹ï¼ˆä¸ºæ¯ä¸ªæ—¶é—´æ¡†æ¶è®­ç»ƒç‹¬ç«‹æ¨¡å‹ï¼‰"""
        try:
            logger.info("ğŸš€ å¼€å§‹å¤šæ—¶é—´æ¡†æ¶æ¨¡å‹è®­ç»ƒ...")
            logger.info(f"GPUé…ç½®: USE_GPU={settings.USE_GPU}")
            logger.info(f"æ—¶é—´æ¡†æ¶: {settings.TIMEFRAMES}")
            
            all_metrics = {}
            all_training_data = []  # æ”¶é›†æ‰€æœ‰è®­ç»ƒæ•°æ®
            
            # ä¸ºæ¯ä¸ªæ—¶é—´æ¡†æ¶è®­ç»ƒç‹¬ç«‹æ¨¡å‹
            for timeframe in settings.TIMEFRAMES:
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ“Š è®­ç»ƒ {timeframe} æ—¶é—´æ¡†æ¶æ¨¡å‹...")
                logger.info(f"{'='*60}")
                
                try:
                    # è®­ç»ƒå•ä¸ªæ—¶é—´æ¡†æ¶ï¼ˆè¿”å›metricså’Œtraining_dataï¼‰
                    metrics, training_data = await self._train_single_timeframe(timeframe)
                    all_metrics[timeframe] = metrics
                    all_training_data.append(training_data)
                    logger.info(f"âœ… {timeframe} æ¨¡å‹è®­ç»ƒå®Œæˆ - å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
                except Exception as e:
                    logger.error(f"âŒ {timeframe} æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                    logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    all_metrics[timeframe] = {'success': False, 'error': str(e), 'accuracy': 0.0, 'training_time': 0.0}
            
            # ä¿å­˜æ¨¡å‹ï¼ˆå³ä½¿æœ‰ä¸ªåˆ«æ—¶é—´æ¡†æ¶å¤±è´¥ä¹Ÿä¿å­˜æˆåŠŸçš„ï¼‰
            await self._save_model()
            
            # ğŸ”¥ ç¦ç”¨é¦–æ¬¡è®­ç»ƒæ•°æ®å†™å…¥ï¼ˆèŠ‚çœ2åˆ†é’Ÿï¼‰
            # åŸå› ï¼š
            # 1. æ•°æ®åº“æ•°æ®ä»…ç”¨äºå‰ç«¯å±•ç¤ºï¼Œä¸å½±å“é¢„æµ‹
            # 2. WebSocketç¼“å†²åŒºå·²æœ‰60å¤©æ•°æ®ï¼Œè¶³å¤Ÿé¢„æµ‹
            # 3. å®æ—¶WebSocketæ•°æ®ä¼šæŒç»­å†™å…¥æ•°æ®åº“
            # 4. èŠ‚çœ144ç§’å¯åŠ¨æ—¶é—´ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
            # if self.is_first_training and all_training_data:
            #     try:
            #         await self._save_training_data_to_db(all_training_data)
            #         logger.info("ğŸ’¡ é¦–æ¬¡è®­ç»ƒå®Œæˆï¼Œåç»­è®­ç»ƒå°†ä¸å†å†™å…¥å†å²æ•°æ®")
            #     except Exception as e:
            #         logger.warning(f"ä¿å­˜è®­ç»ƒæ•°æ®åˆ°æ•°æ®åº“å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")
            #     finally:
            #         self.is_first_training = False
            
            logger.info("ğŸ’¡ é¦–æ¬¡è®­ç»ƒå®Œæˆï¼ˆå†å²æ•°æ®å·²ç¦ç”¨å†™å…¥ï¼Œä»…ä¿ç•™å®æ—¶WebSocketæ•°æ®ï¼‰")
            self.is_first_training = False
            
            # æ±‡æ€»æ‰€æœ‰æ¨¡å‹æŒ‡æ ‡
            successful_metrics = [m for m in all_metrics.values() if 'accuracy' in m and not np.isnan(m['accuracy'])]
            avg_accuracy = np.mean([m['accuracy'] for m in successful_metrics]) if successful_metrics else 0.0
            total_training_time = sum([m.get('training_time', 0) for m in all_metrics.values()])
            
            self.model_metrics = {
                'timeframe_metrics': all_metrics,
                'average_accuracy': avg_accuracy,
                'accuracy': avg_accuracy,  # å…¼å®¹æ—§ä»£ç 
                'training_time': total_training_time,
                'version': '2.0',
                'training_date': datetime.now().isoformat()
            }
            
            # ç¼“å­˜æ¨¡å‹æŒ‡æ ‡ï¼ˆä¸è¿‡æœŸï¼‰
            await cache_manager.set_model_metrics(settings.SYMBOL, self.model_metrics, expire=None)
            
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ‰ å¤šæ—¶é—´æ¡†æ¶æ¨¡å‹è®­ç»ƒå®Œæˆ")
            logger.info(f"æˆåŠŸè®­ç»ƒ: {len(successful_metrics)}/{len(settings.TIMEFRAMES)} ä¸ªæ—¶é—´æ¡†æ¶")
            logger.info(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
            logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.2f}ç§’")
            logger.info(f"{'='*60}\n")
            
            return self.model_metrics
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {}
    
    async def _train_single_timeframe(self, timeframe: str) -> tuple:
        """è®­ç»ƒå•ä¸ªæ—¶é—´æ¡†æ¶çš„æ¨¡å‹
        
        Returns:
            tuple: (metrics, training_data_with_timeframe) 
        """
        try:
            start_time = time.time()
            
            # è·å–è¯¥æ—¶é—´æ¡†æ¶çš„è®­ç»ƒæ•°æ®
            train_data = await self._prepare_training_data_for_timeframe(timeframe)
            
            if train_data.empty:
                raise Exception(f"{timeframe} è®­ç»ƒæ•°æ®ä¸ºç©º")
            
            
            # ä¿å­˜åŸå§‹è®­ç»ƒæ•°æ®ï¼ˆç”¨äºåç»­å†™å…¥æ•°æ®åº“ï¼‰
            train_data_with_timeframe = train_data.copy()
            train_data_with_timeframe['timeframe'] = timeframe
            
            # ç‰¹å¾å·¥ç¨‹
            train_data = feature_engineer.create_features(train_data)
            
            if train_data.empty:
                raise Exception(f"{timeframe} ç‰¹å¾å·¥ç¨‹åæ•°æ®ä¸ºç©º")
            
            # åˆ›å»ºæ ‡ç­¾ï¼ˆä¼ å…¥timeframeä½¿ç”¨å·®å¼‚åŒ–é˜ˆå€¼ï¼‰
            train_data = self._create_labels(train_data, timeframe=timeframe)
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X, y = self._prepare_features_labels(train_data, timeframe)
            
            if len(X) == 0:
                raise Exception(f"{timeframe} ç‰¹å¾æ•°æ®ä¸ºç©º")
            
            # æ•°æ®é¢„å¤„ç†
            X_scaled = self._scale_features(X, timeframe=timeframe, fit=True)
            
            # æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆå‰80%è®­ç»ƒï¼Œå20%éªŒè¯ï¼‰
            split_idx = int(len(X_scaled) * 0.8)
            X_train = X_scaled[:split_idx]
            X_val = X_scaled[split_idx:]
            y_train = y.iloc[:split_idx]
            y_val = y.iloc[split_idx:]
            
            logger.info(f"ğŸ“Š {timeframe} æ—¶é—´åºåˆ—åˆ†å‰²: è®­ç»ƒ{len(X_train)}æ¡, éªŒè¯{len(X_val)}æ¡")
            
            # è®­ç»ƒæ¨¡å‹ï¼ˆä¼ å…¥timeframeä»¥ä½¿ç”¨å·®å¼‚åŒ–å‚æ•°ï¼‰
            model = self._train_lightgbm(X_train, y_train, X_val, y_val, timeframe=timeframe)
            self.models[timeframe] = model
            
            # è¯„ä¼°æ¨¡å‹
            metrics = self._evaluate_model_for_timeframe(X_val, y_val, timeframe)
            
            training_time = time.time() - start_time
            metrics['training_time'] = training_time
            metrics['timeframe'] = timeframe
            
            logger.info(f"â±ï¸ {timeframe} è®­ç»ƒè€—æ—¶: {training_time:.2f}ç§’")
            
            # è¿”å›metricså’Œè®­ç»ƒæ•°æ®
            return metrics, train_data_with_timeframe
            
        except Exception as e:
            logger.error(f"{timeframe} å•æ—¶é—´æ¡†æ¶è®­ç»ƒå¤±è´¥: {e}")
            raise
    
    async def predict(self, data: pd.DataFrame, timeframe: str) -> Dict[str, Any]:
        """æ¨¡å‹é¢„æµ‹ï¼ˆéœ€æŒ‡å®šæ—¶é—´æ¡†æ¶ï¼‰"""
        try:
            
            # æ£€æŸ¥è¯¥æ—¶é—´æ¡†æ¶çš„æ¨¡å‹æ˜¯å¦åŠ è½½
            if timeframe not in self.models or self.models[timeframe] is None:
                logger.warning(f"âš ï¸ {timeframe} æ¨¡å‹æœªåŠ è½½ï¼Œå°è¯•åŠ è½½æ¨¡å‹")
                await self._load_model()
                
                if timeframe not in self.models or self.models[timeframe] is None:
                    raise Exception(f"{timeframe} æ¨¡å‹ä¸å¯ç”¨")
            
            # ç‰¹å¾å·¥ç¨‹
            logger.debug(f"ğŸ“Š {timeframe} ç‰¹å¾å·¥ç¨‹...")
            processed_data = feature_engineer.create_features(data.copy())
            
            if processed_data.empty:
                raise Exception("ç‰¹å¾å·¥ç¨‹åæ•°æ®ä¸ºç©º")
            
            
            # è·å–æœ€æ–°ä¸€è¡Œæ•°æ®ï¼ˆä½¿ç”¨è¯¥æ—¶é—´æ¡†æ¶çš„ç‰¹å¾åˆ—ï¼‰
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            if not feature_columns:
                raise Exception(f"{timeframe} ç‰¹å¾åˆ—æœªæ‰¾åˆ°")
            
            latest_data = processed_data.iloc[-1:][feature_columns]
            
            # âœ… è°ƒè¯•æ—¥å¿—ï¼šéªŒè¯è¾“å…¥æ•°æ®
            if 'close' in processed_data.columns:
                last_3_closes = processed_data['close'].tail(3).tolist()
            
            
            # âœ… è®°å½•å…³é”®ç‰¹å¾å€¼ï¼ˆç”¨äºè¯Šæ–­ï¼‰
            if 'price_change' in latest_data.columns:
                price_change = latest_data['price_change'].iloc[0]
            
            # æ•°æ®é¢„å¤„ç†ï¼ˆä½¿ç”¨è¯¥æ—¶é—´æ¡†æ¶çš„scalerï¼‰
            X_scaled = self._scale_features(latest_data, timeframe=timeframe, fit=False)
            
            # é¢„æµ‹ï¼ˆä½¿ç”¨è¯¥æ—¶é—´æ¡†æ¶çš„æ¨¡å‹ï¼‰
            model = self.models[timeframe]
            probabilities = model.predict_proba(X_scaled)[0]
            prediction = np.argmax(probabilities)
            confidence = np.max(probabilities)
            
            # è½¬æ¢é¢„æµ‹ç»“æœ
            signal_map = {0: 'SHORT', 1: 'HOLD', 2: 'LONG'}
            signal_type = signal_map[prediction]
            
            # ç®€æ´è®°å½•é¢„æµ‹ç»“æœï¼ˆä½¿ç”¨å›¾æ ‡+ä¸­æ–‡ï¼‰
            logger.info(f"ğŸ¯ {timeframe} é¢„æµ‹: {format_signal_type(signal_type)} (ç½®ä¿¡åº¦={confidence:.4f}, æ¦‚ç‡: ğŸ“‰{probabilities[0]:.2f} â¸ï¸{probabilities[1]:.2f} ğŸ“ˆ{probabilities[2]:.2f})")
            
            result = {
                'signal_type': signal_type,
                'confidence': float(confidence),
                'probabilities': {
                    'short': float(probabilities[0]),
                    'hold': float(probabilities[1]),
                    'long': float(probabilities[2])
                },
                'timestamp': datetime.now(),
                'model_version': self.model_metrics.get('version', '1.0')
            }
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ {timeframe} æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}", exc_info=True)
            return {}
    
    async def _prepare_training_data_for_timeframe(self, timeframe: str) -> pd.DataFrame:
        """ä¸ºå•ä¸ªæ—¶é—´æ¡†æ¶å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå·®å¼‚åŒ–è®­ç»ƒå¤©æ•°ï¼‰"""
        try:
            symbol = settings.SYMBOL
            
            # ğŸ”‘ è¶…çŸ­çº¿è®­ç»ƒå¤©æ•°é…ç½®ï¼šç¡®ä¿è¶³å¤Ÿçš„é«˜é¢‘æ ·æœ¬
            training_days_config = {
                '3m': 120,   # è¶…çŸ­æœŸï¼š120å¤©ï¼ˆ57,600æ¡ï¼‰
                '5m': 120,   # ä¸»æ—¶é—´æ¡†æ¶ï¼š120å¤©ï¼ˆ34,560æ¡ï¼‰
                '15m': 120   # è¶‹åŠ¿ç¡®è®¤ï¼š120å¤©ï¼ˆ11,520æ¡ï¼‰
            }
            training_days = training_days_config.get(timeframe, 120)
            
            # æ—¶é—´å‘¨æœŸå¯¹åº”çš„åˆ†é’Ÿæ•°
            interval_minutes = {
                '1m': 1, '3m': 3, '5m': 5, '15m': 15, '30m': 30,
                '1h': 60, '2h': 120, '4h': 240, '6h': 360, '8h': 480,
                '12h': 720, '1d': 1440
            }
            
            # æ ¹æ®æ—¶é—´å‘¨æœŸè®¡ç®—éœ€è¦çš„Kçº¿æ•°é‡
            minutes = interval_minutes.get(timeframe, 60)
            required_klines = int((training_days * 24 * 60) / minutes)
            
            logger.info(f"ğŸ“¥ è·å– {timeframe} æ•°æ®: {required_klines}æ¡Kçº¿ ({training_days}å¤©)")
            
            # âœ… ç»Ÿä¸€ä½¿ç”¨åˆ†é¡µæ–¹æ³•ï¼ˆè‡ªåŠ¨å¤„ç†è¶…è¿‡1500çš„æƒ…å†µï¼Œæ”¯æŒå¤šäº¤æ˜“æ‰€ï¼‰
            all_klines = self.exchange_client.get_klines_paginated(
                symbol=symbol,
                interval=timeframe,
                limit=required_klines,
                rate_limit_delay=0.1
            )
            
            # è½¬æ¢ä¸ºDataFrameï¼ˆä¸ä¾èµ–reverseï¼Œç›´æ¥ç”¨æ—¶é—´æˆ³æ’åºï¼‰
            df = pd.DataFrame(all_klines)
            
            if not df.empty:
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                
                # ğŸ”‘ å…³é”®ï¼šä¾èµ–æ—¶é—´æˆ³æ’åºï¼Œè€Œä¸æ˜¯å‡è®¾APIè¿”å›é¡ºåº
                df = df.sort_values('timestamp', ascending=True)  # æ˜ç¡®æŒ‡å®šå‡åºï¼ˆæ—§â†’æ–°ï¼‰
                
                # âœ… å»é‡ï¼ˆé˜²æ­¢æ‰¹æ¬¡è¾¹ç•Œé‡å¤ï¼‰
                df = df.drop_duplicates(subset=['timestamp'], keep='last')
                
                # è®¾ç½®ç´¢å¼•
                df = df.set_index('timestamp')
                
                logger.info(f"âœ… {timeframe} æ•°æ®è·å–æˆåŠŸ: {len(df)}æ¡")
            else:
                logger.warning(f"âš ï¸ {timeframe} æ•°æ®ä¸ºç©º")
            
            return df
            
        except Exception as e:
            logger.error(f"ä¸º{timeframe}å‡†å¤‡è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    
    async def _save_training_data_to_db(self, all_data: list):
        """å°†è®­ç»ƒæ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä¾›å‰ç«¯å±•ç¤ºï¼‰
        
        ä¼˜åŒ–ï¼šåˆå¹¶æ‰€æœ‰æ—¶é—´æ¡†æ¶ï¼Œä¸€æ¬¡æ€§æ‰¹é‡å†™å…¥ï¼Œå‡å°‘æ•°æ®åº“è¿æ¥å‹åŠ›
        
        Args:
            all_data: List of DataFrames with 'timeframe' column
        """
        try:
            logger.info("ğŸ“¥ å¼€å§‹å°†è®­ç»ƒæ•°æ®å†™å…¥æ•°æ®åº“...")
            
            # ğŸ”¥ ä¼˜åŒ–ï¼šå…ˆæ”¶é›†æ‰€æœ‰æ•°æ®ï¼Œç„¶åä¸€æ¬¡æ€§å†™å…¥
            all_klines = []
            
            for df in all_data:
                if df is None or df.empty:
                    logger.warning("è·³è¿‡ç©ºDataFrame")
                    continue
                
                # æ£€æŸ¥timeframeåˆ—æ˜¯å¦å­˜åœ¨
                if 'timeframe' not in df.columns:
                    logger.warning(f"DataFrameç¼ºå°‘timeframeåˆ—ï¼Œè·³è¿‡: {df.columns.tolist()}")
                    continue
                
                # è·å–æ—¶é—´æ¡†æ¶
                timeframe = df['timeframe'].iloc[0]
                logger.info(f"  å¤„ç† {timeframe} æ•°æ®...")
                
                # ç§»é™¤timeframeåˆ—ï¼Œå‡†å¤‡å†™å…¥
                df_to_save = df.drop('timeframe', axis=1).copy()
                
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
                for idx, row in df_to_save.iterrows():
                    try:
                        kline = {
                            'symbol': settings.SYMBOL,
                            'interval': timeframe,
                            'timestamp': int(idx.timestamp() * 1000),
                            'open': float(row['open']),
                            'high': float(row['high']),
                            'low': float(row['low']),
                            'close': float(row['close']),
                            'volume': float(row['volume']),
                            'close_time': int(idx.timestamp() * 1000),
                            'quote_volume': float(row.get('quote_volume', 0)),
                            'trades': int(row.get('trades', 0)),
                            'taker_buy_base_volume': float(row.get('taker_buy_base_volume', 0)),
                            'taker_buy_quote_volume': float(row.get('taker_buy_quote_volume', 0))
                        }
                        all_klines.append(kline)
                    except Exception as e:
                        logger.warning(f"è·³è¿‡æ— æ•ˆè¡Œ: {e}")
                        continue
                
                logger.info(f"  âœ“ {timeframe} å‡†å¤‡å®Œæˆ: {len(df_to_save)}æ¡")
            
            # ğŸ”¥ ä¸€æ¬¡æ€§å†™å…¥æ‰€æœ‰æ•°æ®ï¼ˆå‡å°‘è¿æ¥æ± å‹åŠ›ï¼‰
            if all_klines:
                logger.info(f"ğŸ“Š å¼€å§‹ä¸€æ¬¡æ€§å†™å…¥æ‰€æœ‰æ•°æ®: {len(all_klines)}æ¡...")
                try:
                    await postgresql_manager.write_kline_data(all_klines)
                    logger.info(f"âœ… è®­ç»ƒæ•°æ®å†™å…¥æ•°æ®åº“å®Œæˆ: æ€»è®¡{len(all_klines)}æ¡")
                except Exception as e:
                    logger.error(f"  âœ— æ•°æ®åº“å†™å…¥å¤±è´¥: {e}")
                    raise
            else:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å†™å…¥")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæ•°æ®å†™å…¥æ•°æ®åº“å¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            raise
    
    def _create_labels(self, df: pd.DataFrame, timeframe: str = None) -> pd.DataFrame:
        """
        åˆ›å»ºæ ‡ç­¾ï¼ˆä¼˜åŒ–ç‰ˆï¼šè§£å†³HOLDå æ¯”è¿‡é«˜é—®é¢˜ï¼‰
        
        æ”¹è¿›:
        1. åŸºäºå¸‚åœºæ³¢åŠ¨ç‡çš„è‡ªé€‚åº”é˜ˆå€¼
        2. åˆ†ä½æ•°é˜ˆå€¼ç¡®ä¿ç±»åˆ«å¹³è¡¡
        3. æ··åˆç­–ç•¥æå‡ç¨³å®šæ€§
        
        ç›®æ ‡åˆ†å¸ƒ:
        - LONG: 28-32%
        - HOLD: 36-44%
        - SHORT: 28-32%
        
        Args:
            df: Kçº¿æ•°æ®
            timeframe: æ—¶é—´æ¡†æ¶ï¼ˆç”¨äºå·®å¼‚åŒ–é˜ˆå€¼é…ç½®ï¼‰
        """
        try:
            # è®¡ç®—ä¸‹ä¸€æ ¹Kçº¿æ”¶ç›Šç‡
            df['next_return'] = df['close'].shift(-1) / df['close'] - 1
            
            # ========================================
            # ğŸ”¥ æ–°å¢ï¼šè‡ªé€‚åº”é˜ˆå€¼è®¡ç®—ï¼ˆä¸‰ç§æ–¹æ³•ï¼‰
            # ========================================
            
            # æ–¹æ³•1ï¼šåŸºäºå†å²æ³¢åŠ¨ç‡
            returns = df['close'].pct_change()
            historical_volatility = returns.rolling(100).std()
            median_vol = historical_volatility.median()
            
            # æ—¶é—´æ¡†æ¶ç³»æ•°ï¼ˆä¼˜åŒ–ç‰ˆv2ï¼šé’ˆå¯¹ä½æ³¢åŠ¨å¸‚åœºï¼‰
            # åˆ†æï¼šå½“å‰é˜ˆå€¼0.215%ä»å¯¼è‡´HOLDå æ¯”88%ï¼Œè¯´æ˜éœ€è¦æ›´æ¿€è¿›çš„ç³»æ•°
            timeframe_multiplier = {
                '3m': 1.50,   # 150%å†å²æ³¢åŠ¨ç‡ï¼ˆé™ä½ç³»æ•°ï¼Œè®©åˆ†ä½æ•°ä¸»å¯¼ï¼‰
                '5m': 1.60,   # 160%å†å²æ³¢åŠ¨ç‡
                '15m': 1.80   # 180%å†å²æ³¢åŠ¨ç‡
            }
            multiplier = timeframe_multiplier.get(timeframe, 1.60)
            vol_threshold = median_vol * multiplier if not pd.isna(median_vol) else 0.0025
            
            # æ–¹æ³•2ï¼šåŸºäºæ”¶ç›Šç‡åˆ†ä½æ•°ï¼ˆç¼©å°èŒƒå›´ä»¥é™ä½HOLDå æ¯”ï¼‰
            returns_clean = returns.dropna()
            if len(returns_clean) > 100:
                # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨60%/40%åˆ†ä½æ•°ï¼ˆç¼©å°èŒƒå›´ï¼Œé˜ˆå€¼æ›´å°ï¼Œæ›´å¤šLONG/SHORTï¼‰
                # åŸç†ï¼š60%/40%æ„å‘³ç€åªæœ‰ä¸­é—´20%æ˜¯HOLDï¼Œå…¶ä½™80%æ˜¯LONG/SHORT
                upper_quantile = returns_clean.quantile(0.60)  # 60%åˆ†ä½æ•°ï¼ˆç¼©å°èŒƒå›´ï¼‰
                lower_quantile = returns_clean.quantile(0.40)  # 40%åˆ†ä½æ•°ï¼ˆç¼©å°èŒƒå›´ï¼‰
                quantile_threshold = max(abs(upper_quantile), abs(lower_quantile))
            else:
                quantile_threshold = 0.0025
            
            # æ–¹æ³•3ï¼šæ··åˆé˜ˆå€¼ï¼ˆæé«˜åˆ†ä½æ•°æƒé‡ï¼‰
            # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¢åŠ åˆ†ä½æ•°æƒé‡åˆ°90%ï¼Œå‡ ä¹å®Œå…¨ä¾èµ–åˆ†ä½æ•°
            # åŸå› ï¼šåœ¨ä½æ³¢åŠ¨å¸‚åœºï¼Œåˆ†ä½æ•°æ³•æ›´å¯é ï¼Œèƒ½ç¡®ä¿ç±»åˆ«å¹³è¡¡
            hybrid_threshold = vol_threshold * 0.10 + quantile_threshold * 0.90
            
            # è®¾ç½®åˆç†èŒƒå›´ï¼ˆé˜²æ­¢æç«¯å€¼ï¼Œä½†æ”¾å®½æœ€å°å€¼ä»¥é€‚é…ä½æ³¢åŠ¨å¸‚åœºï¼‰
            min_threshold_config = {
                '3m': 0.0003,  # æœ€å°0.03%ï¼ˆ3åˆ†é’Ÿå•æœŸæ³¢åŠ¨ç‡çº¦0.06%ï¼Œå…è®¸æ›´ä½é˜ˆå€¼ï¼‰
                '5m': 0.0004,  # æœ€å°0.04%
                '15m': 0.0008  # æœ€å°0.08%
            }
            max_threshold_config = {
                '3m': 0.0050,  # æœ€å¤§0.50%
                '5m': 0.0060,  # æœ€å¤§0.60%
                '15m': 0.0080  # æœ€å¤§0.80%
            }
            
            min_threshold = min_threshold_config.get(timeframe, 0.0020)
            max_threshold = max_threshold_config.get(timeframe, 0.0060)
            
            # æœ€ç»ˆé˜ˆå€¼ï¼ˆé™åˆ¶åœ¨åˆç†èŒƒå›´ï¼‰
            up_threshold = np.clip(hybrid_threshold, min_threshold, max_threshold)
            down_threshold = -up_threshold
            
            # ========================================
            # åˆ›å»ºåˆ†ç±»æ ‡ç­¾
            # ========================================
            conditions = [
                df['next_return'] <= down_threshold,  # SHORT (0)
                (df['next_return'] > down_threshold) & (df['next_return'] < up_threshold),  # HOLD (1)
                df['next_return'] >= up_threshold     # LONG (2)
            ]
            
            choices = [0, 1, 2]
            df['label'] = np.select(conditions, choices, default=1)
            
            # ç§»é™¤æœ€å1è¡Œï¼ˆæ²¡æœ‰next_returnï¼‰
            df = df[:-1]
            
            # ========================================
            # æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡ä¸è´¨é‡æ£€æŸ¥
            # ========================================
            label_counts = df['label'].value_counts().sort_index()
            total = len(df)
            
            short_count = label_counts.get(0, 0)
            hold_count = label_counts.get(1, 0)
            long_count = label_counts.get(2, 0)
            
            short_pct = short_count / total * 100
            hold_pct = hold_count / total * 100
            long_pct = long_count / total * 100
            
            logger.info(f"ğŸ“Š {timeframe} æ ‡ç­¾åˆ†å¸ƒï¼ˆè‡ªé€‚åº”é˜ˆå€¼: Â±{up_threshold*100:.3f}%ï¼‰:")
            logger.info(f"  SHORT (0): {short_count:5d}æ¡ ({short_pct:5.1f}%)")
            logger.info(f"  HOLD  (1): {hold_count:5d}æ¡ ({hold_pct:5.1f}%)")
            logger.info(f"  LONG  (2): {long_count:5d}æ¡ ({long_pct:5.1f}%)")
            logger.info(f"  é˜ˆå€¼æ¥æº: æ³¢åŠ¨ç‡={vol_threshold*100:.3f}%, "
                       f"åˆ†ä½æ•°={quantile_threshold*100:.3f}%, "
                       f"æ··åˆ={hybrid_threshold*100:.3f}%")
            
            # è´¨é‡æ£€æŸ¥ä¸å‘Šè­¦
            if hold_pct > 50:
                logger.warning(f"âš ï¸ {timeframe} HOLDå æ¯”ä»ç„¶è¿‡é«˜ ({hold_pct:.1f}%)ï¼Œ"
                             f"å»ºè®®æ£€æŸ¥å¸‚åœºæ³¢åŠ¨ç‡æˆ–è°ƒæ•´ç³»æ•°")
            elif hold_pct < 30:
                logger.warning(f"âš ï¸ {timeframe} HOLDå æ¯”è¿‡ä½ ({hold_pct:.1f}%)ï¼Œ"
                             f"å¯èƒ½å¯¼è‡´è¿‡åº¦äº¤æ˜“")
            else:
                logger.info(f"âœ… {timeframe} æ ‡ç­¾åˆ†å¸ƒå¥åº· (HOLD={hold_pct:.1f}%)")
            
            if short_pct < 25 or long_pct < 25:
                logger.warning(f"âš ï¸ {timeframe} LONG/SHORTå æ¯”ä¸è¶³ "
                             f"(LONG={long_pct:.1f}%, SHORT={short_pct:.1f}%)ï¼Œ"
                             f"å¯èƒ½å½±å“æ¨¡å‹å­¦ä¹ ")
            
            return df
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ ‡ç­¾å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return df
    
    def _prepare_features_labels(self, df: pd.DataFrame, timeframe: str) -> Tuple[pd.DataFrame, pd.Series]:
        """å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå¤šæ—¶é—´æ¡†æ¶ç‹¬ç«‹ç‰¹å¾ï¼‰
        
        Args:
            df: åŒ…å«labelåˆ—çš„DataFrame
            timeframe: æ—¶é—´æ¡†æ¶ï¼ˆå¿…éœ€ï¼‰
            
        Returns:
            (X, y): ç‰¹å¾DataFrameå’Œæ ‡ç­¾Series
        """
        try:
            # æ’é™¤éç‰¹å¾åˆ—
            exclude_cols = [
                'timestamp', 'datetime', 'open', 'high', 'low', 'close', 
                'volume', 'quote_volume', 'label', 'next_return'
            ]
            
            feature_cols = [col for col in df.columns if col not in exclude_cols]
            
            # ğŸ”‘ å…ˆæå–æ ‡ç­¾ï¼ˆç‰¹å¾é€‰æ‹©éœ€è¦ç”¨åˆ°ï¼‰
            y = df['label'].copy()
            
            # ä¸ºæ¯ä¸ªæ—¶é—´æ¡†æ¶é€‰æ‹©ç‹¬ç«‹çš„é‡è¦ç‰¹å¾ï¼ˆåŸºäºæ¨¡å‹çš„ä¸¤é˜¶æ®µé€‰æ‹©ï¼‰
            if timeframe not in self.feature_columns_dict or not self.feature_columns_dict[timeframe]:
                # ğŸ†• æ™ºèƒ½ç‰¹å¾é€‰æ‹©ï¼šåŸºäºLightGBMé‡è¦æ€§çš„ä¸¤é˜¶æ®µé€‰æ‹©
                selected_features = self._select_features_intelligent(
                    df[feature_cols], 
                    y, 
                    timeframe
                )
                self.feature_columns_dict[timeframe] = selected_features
                logger.info(f"âœ… {timeframe} ç‰¹å¾é€‰æ‹©å®Œæˆ: {len(selected_features)}/{len(feature_cols)} ä¸ªç‰¹å¾")
            
            feature_columns = self.feature_columns_dict[timeframe]
            
            X = df[feature_columns].copy()
            
            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            mask = ~(X.isna().any(axis=1) | y.isna())
            X = X[mask]
            y = y[mask]
            
            logger.info(f"ç‰¹å¾æ•°é‡: {len(feature_columns)}, æ ·æœ¬æ•°é‡: {len(X)}")
            
            return X, y
            
        except Exception as e:
            logger.error(f"å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾å¤±è´¥: {e}")
            return pd.DataFrame(), pd.Series()
    
    def _select_features_intelligent(
        self, 
        X: pd.DataFrame, 
        y: pd.Series, 
        timeframe: str
    ) -> list:
        """
        æ™ºèƒ½ç‰¹å¾é€‰æ‹©ï¼ˆä¸¤é˜¶æ®µ + åŠ¨æ€é¢„ç®—ï¼‰
        
        é˜¶æ®µ1: Filterè¿‡æ»¤é›¶å¢ç›Šç‰¹å¾
        é˜¶æ®µ2: åµŒå…¥å¼é€‰æ‹© + åŠ¨æ€é¢„ç®—
        
        Args:
            X: ç‰¹å¾DataFrame
            y: æ ‡ç­¾Series
            timeframe: æ—¶é—´æ¡†æ¶
        
        Returns:
            é€‰ä¸­çš„ç‰¹å¾åˆ—è¡¨
        """
        try:
            n_samples = len(X)
            n_feats = len(X.columns)
            ratio = n_samples / n_feats if n_feats > 0 else 0
            
            # 1. åŠ¨æ€é¢„ç®—è®¡ç®—ï¼ˆæ ¹æ®æ ·æœ¬/ç‰¹å¾æ¯”ï¼‰
            # ä¸åŒæ—¶é—´æ¡†æ¶çš„æœ€å°‘æ ·æœ¬æ•°/ç‰¹å¾ç³»æ•°
            # ğŸ¯ è°ƒæ•´ç­–ç•¥ï¼šå…è®¸æ›´å¤šç‰¹å¾ä»¥è¾¾åˆ°50%å‡†ç¡®ç‡ç›®æ ‡
            ratio_map = {
                '15m': 120,  # 150â†’120ï¼Œå…è®¸æ›´å¤šç‰¹å¾ï¼ˆ34360/120=286ä¸ªï¼Œå–150ï¼‰
                '2h': 80,    # ä»150é™ä½â†’å…è®¸æ›´å¤šç‰¹å¾ï¼ˆ3040/80=38ä¸ªï¼‰
                '4h': 50     # ä»100é™ä½â†’å…è®¸æ›´å¤šç‰¹å¾ï¼ˆ1960/50=39ä¸ªï¼‰
            }
            k = ratio_map.get(timeframe, 100)
            # ğŸ†• Kimå»ºè®®2: ä¿åº•8ä¸ªç‰¹å¾ï¼Œå°é¡¶150
            budget = max(8, min(int(n_samples / k), 150))
            
            logger.info(f"ğŸ“Š {timeframe} æ ·æœ¬/ç‰¹å¾æ¯”={ratio:.1f}, åŠ¨æ€é¢„ç®—={budget}ä¸ªç‰¹å¾")
            
            # 2. é˜¶æ®µâ‘ ï¼šFilterè¿‡æ»¤é›¶å¢ç›Šç‰¹å¾
            logger.info(f"ğŸ” é˜¶æ®µ1: Filteré›¶å¢ç›Šç‰¹å¾...")
            lgb_filter = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                random_state=42,
                n_jobs=-1,
                verbose=-1  # é™é»˜æ¨¡å¼
            )
            lgb_filter.fit(X, y)
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            imp = lgb_filter.feature_importances_
            # ğŸ†• Kimå»ºè®®1: ä½¿ç”¨å‡å€¼é˜ˆå€¼ï¼Œè¿‡æ»¤å™ªéŸ³ç‰¹å¾ï¼ˆå¦‚1e-6ï¼‰
            imp_threshold = imp.mean() * 0.1  # å‡å€¼çš„10%
            stage1_mask = imp > imp_threshold
            stage1_cols = X.columns[stage1_mask].tolist()
            
            filtered_count = n_feats - len(stage1_cols)
            logger.info(f"âœ… è¿‡æ»¤äº†{filtered_count}ä¸ªä½é‡è¦æ€§ç‰¹å¾(<{imp_threshold:.6f}), å‰©ä½™{len(stage1_cols)}ä¸ª")
            
            # ğŸ†• Kimå»ºè®®4: é‡Šæ”¾å†…å­˜
            del lgb_filter
            gc.collect()
            
            # å¦‚æœè¿‡æ»¤åç‰¹å¾æ•°å·²ç»<=é¢„ç®—ï¼Œç›´æ¥è¿”å›
            if len(stage1_cols) <= budget:
                logger.info(f"âœ… {timeframe} ç‰¹å¾æ•°å·²æ»¡è¶³é¢„ç®—ï¼Œè·³è¿‡é˜¶æ®µ2")
                return stage1_cols
            
            # 3. é˜¶æ®µâ‘¡ï¼šåµŒå…¥å¼é€‰æ‹©ï¼ˆåŸºäºé¢„ç®—ï¼‰
            logger.info(f"ğŸ” é˜¶æ®µ2: åµŒå…¥å¼é€‰æ‹©Top {budget}...")
            selector = SelectFromModel(
                lgb.LGBMClassifier(
                    n_estimators=300,
                    learning_rate=0.05,
                    reg_alpha=0.1,
                    reg_lambda=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    random_state=42,
                    n_jobs=-1,
                    verbose=-1
                ),
                max_features=budget,
                threshold=-np.inf,  # åªå—max_featuresé™åˆ¶
                importance_getter='auto',
                prefit=False  # ğŸ†• Kimå»ºè®®3: æ˜¾å¼å£°æ˜ï¼Œä¿æŒå¯æ§
            )
            
            selector.fit(X[stage1_cols], y)
            stage2_mask = selector.get_support()
            selected_cols = [stage1_cols[i] for i, selected in enumerate(stage2_mask) if selected]
            
            # ğŸ†• Kimå»ºè®®4: é‡Šæ”¾å†…å­˜
            del selector
            gc.collect()
            
            # è®¡ç®—æœ€ç»ˆæ ·æœ¬/ç‰¹å¾æ¯”
            final_ratio = n_samples / len(selected_cols) if len(selected_cols) > 0 else 0
            
            logger.info(f"âœ… {timeframe} ä¸¤é˜¶æ®µç‰¹å¾é€‰æ‹©å®Œæˆ:")
            logger.info(f"   åŸå§‹: {n_feats}ä¸ª â†’ Filter: {len(stage1_cols)}ä¸ª â†’ æœ€ç»ˆ: {len(selected_cols)}ä¸ª")
            logger.info(f"   æ ·æœ¬æ•°: {n_samples}, æ ·æœ¬/ç‰¹å¾æ¯”: {final_ratio:.1f}")
            
            # ğŸ†• Kimå»ºè®®5: è¿‡æ‹Ÿåˆè­¦æˆ’çº¿
            if final_ratio < 50:
                logger.warning(f"âš ï¸ {timeframe} æ ·æœ¬/ç‰¹å¾æ¯” <50ï¼Œå»ºè®®è°ƒå¤§kå€¼æˆ–å¢åŠ å¤–éƒ¨æ­£åˆ™åŒ–")
            elif final_ratio < 100:
                logger.warning(f"âš ï¸ {timeframe} æ ·æœ¬/ç‰¹å¾æ¯” <100ï¼Œæ³¨æ„ç›‘æ§è¿‡æ‹Ÿåˆ")
            
            return selected_cols
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•çš„top_né€‰æ‹©
            logger.warning(f"âš ï¸ é™çº§åˆ°ç®€å•ç‰¹å¾é€‰æ‹©...")
            top_n = {'15m': 100, '2h': 80, '4h': 60}.get(timeframe, 80)
            feature_importance = feature_engineer.get_feature_importance(X)
            selected = list(feature_importance.keys())[:top_n]
            return selected
    
    def _scale_features(self, X: pd.DataFrame, timeframe: str, fit: bool = False) -> np.ndarray:
        """ç‰¹å¾ç¼©æ”¾ï¼ˆå¤šæ—¶é—´æ¡†æ¶ç‹¬ç«‹Scalerï¼‰
        
        Args:
            X: ç‰¹å¾DataFrame
            timeframe: æ—¶é—´æ¡†æ¶ï¼ˆå¿…éœ€ï¼‰
            fit: æ˜¯å¦æ‹Ÿåˆæ–°çš„scaler
            
        Returns:
            ç¼©æ”¾åçš„ç‰¹å¾æ•°ç»„
        """
        try:
            # ğŸ”¥ ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥å¹¶å¤„ç†æ— ç©·å¤§å€¼ï¼ˆCritical Fixï¼‰
            inf_count = 0
            nan_count = 0
            large_count = 0
            
            for col in X.columns:
                if pd.api.types.is_numeric_dtype(X[col]):
                    # æ£€æŸ¥inf
                    inf_mask = np.isinf(X[col])
                    if inf_mask.any():
                        inf_count += inf_mask.sum()
                        # ç”¨è¯¥åˆ—çš„æœ€å¤§æœ‰é™å€¼æ›¿æ¢æ­£æ— ç©·ï¼Œæœ€å°æœ‰é™å€¼æ›¿æ¢è´Ÿæ— ç©·
                        finite_values = X.loc[~inf_mask, col]
                        if len(finite_values) > 0:
                            X.loc[X[col] == np.inf, col] = finite_values.max()
                            X.loc[X[col] == -np.inf, col] = finite_values.min()
                        else:
                            X.loc[inf_mask, col] = 0  # å¦‚æœæ²¡æœ‰æœ‰é™å€¼ï¼Œç”¨0å¡«å……
                    
                    # æ£€æŸ¥è¿‡å¤§å€¼ï¼ˆå¯èƒ½å¯¼è‡´ç¼©æ”¾æ—¶æº¢å‡ºï¼‰
                    large_value_threshold = 1e15
                    large_mask = np.abs(X[col]) > large_value_threshold
                    if large_mask.any():
                        large_count += large_mask.sum()
                        # é™åˆ¶åœ¨é˜ˆå€¼èŒƒå›´å†…
                        X.loc[large_mask & (X[col] > 0), col] = large_value_threshold
                        X.loc[large_mask & (X[col] < 0), col] = -large_value_threshold
                    
                    # æ£€æŸ¥NaN
                    nan_mask = X[col].isna()
                    if nan_mask.any():
                        nan_count += nan_mask.sum()
                        X.loc[nan_mask, col] = X[col].median()  # ç”¨ä¸­ä½æ•°å¡«å……NaN
            
            if inf_count > 0:
                logger.warning(f"âš ï¸ ç‰¹å¾ç¼©æ”¾å‰å¤„ç†äº†{inf_count}ä¸ªæ— ç©·å¤§å€¼ï¼ˆinfï¼‰")
            if large_count > 0:
                logger.warning(f"âš ï¸ ç‰¹å¾ç¼©æ”¾å‰å¤„ç†äº†{large_count}ä¸ªè¿‡å¤§å€¼ï¼ˆ>1e15ï¼‰")
            if nan_count > 0:
                logger.warning(f"âš ï¸ ç‰¹å¾ç¼©æ”¾å‰å¤„ç†äº†{nan_count}ä¸ªç¼ºå¤±å€¼ï¼ˆNaNï¼‰")
            
            # æ¯ä¸ªæ—¶é—´æ¡†æ¶ç‹¬ç«‹çš„scaler
            # ğŸ”§ ä¿®å¤ï¼šæ”¯æŒå­—å…¸ç»“æ„çš„scalerï¼ˆç”¨äºInformer-2ï¼‰
            if fit or timeframe not in self.scalers or self.scalers[timeframe] is None:
                # åˆ›å»ºæ–°çš„scalerï¼ˆå¦‚æœæ˜¯å­—å…¸ç»“æ„ï¼Œéœ€è¦åˆ›å»º'traditional'é”®ï¼‰
                if isinstance(self.scalers.get(timeframe), dict):
                    # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œåˆ›å»ºæ–°çš„traditional scaler
                    if self.scalers[timeframe] is None:
                        self.scalers[timeframe] = {}
                    self.scalers[timeframe]['traditional'] = StandardScaler()
                    X_scaled = self.scalers[timeframe]['traditional'].fit_transform(X)
                else:
                    # ç›´æ¥åˆ›å»ºStandardScalerå¯¹è±¡
                    self.scalers[timeframe] = StandardScaler()
                    X_scaled = self.scalers[timeframe].fit_transform(X)
            else:
                # æ£€æŸ¥scaleræ˜¯å­—å…¸è¿˜æ˜¯StandardScalerå¯¹è±¡
                scaler = self.scalers[timeframe]
                if isinstance(scaler, dict):
                    # å­—å…¸ç»“æ„ï¼šä½¿ç”¨'traditional'é”®çš„scalerï¼ˆä¼ ç»Ÿæ¨¡å‹ï¼‰
                    if 'traditional' in scaler:
                        X_scaled = scaler['traditional'].transform(X)
                    else:
                        # å¦‚æœæ²¡æœ‰'traditional'é”®ï¼Œåˆ›å»ºæ–°çš„scaler
                        logger.warning(f"âš ï¸ {timeframe} scalerå­—å…¸ä¸­ç¼ºå°‘'traditional'é”®ï¼Œåˆ›å»ºæ–°çš„scaler")
                        self.scalers[timeframe]['traditional'] = StandardScaler()
                        X_scaled = self.scalers[timeframe]['traditional'].fit_transform(X)
                else:
                    # StandardScalerå¯¹è±¡ï¼šç›´æ¥ä½¿ç”¨
                    X_scaled = scaler.transform(X)
            
            return X_scaled
            
        except Exception as e:
            logger.error(f"ç‰¹å¾ç¼©æ”¾å¤±è´¥: {e}", exc_info=True)
            return X.values
    
    # æ³¨ï¼š_train_lightgbm() æ–¹æ³•å·²ç§»è‡³ ensemble_ml_service.pyï¼ˆç»Ÿä¸€ä¸‰æ¨¡å‹è®­ç»ƒä»£ç ä½ç½®ï¼‰
    # åŸå®ç°å·²è¢«å­ç±»è¦†ç›–ï¼Œæ­¤å¤„åˆ é™¤ä»¥é¿å…ä»£ç å†—ä½™
    
    def _evaluate_model_for_timeframe(self, X_val: np.ndarray, y_val: np.ndarray, timeframe: str) -> Dict[str, Any]:
        """è¯„ä¼°ç‰¹å®šæ—¶é—´æ¡†æ¶çš„æ¨¡å‹"""
        try:
            model = self.models.get(timeframe)
            if not model:
                raise Exception(f"{timeframe} æ¨¡å‹ä¸å­˜åœ¨")
            
            # é¢„æµ‹
            y_pred = model.predict(X_val)
            y_pred_proba = model.predict_proba(X_val)
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)
            
            # å¤šåˆ†ç±»AUC
            try:
                auc = roc_auc_score(y_val, y_pred_proba, multi_class='ovr')
            except:
                auc = 0.0
            
            # ç‰¹å¾é‡è¦æ€§
            feature_columns = self.feature_columns_dict.get(timeframe, [])
            feature_importance = dict(zip(
                feature_columns, 
                model.feature_importances_
            ))
            
            # æŒ‰é‡è¦æ€§æ’åºï¼Œå¹¶è½¬æ¢ä¸ºPythonåŸç”Ÿfloat
            feature_importance = {
                k: float(v) for k, v in sorted(
                    feature_importance.items(), 
                    key=lambda x: x[1], 
                    reverse=True
                )
            }
            
            # âœ… è½¬æ¢æ‰€æœ‰numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼ˆé˜²æ­¢JSONåºåˆ—åŒ–é”™è¯¯ï¼‰
            metrics = {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1),
                'auc': float(auc),
                'feature_importance': feature_importance,
                'timeframe': timeframe,
                'training_time': datetime.now().isoformat(),
                'version': '2.0'  # å¤šæ—¶é—´æ¡†æ¶ç‰ˆæœ¬
            }
            
            logger.info(f"ğŸ“Š {timeframe} æ¨¡å‹è¯„ä¼°:")
            logger.info(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
            logger.info(f"  ç²¾ç¡®ç‡: {precision:.4f}")
            logger.info(f"  å¬å›ç‡: {recall:.4f}")
            logger.info(f"  F1åˆ†æ•°: {f1:.4f}")
            logger.info(f"  AUC: {auc:.4f}")
            
            return metrics
            
        except Exception as e:
            logger.error(f"{timeframe} æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return {}
    
    async def _save_model(self):
        """ä¿å­˜å¤šæ—¶é—´æ¡†æ¶æ¨¡å‹"""
        try:
            saved_count = 0
            for timeframe in settings.TIMEFRAMES:
                if timeframe in self.models:
                    paths = self._get_model_paths(timeframe)
                    
                    # ä¿å­˜æ¨¡å‹
                    joblib.dump(self.models[timeframe], paths['model'])
                    
                    # ä¿å­˜ç¼©æ”¾å™¨
                    if timeframe in self.scalers:
                        joblib.dump(self.scalers[timeframe], paths['scaler'])
                    
                    # ä¿å­˜ç‰¹å¾åˆ—
                    if timeframe in self.feature_columns_dict:
                        with open(paths['features'], 'wb') as f:
                            pickle.dump(self.feature_columns_dict[timeframe], f)
                    
                    saved_count += 1
                    logger.info(f"âœ… {timeframe} æ¨¡å‹ä¿å­˜å®Œæˆ")
            
            logger.info(f"ğŸ‰ æ‰€æœ‰æ¨¡å‹ä¿å­˜å®Œæˆ ({saved_count}ä¸ªæ—¶é—´æ¡†æ¶)")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
    
    async def _load_model(self):
        """åŠ è½½å¤šæ—¶é—´æ¡†æ¶æ¨¡å‹"""
        try:
            loaded_count = 0
            for timeframe in settings.TIMEFRAMES:
                paths = self._get_model_paths(timeframe)
                
                try:
                    # åŠ è½½æ¨¡å‹
                    if os.path.exists(paths['model']):
                        self.models[timeframe] = joblib.load(paths['model'])
                        loaded_count += 1
                    
                    # åŠ è½½ç¼©æ”¾å™¨
                    if os.path.exists(paths['scaler']):
                        self.scalers[timeframe] = joblib.load(paths['scaler'])
                    
                    # åŠ è½½ç‰¹å¾åˆ—
                    if os.path.exists(paths['features']):
                        with open(paths['features'], 'rb') as f:
                            self.feature_columns_dict[timeframe] = pickle.load(f)
                    
                    if timeframe in self.models:
                        feature_count = len(self.feature_columns_dict.get(timeframe, []))
                
                except Exception as e:
                    logger.warning(f"âš ï¸ {timeframe} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            
            if loaded_count > 0:
                logger.info(f"ğŸ‰ æ¨¡å‹åŠ è½½å®Œæˆ ({loaded_count}/{len(settings.TIMEFRAMES)}ä¸ªæ—¶é—´æ¡†æ¶)")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œéœ€è¦è®­ç»ƒ")
            
            # åŠ è½½æ¨¡å‹æŒ‡æ ‡
            cached_metrics = await cache_manager.get_model_metrics(settings.SYMBOL)
            if cached_metrics:
                self.model_metrics = cached_metrics
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    
    # æ³¨æ„ï¼šè‡ªåŠ¨è®­ç»ƒå¾ªç¯å·²ç§»é™¤ï¼Œæ”¹ç”± scheduler ç»Ÿä¸€ç®¡ç†
    # scheduler ä¼šåœ¨æ¯å¤©00:01è°ƒç”¨ train_model() æ–¹æ³•
    
    async def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯ï¼ˆå¤šæ—¶é—´æ¡†æ¶ç‰ˆæœ¬ï¼‰"""
        try:
            # ç»Ÿè®¡å·²åŠ è½½çš„æ¨¡å‹
            loaded_models = {}
            total_features = 0
            
            for timeframe in settings.TIMEFRAMES:
                is_loaded = timeframe in self.models and self.models[timeframe] is not None
                feature_count = len(self.feature_columns_dict.get(timeframe, []))
                
                loaded_models[timeframe] = {
                    'loaded': is_loaded,
                    'feature_count': feature_count,
                    'model_path': self._get_model_paths(timeframe)['model'] if is_loaded else None
                }
                
                if is_loaded:
                    total_features += feature_count
            
            info = {
                'models_loaded': loaded_models,
                'total_models': len(self.models),
                'expected_models': len(settings.TIMEFRAMES),
                'total_features': total_features,
                'metrics': self.model_metrics,
                'last_training': self.model_metrics.get('training_date', 'Unknown'),
                'version': self.model_metrics.get('version', '2.0')
            }
            
            return info
            
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'total_models': 0,
                'expected_models': len(settings.TIMEFRAMES),
                'error': str(e)
            }