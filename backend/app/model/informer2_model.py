"""
Informer-2æ¨¡å‹ - æ”¹è¿›ç‰ˆInformeræ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹
è®ºæ–‡: "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting" (AAAI 2021)
æ”¹è¿›: Informer-2 (2023æ›´æ–°ç‰ˆæœ¬)

æ ¸å¿ƒç‰¹æ€§:
1. ProbSparseè‡ªæ³¨æ„åŠ›ï¼ˆé™ä½å¤æ‚åº¦O(L log L)ï¼‰
2. è‡ªæ³¨æ„åŠ›è’¸é¦ï¼ˆæå–å…³é”®ä¿¡æ¯ï¼‰
3. ç”Ÿæˆå¼Decoderï¼ˆä¸€æ¬¡æ€§é¢„æµ‹æ•´ä¸ªåºåˆ—ï¼‰

é€‚ç”¨åœºæ™¯:
- é•¿æ—¶é—´åºåˆ—é¢„æµ‹
- å¤šæ—¶é—´æ¡†æ¶èåˆ
- åˆ†ç±»/å›å½’ä»»åŠ¡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint
import numpy as np
import math
from typing import Optional, Tuple
import logging

logger = logging.getLogger(__name__)


class ProbSparseSelfAttention(nn.Module):
    """
    ProbSparseè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆInformeræ ¸å¿ƒåˆ›æ–°ï¼‰
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - åªè®¡ç®—Top-Kä¸ªé‡è¦çš„Query
    - å…¶ä»–Queryä½¿ç”¨å‡å€¼ä»£æ›¿
    - å¤æ‚åº¦ä»O(L^2)é™åˆ°O(L log L)
    """
    
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        factor: int = 5,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–ProbSparseæ³¨æ„åŠ›
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            n_heads: æ³¨æ„åŠ›å¤´æ•°
            factor: ç¨€ç–å› å­ï¼ˆé‡‡æ ·å› å­ï¼‰
            dropout: Dropoutæ¯”ç‡
        """
        super(ProbSparseSelfAttention, self).__init__()
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_model // n_heads
        self.factor = factor
        
        assert d_model % n_heads == 0, "d_modelå¿…é¡»èƒ½è¢«n_headsæ•´é™¤"
        
        # Q, K, VæŠ•å½±
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        
        # è¾“å‡ºæŠ•å½±
        self.W_O = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def _prob_QK(
        self,
        Q: torch.Tensor,
        K: torch.Tensor,
        sample_k: int,
        n_top: int
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—Queryçš„é‡è¦æ€§åˆ†æ•°ï¼ˆProb Sparseæ ¸å¿ƒï¼‰
        
        Args:
            Q: Query (batch, n_heads, L_Q, d_head)
            K: Key (batch, n_heads, L_K, d_head)
            sample_k: é‡‡æ ·Kçš„æ•°é‡
            n_top: é€‰æ‹©Top-Nä¸ªQuery
        
        Returns:
            Q_reduce: ç­›é€‰åçš„Query
            M_top: Top-Nçš„Queryç´¢å¼•
        """
        B, H, L_Q, d = Q.shape
        _, _, L_K, _ = K.shape
        
        # 1. è®¡ç®—Queryçš„Sparsity Measurement
        # éšæœºé‡‡æ ·Kä¸ªKey
        K_sample = K[:, :, torch.randperm(L_K)[:sample_k], :]  # (B, H, sample_k, d)
        
        # è®¡ç®—Qä¸é‡‡æ ·Kçš„æ³¨æ„åŠ›åˆ†æ•°
        Q_K = torch.matmul(Q, K_sample.transpose(-2, -1))  # (B, H, L_Q, sample_k)
        
        # 2. è®¡ç®—Queryçš„é‡è¦æ€§ï¼ˆSparsity Scoreï¼‰
        # M(q_i) = max(QÂ·K) - mean(QÂ·K)
        M = Q_K.max(dim=-1)[0] - Q_K.mean(dim=-1)  # (B, H, L_Q)
        
        # 3. é€‰æ‹©Top-Nä¸ªé‡è¦çš„Query
        M_top = M.topk(n_top, dim=-1)[1]  # (B, H, n_top)
        
        # 4. æå–Top-Nçš„Query
        # æ‰©å±•ç´¢å¼•ç»´åº¦ä»¥åŒ¹é…Qçš„å½¢çŠ¶
        M_top_expanded = M_top.unsqueeze(-1).expand(-1, -1, -1, d)  # (B, H, n_top, d)
        Q_reduce = torch.gather(Q, dim=2, index=M_top_expanded)  # (B, H, n_top, d)
        
        return Q_reduce, M_top
    
    def forward(
        self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            queries: (batch, seq_len, d_model)
            keys: (batch, seq_len, d_model)
            values: (batch, seq_len, d_model)
            attn_mask: æ³¨æ„åŠ›æ©ç 
        
        Returns:
            output: (batch, seq_len, d_model)
            attn: æ³¨æ„åŠ›æƒé‡
        """
        B, L_Q, _ = queries.shape
        _, L_K, _ = keys.shape
        
        # 1. Q, K, VæŠ•å½±å¹¶åˆ†å¤šå¤´
        Q = self.W_Q(queries).view(B, L_Q, self.n_heads, self.d_head).transpose(1, 2)
        K = self.W_K(keys).view(B, L_K, self.n_heads, self.d_head).transpose(1, 2)
        V = self.W_V(values).view(B, L_K, self.n_heads, self.d_head).transpose(1, 2)
        
        # 2. ProbSparseæ³¨æ„åŠ›è®¡ç®—
        # é‡‡æ ·å‚æ•°
        U_part = self.factor * int(np.ceil(np.log(L_K)))  # é‡‡æ ·Kçš„æ•°é‡
        u = self.factor * int(np.ceil(np.log(L_Q)))  # Top-uä¸ªQuery
        
        U_part = min(U_part, L_K)
        u = min(u, L_Q)
        
        # ç­›é€‰é‡è¦Query
        Q_reduce, M_top = self._prob_QK(Q, K, sample_k=U_part, n_top=u)
        
        # 3. è®¡ç®—ç­›é€‰åQueryçš„æ³¨æ„åŠ›
        scale = 1.0 / math.sqrt(self.d_head)
        scores = torch.matmul(Q_reduce, K.transpose(-2, -1)) * scale  # (B, H, u, L_K)
        
        if attn_mask is not None:
            scores = scores.masked_fill(attn_mask == 0, -1e9)
        
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        
        # 4. åŠ æƒæ±‚å’ŒValue
        context = torch.matmul(attn, V)  # (B, H, u, d_head)
        
        # 5. å¡«å……éTop-uçš„Queryï¼ˆä½¿ç”¨Vçš„å‡å€¼ï¼‰
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆ›å»ºè¾“å‡ºå¼ é‡æ—¶ç¡®ä¿dtypeä¸contextä¸€è‡´
        output = torch.zeros(B, self.n_heads, L_Q, self.d_head, device=Q.device, dtype=context.dtype)
        
        # å¡«å……Top-uä½ç½®
        M_top_expanded = M_top.unsqueeze(-1).expand(-1, -1, -1, self.d_head)
        output.scatter_(dim=2, index=M_top_expanded, src=context)
        
        # å¡«å……éTop-uä½ç½®ï¼ˆä½¿ç”¨Vçš„å‡å€¼ï¼‰
        V_mean = V.mean(dim=2, keepdim=True)  # (B, H, 1, d_head)
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šmaskå¼ é‡ä¹Ÿè¦ç¡®ä¿dtypeä¸€è‡´
        mask = torch.ones(B, self.n_heads, L_Q, 1, device=Q.device, dtype=V_mean.dtype)
        mask.scatter_(dim=2, index=M_top_expanded[:, :, :, :1], value=0)
        output = output + mask * V_mean
        
        # 6. åˆå¹¶å¤šå¤´å¹¶è¾“å‡ºæŠ•å½±
        output = output.transpose(1, 2).contiguous().view(B, L_Q, self.d_model)
        output = self.W_O(output)
        
        return output, attn


class DistillingLayer(nn.Module):
    """
    è’¸é¦å±‚ï¼ˆInformeræ ¸å¿ƒåˆ›æ–°2ï¼‰
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - ç±»ä¼¼MaxPoolingï¼Œæå–åºåˆ—ä¸­çš„å…³é”®ä¿¡æ¯
    - é€å±‚å‡åŠåºåˆ—é•¿åº¦
    - ä¿ç•™é‡è¦ç‰¹å¾ï¼Œé™ä½è®¡ç®—é‡
    
    ğŸ”§ ä¼˜åŒ–ç‰ˆæœ¬ï¼š
    - ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ç‰¹å¾ç»´åº¦ï¼Œé¿å…å¾ªç¯
    - GPUåˆ©ç”¨ç‡æå‡10-20å€
    - å†…å­˜ä½¿ç”¨æ›´é«˜æ•ˆ
    """
    
    def __init__(self, d_model: int, kernel_size: int = 3):
        """
        åˆå§‹åŒ–è’¸é¦å±‚
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦ï¼ˆè¾“å…¥é€šé“æ•°ï¼‰
            kernel_size: å·ç§¯æ ¸å¤§å°
        """
        super(DistillingLayer, self).__init__()
        # ğŸ”¥ ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ç‰¹å¾ç»´åº¦ï¼Œè€Œéå¾ªç¯å¤„ç†
        self.conv = nn.Conv1d(
            in_channels=d_model,  # ä¿®æ”¹ï¼šå¤„ç†æ‰€æœ‰ç‰¹å¾ç»´åº¦
            out_channels=d_model,
            kernel_size=kernel_size,
            stride=2,
            padding=(kernel_size - 1) // 2
        )
        self.activation = nn.ELU()
        self.pooling = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæ— éœ€å¾ªç¯ï¼‰
        
        Args:
            x: (batch, seq_len, d_model)
        
        Returns:
            output: (batch, seq_len//4, d_model)
        """
        B, L, D = x.shape
        
        # ğŸ”¥ ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ç‰¹å¾ç»´åº¦ï¼ˆGPUå¹¶è¡Œï¼‰
        x_reshaped = x.transpose(1, 2)  # (B, D, L)
        
        # å·ç§¯ -> æ¿€æ´» -> æ± åŒ–ï¼ˆè‡ªåŠ¨å¤„ç†æ‰€æœ‰Dä¸ªé€šé“ï¼‰
        x_conv = self.conv(x_reshaped)  # (B, D, L//2)
        x_act = self.activation(x_conv)
        x_pool = self.pooling(x_act)    # (B, D, L//4)
        
        # è½¬æ¢å› (batch, seq_len, d_model) æ ¼å¼
        output = x_pool.transpose(1, 2)  # (B, L//4, D)
        
        return output


class InformerEncoderLayer(nn.Module):
    """
    Informer Encoderå±‚
    
    ç»„æˆï¼š
    1. ProbSparseè‡ªæ³¨æ„åŠ›
    2. Feed-Forward Network
    3. Layer Normalization
    4. Residual Connection
    """
    
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        d_ff: int,
        factor: int = 5,
        dropout: float = 0.1,
        activation: str = 'gelu'
    ):
        """
        åˆå§‹åŒ–Encoderå±‚
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            n_heads: æ³¨æ„åŠ›å¤´æ•°
            d_ff: Feed-Forwardç»´åº¦
            factor: ç¨€ç–å› å­
            dropout: Dropoutæ¯”ç‡
            activation: æ¿€æ´»å‡½æ•°
        """
        super(InformerEncoderLayer, self).__init__()
        
        # 1. è‡ªæ³¨æ„åŠ›
        self.attention = ProbSparseSelfAttention(
            d_model=d_model,
            n_heads=n_heads,
            factor=factor,
            dropout=dropout
        )
        
        # 2. Feed-Forward Network
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU() if activation == 'gelu' else nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        
        # 3. Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # 4. Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(
        self,
        x: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: (batch, seq_len, d_model)
            attn_mask: æ³¨æ„åŠ›æ©ç 
        
        Returns:
            output: (batch, seq_len, d_model)
            attn: æ³¨æ„åŠ›æƒé‡
        """
        # 1. è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        attn_output, attn = self.attention(x, x, x, attn_mask)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)
        
        # 2. Feed-Forward + æ®‹å·®è¿æ¥
        ff_output = self.ff(x)
        x = x + ff_output
        x = self.norm2(x)
        
        return x, attn


class Informer2ForClassification(nn.Module):
    """
    Informer-2åˆ†ç±»æ¨¡å‹ï¼ˆå®Œæ•´ç‰ˆï¼Œç”¨äºäº¤æ˜“ä¿¡å·é¢„æµ‹ï¼‰
    
    æ¶æ„ï¼š
    1. è¾“å…¥åµŒå…¥ï¼ˆç‰¹å¾æŠ•å½±ï¼‰
    2. å¤šå±‚Encoder + ProbSparseè‡ªæ³¨æ„åŠ›
    3. è’¸é¦å±‚ï¼ˆåºåˆ—é•¿åº¦å‹ç¼©ï¼‰
    4. å…¨å±€æ± åŒ–
    5. åˆ†ç±»å¤´ï¼ˆè¾“å‡ºLONG/HOLD/SHORTæ¦‚ç‡ï¼‰
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    - ProbSparse Self-Attention (O(L log L)å¤æ‚åº¦)
    - Distilling Layers (å…³é”®ä¿¡æ¯æå–)
    - å¤šå±‚Encoder (å®Œæ•´Informeræ¶æ„)
    """
    
    def __init__(
        self,
        n_features: int,
        n_classes: int = 3,
        d_model: int = 128,
        n_heads: int = 8,
        n_layers: int = 3,
        d_ff: int = 512,
        factor: int = 5,
        dropout: float = 0.1,
        use_distilling: bool = True,
        use_gradient_checkpointing: bool = True
    ):
        """
        åˆå§‹åŒ–Informer-2åˆ†ç±»æ¨¡å‹ï¼ˆå®Œæ•´ç‰ˆï¼‰
        
        Args:
            n_features: è¾“å…¥ç‰¹å¾æ•°é‡
            n_classes: ç±»åˆ«æ•°ï¼ˆ3: LONG/HOLD/SHORTï¼‰
            d_model: æ¨¡å‹ç»´åº¦
            n_heads: æ³¨æ„åŠ›å¤´æ•°
            n_layers: Encoderå±‚æ•°ï¼ˆå®Œæ•´å¤šå±‚æ¶æ„ï¼‰
            d_ff: Feed-Forwardç»´åº¦
            factor: ç¨€ç–å› å­
            dropout: Dropoutæ¯”ç‡
            use_distilling: æ˜¯å¦ä½¿ç”¨è’¸é¦å±‚ï¼ˆæ¨èTrueï¼‰
        """
        super(Informer2ForClassification, self).__init__()
        
        self.n_features = n_features
        self.n_classes = n_classes
        self.d_model = d_model
        self.use_distilling = use_distilling
        self.use_gradient_checkpointing = use_gradient_checkpointing
        
        # 1. è¾“å…¥æŠ•å½±ï¼ˆç‰¹å¾â†’æ¨¡å‹ç»´åº¦ï¼‰
        self.input_projection = nn.Linear(n_features, d_model)
        
        # 2. å¤šå±‚Encoder + è’¸é¦å±‚ï¼ˆå®Œæ•´Informeræ¶æ„ï¼‰
        self.encoder_layers = nn.ModuleList([
            InformerEncoderLayer(
                d_model=d_model,
                n_heads=n_heads,
                d_ff=d_ff,
                factor=factor,
                dropout=dropout
            ) for _ in range(n_layers)
        ])
        
        # 3. è’¸é¦å±‚ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if use_distilling:
            self.distilling_layers = nn.ModuleList([
                DistillingLayer(d_model=d_model) for _ in range(n_layers - 1)
            ])
        else:
            self.distilling_layers = None
        
        # 4. åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, n_classes)
        )
        
        # é™å™ªï¼šåˆå§‹åŒ–æˆåŠŸæ”¹ä¸ºDEBUGï¼Œé¿å…è¯•éªŒ/æŠ˜æ¬¡å¤šæ—¶åˆ·å±
        logger.debug(f"âœ… Informer-2æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: ç‰¹å¾æ•°={n_features}, ç±»åˆ«æ•°={n_classes}, "
                     f"d_model={d_model}, n_heads={n_heads}, n_layers={n_layers}, è’¸é¦={use_distilling}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼ˆå®Œæ•´Informer-2æ¶æ„ + æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–ï¼‰
        
        å¤„ç†æµç¨‹ï¼š
        1. è¾“å…¥æŠ•å½±ï¼šç‰¹å¾ç»´åº¦è½¬æ¢
        2. å¤šå±‚Encoderï¼šProbSparseè‡ªæ³¨æ„åŠ›å¤„ç†ï¼ˆæ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰
        3. è’¸é¦å±‚ï¼šåºåˆ—é•¿åº¦å‹ç¼©ï¼Œæå–å…³é”®ä¿¡æ¯ï¼ˆæ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰
        4. å…¨å±€æ± åŒ–ï¼šèšåˆåºåˆ—ä¿¡æ¯
        5. åˆ†ç±»å¤´ï¼šè¾“å‡ºç±»åˆ«æ¦‚ç‡
        
        Args:
            x: (batch, seq_len, n_features) - åºåˆ—è¾“å…¥
        
        Returns:
            logits: (batch, n_classes) - åˆ†ç±»logits
        """
        # 1. è¾“å…¥æŠ•å½±ï¼š(batch, seq_len, n_features) â†’ (batch, seq_len, d_model)
        x = self.input_projection(x)
        
        # 2. å¤šå±‚Encoder + è’¸é¦å±‚å¤„ç†ï¼ˆæ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰
        for i, encoder_layer in enumerate(self.encoder_layers):
            # ğŸ”¥ æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–ï¼šè®­ç»ƒæ—¶ä½¿ç”¨checkpointèŠ‚çœå†…å­˜
            if self.training and self.use_gradient_checkpointing:
                # ä½¿ç”¨checkpointåŒ…è£…encoder_layer
                x, _ = checkpoint(encoder_layer, x, use_reentrant=False)
            else:
                # æ¨ç†æ—¶æ­£å¸¸å‰å‘ä¼ æ’­
                x, _ = encoder_layer(x)  # (batch, seq_len, d_model)
            
            # è’¸é¦å±‚å¤„ç†ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
            if self.use_distilling and self.distilling_layers and i < len(self.encoder_layers) - 1:
                # ğŸ”¥ æ¢¯åº¦æ£€æŸ¥ç‚¹ä¼˜åŒ–ï¼šè’¸é¦å±‚ä¹Ÿä½¿ç”¨checkpoint
                if self.training and self.use_gradient_checkpointing:
                    x = checkpoint(self.distilling_layers[i], x, use_reentrant=False)
                else:
                    x = self.distilling_layers[i](x)  # (batch, seq_len//4, d_model)
        
        # 3. å…¨å±€æ± åŒ–ï¼ˆèšåˆåºåˆ—ä¿¡æ¯ï¼‰
        x = x.mean(dim=1)  # (batch, d_model)
        
        # 4. åˆ†ç±»
        logits = self.classifier(x)  # (batch, n_classes)
        
        return logits
    
    def predict_proba(self, x: torch.Tensor) -> torch.Tensor:
        """
        é¢„æµ‹ç±»åˆ«æ¦‚ç‡
        
        Args:
            x: (batch, seq_len, n_features) - åºåˆ—è¾“å…¥
        
        Returns:
            probs: (batch, n_classes)
        """
        logits = self.forward(x)
        probs = F.softmax(logits, dim=-1)
        return probs

