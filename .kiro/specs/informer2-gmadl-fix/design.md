# è®¾è®¡æ–‡æ¡£ - Informer-2 + GMADL é‡åŒ–æ¨¡å‹ä¼˜åŒ–

## æ¦‚è¿°

æœ¬è®¾è®¡æ–‡æ¡£é’ˆå¯¹åŠ å¯†è´§å¸è‡ªåŠ¨äº¤æ˜“ç³»ç»Ÿä¸­Informer-2æ¨¡å‹çš„ç»´åº¦é”™è¯¯é—®é¢˜ï¼Œæä¾›è¯¦ç»†çš„æŠ€æœ¯åˆ†æå’Œè§£å†³æ–¹æ¡ˆã€‚ç³»ç»Ÿä½¿ç”¨å››æ¨¡å‹Stackingé›†æˆï¼ˆLightGBM + XGBoost + CatBoost + Informer-2ï¼‰è¿›è¡Œäº¤æ˜“ä¿¡å·é¢„æµ‹ã€‚

### å½“å‰é—®é¢˜è¯Šæ–­

æ ¹æ®é”™è¯¯æ—¥å¿—åˆ†æï¼Œæ ¸å¿ƒé—®é¢˜æ˜¯ï¼š

```
ERROR - Informer-2è®­ç»ƒå¤±è´¥: max(): Expected reduction dim 3 to have non-zero size.
é”™è¯¯ä½ç½®: ProbSparseSelfAttention._prob_QK() æ–¹æ³•ä¸­çš„ M = Q_K.max(dim=-1)[0]
```

**æ•°æ®æµç¨‹åˆ†æ**ï¼š
1. **åŸå§‹æ•°æ®**ï¼šKçº¿åºåˆ—ï¼ˆtimestamp, open, high, low, close, volumeï¼‰
2. **ç‰¹å¾å·¥ç¨‹**ï¼šæ¯æ ¹Kçº¿ â†’ 82ä¸ªæŠ€æœ¯æŒ‡æ ‡ï¼ˆRSIã€MACDã€å‡çº¿ç­‰ï¼‰
3. **æ¨¡å‹è¾“å…¥**ï¼šæ¯ä¸ªæ ·æœ¬æ˜¯å•æ ¹Kçº¿çš„ç‰¹å¾å‘é‡ `(batch, 82_features)`
4. **é—®é¢˜**ï¼šInformer-2æœŸæœ›åºåˆ—è¾“å…¥ `(batch, seq_len, features)`ï¼Œä½†æ”¶åˆ°çš„æ˜¯ `(batch, features)`

**æ ¹æœ¬åŸå› **ï¼š
1. Informer-2æ˜¯ä¸ºé•¿åºåˆ—æ—¶é—´åºåˆ—é¢„æµ‹è®¾è®¡çš„ï¼ŒæœŸæœ›è¾“å…¥å½¢çŠ¶ä¸º `(batch, seq_len, features)`
2. å½“å‰å®ç°æ¥æ”¶çš„æ˜¯å•æ—¶é—´ç‚¹ç‰¹å¾ `(batch, 82)`ï¼Œé€šè¿‡ `unsqueeze(1)` å¼ºè¡Œæ·»åŠ  `seq_len=1`
3. å½“ `seq_len=1` æ—¶ï¼ŒProbSparseæ³¨æ„åŠ›çš„é‡‡æ ·å‚æ•° `sample_k = factor * ceil(log(1)) = 5 * 0 = 0`
4. å¯¼è‡´ `K_sample` ä¸ºç©ºå¼ é‡ï¼Œ`Q_K.max(dim=-1)` åœ¨ç©ºç»´åº¦ä¸Šæ“ä½œè§¦å‘é”™è¯¯

**ä¸ºä»€ä¹ˆä¸æ˜¯åºåˆ—è¾“å…¥**ï¼š
- è™½ç„¶åŸå§‹æ•°æ®æ˜¯Kçº¿åºåˆ—ï¼Œä½†ç‰¹å¾å·¥ç¨‹å°†æ¯æ ¹Kçº¿è½¬æ¢æˆç‹¬ç«‹çš„ç‰¹å¾å‘é‡
- æŠ€æœ¯æŒ‡æ ‡ï¼ˆå¦‚RSI_14ã€SMA_20ï¼‰å·²ç»åŒ…å«äº†å†å²ä¿¡æ¯ï¼ˆé€šè¿‡æ»šåŠ¨çª—å£è®¡ç®—ï¼‰
- æ¨¡å‹è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç‰¹å¾å‘é‡ï¼Œä¸æ˜¯åºåˆ—

## æ¶æ„è®¾è®¡

### æ–¹æ¡ˆé€‰æ‹©

ç»è¿‡åˆ†æï¼Œæä¾›ä¸‰ç§è§£å†³æ–¹æ¡ˆï¼š

#### æ–¹æ¡ˆAï¼šç®€åŒ–Informer-2ï¼ˆæ¨è - çŸ­æœŸä¿®å¤ï¼‰

**è®¾è®¡æ€è·¯**ï¼š
- ç§»é™¤ProbSparseæ³¨æ„åŠ›ï¼Œä½¿ç”¨æ ‡å‡†MultiHeadAttention
- ç§»é™¤è’¸é¦å±‚ï¼ˆDistilling Layerï¼‰
- ç®€åŒ–ä¸ºå•å±‚Transformer Encoder
- ä¿ç•™GMADLæŸå¤±å‡½æ•°

**ä¼˜ç‚¹**ï¼š
- å¿«é€Ÿä¿®å¤ï¼Œæ— éœ€é‡æ„æ•°æ®ç®¡é“
- ä¿ç•™Transformerçš„ç‰¹å¾æå–èƒ½åŠ›
- ç»§ç»­ä½¿ç”¨GMADLæŸå¤±å‡½æ•°çš„ä¼˜åŠ¿

**ç¼ºç‚¹**ï¼š
- å¤±å»Informerçš„æ ¸å¿ƒåˆ›æ–°ï¼ˆProbSparseã€è’¸é¦ï¼‰
- æœ¬è´¨ä¸Šå˜æˆæ™®é€šTransformer

**å®ç°å¤æ‚åº¦**ï¼šä½

#### æ–¹æ¡ˆBï¼šæ„é€ åºåˆ—è¾“å…¥ï¼ˆæ¨è - ä¸­æœŸä¼˜åŒ–ï¼‰

**è®¾è®¡æ€è·¯**ï¼š
- é‡æ–°è®¾è®¡æ•°æ®ç®¡é“ï¼Œæ„é€ æ—¶é—´åºåˆ—è¾“å…¥
- ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼Œå°†è¿‡å»Nä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾ç»„åˆæˆåºåˆ—
- å……åˆ†åˆ©ç”¨Informer-2çš„é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›

**ä¼˜ç‚¹**ï¼š
- å……åˆ†å‘æŒ¥Informer-2çš„è®¾è®¡ä¼˜åŠ¿
- åˆ©ç”¨å†å²æ—¶é—´åºåˆ—ä¿¡æ¯æå‡é¢„æµ‹
- ä¿ç•™æ‰€æœ‰åˆ›æ–°ç»„ä»¶ï¼ˆProbSparseã€è’¸é¦ï¼‰

**ç¼ºç‚¹**ï¼š
- éœ€è¦é‡æ„æ•°æ®ç®¡é“
- è®­ç»ƒæ ·æœ¬å‡å°‘ï¼ˆå‰Nä¸ªæ ·æœ¬æ— æ³•ä½¿ç”¨ï¼‰
- æ¨ç†æ—¶éœ€è¦Nä¸ªå†å²æ—¶é—´æ­¥

**å®ç°å¤æ‚åº¦**ï¼šä¸­

#### æ–¹æ¡ˆCï¼šæ›¿æ¢ä¸ºTabNetï¼ˆæ¨è - é•¿æœŸä¼˜åŒ–ï¼‰

**è®¾è®¡æ€è·¯**ï¼š
- ä½¿ç”¨Googleçš„TabNetæ¨¡å‹æ›¿ä»£Informer-2
- TabNetä¸“ä¸ºè¡¨æ ¼æ•°æ®è®¾è®¡ï¼Œå¤©ç„¶é€‚åˆå•æ—¶é—´ç‚¹ç‰¹å¾åˆ†ç±»
- æä¾›ç‰¹å¾é‡è¦æ€§è§£é‡Šèƒ½åŠ›

**ä¼˜ç‚¹**ï¼š
- æ¶æ„å®Œå…¨åŒ¹é…ä»»åŠ¡éœ€æ±‚
- æ€§èƒ½ä¼˜äºä¼ ç»ŸGBDT
- å¯è§£é‡Šæ€§å¼º

**ç¼ºç‚¹**ï¼š
- éœ€è¦å¼•å…¥æ–°ä¾èµ–ï¼ˆpytorch-tabnetï¼‰
- éœ€è¦é‡æ–°è®­ç»ƒå’Œè°ƒä¼˜

**å®ç°å¤æ‚åº¦**ï¼šä¸­

### æ¨èå®æ–½è·¯çº¿

**ç”¨æˆ·é€‰æ‹©ï¼šæ–¹æ¡ˆB - æ„é€ åºåˆ—è¾“å…¥**

è¿™æ˜¯æœ€èƒ½å‘æŒ¥Informer-2ä¼˜åŠ¿çš„æ–¹æ¡ˆï¼Œå……åˆ†åˆ©ç”¨å†å²æ—¶é—´åºåˆ—ä¿¡æ¯æå‡é¢„æµ‹å‡†ç¡®ç‡ã€‚

## ç»„ä»¶è®¾è®¡

### 1. åºåˆ—è¾“å…¥æ„é€ ï¼ˆæ–¹æ¡ˆB - ç”¨æˆ·é€‰æ‹©ï¼‰

#### 1.1 è®¾è®¡æ€è·¯

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼Œå°†è¿‡å»Næ ¹Kçº¿çš„ç‰¹å¾ç»„åˆæˆåºåˆ—
- æ¯ä¸ªæ ·æœ¬ä» `(82_features)` å˜æˆ `(seq_len, 82_features)`
- å……åˆ†åˆ©ç”¨Informer-2çš„é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›

**åºåˆ—é•¿åº¦é€‰æ‹©**ï¼š
```python
seq_len_config = {
    '15m': 96,   # 96 Ã— 15åˆ†é’Ÿ = 24å°æ—¶
    '2h': 48,    # 48 Ã— 2å°æ—¶ = 4å¤©
    '4h': 24     # 24 Ã— 4å°æ—¶ = 4å¤©
}
```

**ä¼˜ç‚¹**ï¼š
- ä¿ç•™Informer-2çš„æ‰€æœ‰åˆ›æ–°ç»„ä»¶ï¼ˆProbSparseã€è’¸é¦ï¼‰
- åˆ©ç”¨å†å²æ—¶é—´åºåˆ—ä¿¡æ¯ï¼Œæå‡é¢„æµ‹å‡†ç¡®ç‡
- ç¬¦åˆInformer-2çš„è®¾è®¡åˆè¡·

**ç¼ºç‚¹**ï¼š
- è®­ç»ƒæ ·æœ¬å‡å°‘ï¼ˆå‰Nä¸ªæ ·æœ¬æ— æ³•ä½¿ç”¨ï¼‰
- æ¨ç†æ—¶éœ€è¦Nä¸ªå†å²æ—¶é—´æ­¥
- éœ€è¦é‡æ„æ•°æ®ç®¡é“

#### 1.2 æ•°æ®ç®¡é“é‡æ„

**æ–°å¢æ–¹æ³•ï¼šæ„é€ åºåˆ—è¾“å…¥**

```python
def _create_sequence_input(
    self,
    df: pd.DataFrame,
    seq_len: int,
    timeframe: str
) -> Tuple[np.ndarray, np.ndarray]:
    """
    æ„é€ åºåˆ—è¾“å…¥
    
    Args:
        df: ç‰¹å¾å·¥ç¨‹åçš„DataFrameï¼ˆåŒ…å«labelåˆ—ï¼‰
        seq_len: åºåˆ—é•¿åº¦
        timeframe: æ—¶é—´æ¡†æ¶
    
    Returns:
        X_seq: (n_samples, seq_len, n_features)
        y: (n_samples,)
    """
    feature_columns = self.feature_columns_dict.get(timeframe, [])
    
    X_list = []
    y_list = []
    
    # æ»‘åŠ¨çª—å£
    for i in range(seq_len, len(df)):
        # å–è¿‡å»seq_lenä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾
        X_window = df.iloc[i-seq_len:i][feature_columns].values
        y_label = df.iloc[i]['label']
        
        X_list.append(X_window)
        y_list.append(y_label)
    
    X_seq = np.array(X_list)  # (n_samples, seq_len, n_features)
    y = np.array(y_list)      # (n_samples,)
    
    logger.info(f"âœ… åºåˆ—è¾“å…¥æ„é€ å®Œæˆ: {X_seq.shape}")
    return X_seq, y
```

**ä¿®æ”¹è®­ç»ƒæµç¨‹**ï¼š

```python
async def _train_ensemble_single_timeframe(self, timeframe: str):
    # ... å‰é¢çš„æ•°æ®å‡†å¤‡ä»£ç  ...
    
    # ç‰¹å¾å·¥ç¨‹
    data_lgb = self.feature_engineer.create_features(data_lgb)
    data_lgb = self._create_labels(data_lgb, timeframe=timeframe)
    
    # ğŸ†• æ„é€ åºåˆ—è¾“å…¥ï¼ˆä»…ç”¨äºInformer-2ï¼‰
    seq_len = self.seq_len_config[timeframe]
    X_seq, y_seq = self._create_sequence_input(data_lgb, seq_len, timeframe)
    
    # æ—¶é—´åºåˆ—åˆ†å‰²
    split_idx = int(len(X_seq) * 0.8)
    X_seq_train, X_seq_val = X_seq[:split_idx], X_seq[split_idx:]
    y_seq_train, y_seq_val = y_seq[:split_idx], y_seq[split_idx:]
    
    # è®­ç»ƒInformer-2ï¼ˆä½¿ç”¨åºåˆ—è¾“å…¥ï¼‰
    inf_model = self._train_informer2(X_seq_train, y_seq_train, timeframe)
    
    # ... åç»­ä»£ç  ...
```

#### 1.3 Informer-2æ¨¡å‹é€‚é…

**ä¿®æ”¹forwardæ–¹æ³•**ï¼š

```python
class Informer2ForClassification(nn.Module):
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: (batch, seq_len, n_features) - åºåˆ—è¾“å…¥
        
        Returns:
            logits: (batch, n_classes) - åˆ†ç±»logits
        """
        # 1. è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # (batch, seq_len, d_model)
        
        # 2. Encoderå¤„ç†ï¼ˆä¿ç•™ProbSparseå’Œè’¸é¦ï¼‰
        for encoder_layer in self.encoder_layers:
            x, _ = encoder_layer(x)  # (batch, seq_len, d_model)
            
            # è’¸é¦å±‚ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_distilling:
                x = self.distilling_layer(x)  # (batch, seq_len//2, d_model)
        
        # 3. å…¨å±€æ± åŒ–ï¼ˆèšåˆåºåˆ—ä¿¡æ¯ï¼‰
        x = x.mean(dim=1)  # (batch, d_model)
        
        # 4. åˆ†ç±»
        logits = self.classifier(x)  # (batch, n_classes)
        
        return logits
```

**å…³é”®ä¿®æ”¹**ï¼š
- ç§»é™¤ `unsqueeze(1)`ï¼Œç›´æ¥æ¥æ”¶åºåˆ—è¾“å…¥
- ProbSparseæ³¨æ„åŠ›æ­£å¸¸å·¥ä½œï¼ˆseq_len >= 24ï¼‰
- è’¸é¦å±‚æ­£å¸¸å·¥ä½œï¼ˆseq_lené€å±‚å‡åŠï¼‰

#### 1.4 æ¨ç†æµç¨‹é€‚é…

**ä¿®æ”¹predictæ–¹æ³•**ï¼š

```python
async def predict(self, data: pd.DataFrame, timeframe: str):
    # ç‰¹å¾å·¥ç¨‹
    processed_data = feature_engineer.create_features(data.copy())
    
    # ğŸ†• æ„é€ åºåˆ—è¾“å…¥ï¼ˆå–æœ€æ–°seq_lenä¸ªæ—¶é—´æ­¥ï¼‰
    seq_len = self.seq_len_config[timeframe]
    
    if len(processed_data) < seq_len:
        raise Exception(f"æ•°æ®ä¸è¶³ï¼šéœ€è¦{seq_len}ä¸ªæ—¶é—´æ­¥ï¼Œå®é™…{len(processed_data)}ä¸ª")
    
    # å–æœ€æ–°seq_lenä¸ªæ—¶é—´æ­¥
    latest_seq = processed_data.iloc[-seq_len:][feature_columns].values
    latest_seq = latest_seq.reshape(1, seq_len, -1)  # (1, seq_len, n_features)
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_tensor = torch.FloatTensor(latest_seq).to(device)
    
    # é¢„æµ‹
    with torch.no_grad():
        logits = inf_model(X_tensor)
        probs = F.softmax(logits, dim=-1)[0].cpu().numpy()
    
    # ... åç»­ä»£ç  ...
```

### 2. ç®€åŒ–Informer-2æ¨¡å‹ï¼ˆæ–¹æ¡ˆA - å¤‡é€‰ï¼‰

#### 1.1 ç§»é™¤ProbSparseæ³¨æ„åŠ›

**å½“å‰å®ç°**ï¼š
```python
class ProbSparseSelfAttention(nn.Module):
    def _prob_QK(self, Q, K, sample_k, n_top):
        # é—®é¢˜ï¼šå½“sample_k=0æ—¶ï¼ŒK_sampleä¸ºç©º
        K_sample = K[:, :, torch.randperm(L_K)[:sample_k], :]
        Q_K = torch.matmul(Q, K_sample.transpose(-2, -1))
        M = Q_K.max(dim=-1)[0]  # é”™è¯¯å‘ç”Ÿåœ¨è¿™é‡Œ
```

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# ä½¿ç”¨PyTorchæ ‡å‡†MultiHeadAttentionæ›¿ä»£
self.attention = nn.MultiheadAttention(
    embed_dim=d_model,
    num_heads=n_heads,
    dropout=dropout,
    batch_first=True
)
```

#### 1.2 ç§»é™¤è’¸é¦å±‚

**å½“å‰å®ç°**ï¼š
```python
class DistillingLayer(nn.Module):
    def forward(self, x):
        # é—®é¢˜ï¼šå½“L=1æ—¶ï¼ŒL//4=0ï¼Œåºåˆ—æ¶ˆå¤±
        output = self.pooling(feature)  # MaxPool1d(kernel_size=3, stride=2)
```

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
```python
# ç›´æ¥ç§»é™¤è’¸é¦å±‚ï¼Œä¸è¿›è¡Œåºåˆ—é•¿åº¦å‹ç¼©
# åœ¨Informer2ForClassificationä¸­è®¾ç½® use_distilling=False
```

#### 1.3 ç®€åŒ–Encoderå±‚

**ä¿®å¤åçš„æ¶æ„**ï¼š
```python
class SimplifiedInformer2(nn.Module):
    def __init__(self, n_features, n_classes=3, d_model=128, n_heads=8, dropout=0.1):
        super().__init__()
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(n_features, d_model)
        
        # æ ‡å‡†Transformer Encoderï¼ˆå•å±‚ï¼‰
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, n_classes)
        )
    
    def forward(self, x):
        # x: (batch, n_features)
        x = self.input_projection(x)  # (batch, d_model)
        x = x.unsqueeze(1)  # (batch, 1, d_model)
        x = self.encoder(x)  # (batch, 1, d_model)
        x = x.squeeze(1)  # (batch, d_model)
        logits = self.classifier(x)  # (batch, n_classes)
        return logits
```

### 2. GMADLæŸå¤±å‡½æ•°ä¼˜åŒ–

#### 2.1 å½“å‰å®ç°åˆ†æ

**æ•°å­¦å…¬å¼**ï¼š
```
loss = (|error|^beta) / (alpha + |error|^(1-beta))
å…¶ä¸­ error = 1 - P(correct_class)
```

**å‚æ•°åˆ†æ**ï¼š
- `alpha=1.0`: æ§åˆ¶å¯¹å¼‚å¸¸å€¼çš„é²æ£’æ€§ï¼ˆè¶Šå¤§è¶Šé²æ£’ï¼‰
- `beta=0.5`: æ§åˆ¶æŸå¤±çš„å‡¸æ€§ï¼ˆ0.5-1.0ï¼Œè¶Šå°è¶Šå…³æ³¨éš¾åˆ†æ ·æœ¬ï¼‰

**è¯„ä¼°ç»“è®º**ï¼šå®ç°æ­£ç¡®ï¼Œå‚æ•°åˆç†

#### 2.2 HOLDæƒ©ç½šæœºåˆ¶

**å½“å‰å®ç°**ï¼š
```python
hold_weights = torch.where(
    targets == 1,  # HOLDç±»åˆ«
    torch.tensor(0.65),  # æƒ©ç½šç³»æ•°
    torch.tensor(1.0)
)
weighted_loss = loss * hold_weights
```

**è¯„ä¼°ç»“è®º**ï¼š
- æƒ©ç½šç³»æ•°0.65åˆç†ï¼ˆé™ä½HOLDç±»åˆ«çš„æŸå¤±æƒé‡ï¼‰
- æœ‰æ•ˆå‡å°‘è¿‡åº¦é¢„æµ‹HOLDä¿¡å·
- ä¸Optunaä¼˜åŒ–ä¸­çš„HOLDæƒ©ç½šä¿æŒä¸€è‡´

### 3. Optunaè¶…å‚æ•°ä¼˜åŒ–

#### 3.1 æœç´¢ç©ºé—´è®¾è®¡

**å½“å‰é…ç½®åˆ†æ**ï¼š

| æ—¶é—´æ¡†æ¶ | æ ·æœ¬æ•° | æ¨¡å‹å¤æ‚åº¦ | æ­£åˆ™åŒ–å¼ºåº¦ | è¯„ä¼° |
|---------|--------|-----------|-----------|------|
| 15m | å¤šï¼ˆ~27kï¼‰ | é«˜ï¼ˆdepth=6-12ï¼‰ | ä½ï¼ˆreg=0-0.5ï¼‰ | âœ… åˆç† |
| 2h | ä¸­ï¼ˆ~6kï¼‰ | ä¸­ï¼ˆdepth=3-6ï¼‰ | ä¸­ï¼ˆreg=0.5-1.2ï¼‰ | âœ… åˆç† |
| 4h | å°‘ï¼ˆ~3kï¼‰ | ä½ï¼ˆdepth=2-5ï¼‰ | é«˜ï¼ˆreg=0.8-1.5ï¼‰ | âœ… åˆç† |

**è®¾è®¡åŸåˆ™**ï¼š
1. æ ·æœ¬è¶Šå°‘ï¼Œæ¨¡å‹è¶Šç®€å•ï¼Œæ­£åˆ™åŒ–è¶Šå¼º
2. é˜²æ­¢è¿‡æ‹Ÿåˆï¼š2h/4hä½¿ç”¨æ›´å¼ºçš„æ­£åˆ™åŒ–
3. å·®å¼‚åŒ–é…ç½®ï¼šä¸åŒæ—¶é—´æ¡†æ¶ä½¿ç”¨ä¸åŒæœç´¢ç©ºé—´

#### 3.2 Informer-2æœç´¢ç©ºé—´

**å½“å‰é…ç½®**ï¼š
```python
# 15mæ—¶é—´æ¡†æ¶
'd_model': [64, 128, 256]
'n_heads': [4, 8, 16]
'n_layers': [2, 3, 4]
'epochs': [20, 40]
'batch_size': [128, 256, 512]
'lr': [0.0005, 0.005]
'dropout': [0.05, 0.2]
'alpha': [0.5, 2.0]  # GMADLå‚æ•°
'beta': [0.3, 0.7]   # GMADLå‚æ•°
```

**è¯„ä¼°ç»“è®º**ï¼š
- âœ… d_modelèŒƒå›´åˆç†ï¼ˆ64-256ï¼‰
- âœ… n_headsä¸d_modelåŒ¹é…ï¼ˆd_modelå¿…é¡»èƒ½è¢«n_headsæ•´é™¤ï¼‰
- âš ï¸ n_layerså¯èƒ½è¿‡å¤šï¼ˆç®€åŒ–ååº”è®¾ä¸º1ï¼‰
- âœ… epochsåˆç†ï¼ˆ20-40è½®ï¼ŒGPUåŠ é€Ÿä¸‹å¯æ¥å—ï¼‰
- âœ… batch_sizeåˆç†ï¼ˆ128-512ï¼‰
- âœ… å­¦ä¹ ç‡èŒƒå›´åˆç†ï¼ˆ0.0005-0.005ï¼‰
- âœ… dropoutèŒƒå›´åˆç†ï¼ˆ0.05-0.2ï¼‰
- âœ… GMADLå‚æ•°èŒƒå›´åˆç†

#### 3.3 è¯•éªŒæ¬¡æ•°å’Œè¶…æ—¶é…ç½®

**å½“å‰é…ç½®**ï¼š
```python
# ä¼ ç»Ÿæ¨¡å‹ï¼ˆLightGBM/XGBoost/CatBoostï¼‰
optuna_n_trials = 100
optuna_timeout = 1800  # 30åˆ†é’Ÿ

# Informer-2
informer_n_trials = 50
informer_timeout = 1200  # 20åˆ†é’Ÿ
```

**è¯„ä¼°ç»“è®º**ï¼š
- âœ… ä¼ ç»Ÿæ¨¡å‹ï¼š100æ¬¡è¯•éªŒ + 30åˆ†é’Ÿè¶…æ—¶ï¼Œå¹³è¡¡æ•ˆç‡å’Œæ•ˆæœ
- âœ… Informer-2ï¼š50æ¬¡è¯•éªŒ + 20åˆ†é’Ÿè¶…æ—¶ï¼Œè€ƒè™‘æ·±åº¦å­¦ä¹ è®­ç»ƒæ—¶é—´
- âœ… GPUåŠ é€Ÿä¸‹ï¼Œæ—¶é—´é…ç½®åˆç†
- âœ… ä½¿ç”¨TimeSeriesSplit 5æŠ˜äº¤å‰éªŒè¯ï¼Œè¯„ä¼°å¯é 

### 4. GPUé…ç½®éªŒè¯

#### 4.1 å½“å‰é…ç½®

```python
# config.py
USE_GPU = True
GPU_DEVICE = "cuda:0"

# ensemble_ml_service.py
self.use_gpu = settings.USE_GPU
self.gpu_device = settings.GPU_DEVICE
```

#### 4.2 å„æ¨¡å‹GPUé…ç½®

**LightGBM**ï¼š
```python
if self.use_gpu:
    base_params['device'] = 'gpu'
    base_params['gpu_platform_id'] = 0
    base_params['gpu_device_id'] = 0
```
âœ… é…ç½®æ­£ç¡®

**XGBoost**ï¼š
```python
if self.use_gpu:
    base_params['tree_method'] = 'gpu_hist'
    base_params['gpu_id'] = 0
```
âœ… é…ç½®æ­£ç¡®

**CatBoost**ï¼š
```python
if self.use_gpu:
    base_params['task_type'] = 'GPU'
    base_params['devices'] = '0'
```
âœ… é…ç½®æ­£ç¡®

**Informer-2**ï¼š
```python
device = torch.device('cuda:0' if self.use_gpu and torch.cuda.is_available() else 'cpu')
model = Informer2ForClassification(...).to(device)
X_tensor = torch.FloatTensor(X).to(device)
```
âœ… é…ç½®æ­£ç¡®

### 5. å…¶ä»–æ¨¡å‹å‚æ•°ä¼˜åŒ–

#### 5.1 å…ƒå­¦ä¹ å™¨é…ç½®

**å½“å‰é…ç½®**ï¼š
```python
meta_learner = lgb.LGBMClassifier(
    n_estimators=50,     # æ ‘æ•°é‡
    max_depth=3,         # æ ‘æ·±åº¦
    learning_rate=0.15,  # å­¦ä¹ ç‡
    num_leaves=7,        # å¶å­æ•°
    min_child_samples=30,  # æœ€å°æ ·æœ¬æ•°
    subsample=0.7,       # è¡Œé‡‡æ ·
    colsample_bytree=0.7,  # åˆ—é‡‡æ ·
    reg_alpha=0.3,       # L1æ­£åˆ™
    reg_lambda=0.3,      # L2æ­£åˆ™
)
```

**è¯„ä¼°ç»“è®º**ï¼š
- âœ… æç®€é…ç½®ï¼Œæœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆ
- âœ… å…ƒå­¦ä¹ å™¨åªéœ€å­¦ä¹ å¦‚ä½•ç»„åˆåŸºç¡€æ¨¡å‹ï¼Œä¸éœ€è¦å¤æ‚æ¨¡å‹
- âœ… å¼ºæ­£åˆ™åŒ–ï¼ˆreg_alpha=0.3, reg_lambda=0.3ï¼‰
- âœ… ä½é‡‡æ ·ç‡ï¼ˆsubsample=0.7, colsample_bytree=0.7ï¼‰

#### 5.2 åŠ¨æ€HOLDæƒ©ç½š

**å½“å‰å®ç°**ï¼š
```python
hold_ratio = (meta_labels_val == 1).sum() / len(meta_labels_val)

if hold_ratio > 0.60:
    meta_hold_penalty_weight = 0.45  # é‡æƒ©ç½š
elif hold_ratio > 0.50:
    meta_hold_penalty_weight = 0.55  # ä¸­ç­‰
elif hold_ratio > 0.40:
    meta_hold_penalty_weight = 0.65  # è½»åº¦
else:
    meta_hold_penalty_weight = 0.75  # æ­£å¸¸
```

**è¯„ä¼°ç»“è®º**ï¼š
- âœ… æ ¹æ®HOLDå æ¯”åŠ¨æ€è°ƒæ•´æƒ©ç½šç³»æ•°
- âœ… HOLDå æ¯”è¶Šé«˜ï¼Œæƒ©ç½šè¶Šé‡
- âœ… æœ‰æ•ˆå¹³è¡¡ç±»åˆ«åˆ†å¸ƒ

## æ•°æ®æ¨¡å‹

### è¾“å…¥æ•°æ®æ ¼å¼

**å½“å‰æ ¼å¼**ï¼š
```python
X: (batch_size, n_features)  # å•æ—¶é—´ç‚¹ç‰¹å¾
y: (batch_size,)  # æ ‡ç­¾ï¼ˆ0=SHORT, 1=HOLD, 2=LONGï¼‰
```

**ç‰¹å¾æ•°é‡**ï¼š82ä¸ªé«˜çº§æŠ€æœ¯æŒ‡æ ‡

### æ¨¡å‹è¾“å‡ºæ ¼å¼

**Informer-2è¾“å‡º**ï¼š
```python
logits: (batch_size, 3)  # åŸå§‹åˆ†æ•°
probs: (batch_size, 3)   # softmaxæ¦‚ç‡
```

### å…ƒç‰¹å¾æ ¼å¼ï¼ˆStackingï¼‰

**ä¸å«Informer-2**ï¼š
```python
meta_features: (batch_size, 20)
# åŒ…å«ï¼š
# - lgb_proba (3) + xgb_proba (3) + cat_proba (3)
# - agreement (1) + max_prob (3) + entropy (3)
# - avg_proba (3) + prob_std_max (1)
```

**å«Informer-2**ï¼š
```python
meta_features: (batch_size, 23)
# é¢å¤–å¢åŠ ï¼š
# - inf_proba (3) + inf_max_prob (1) + inf_entropy (1)
```

## é”™è¯¯å¤„ç†

### 1. Informer-2è®­ç»ƒå¤±è´¥

**é”™è¯¯ç±»å‹**ï¼šç»´åº¦é”™è¯¯ã€GPUå†…å­˜ä¸è¶³

**å¤„ç†ç­–ç•¥**ï¼š
```python
try:
    inf_model = self._train_informer2(X_train, y_train, timeframe)
except Exception as e:
    logger.error(f"Informer-2è®­ç»ƒå¤±è´¥: {e}")
    inf_model = None  # é™çº§åˆ°ä¸‰æ¨¡å‹é›†æˆ
```

### 2. GPUä¸å¯ç”¨

**å¤„ç†ç­–ç•¥**ï¼š
```python
if not torch.cuda.is_available():
    logger.warning("GPUä¸å¯ç”¨ï¼Œé™çº§åˆ°CPUè®­ç»ƒ")
    device = torch.device('cpu')
```

### 3. Optunaä¼˜åŒ–è¶…æ—¶

**å¤„ç†ç­–ç•¥**ï¼š
```python
try:
    study.optimize(objective, n_trials=100, timeout=1800)
except KeyboardInterrupt:
    logger.warning("ä¼˜åŒ–è¢«ç”¨æˆ·ä¸­æ–­")
# ä½¿ç”¨å·²å®Œæˆçš„è¯•éªŒä¸­çš„æœ€ä½³å‚æ•°
best_params = study.best_params
```

## æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•

- æµ‹è¯•ç®€åŒ–Informer-2çš„forwardæ–¹æ³•
- æµ‹è¯•GMADLæŸå¤±å‡½æ•°çš„æ¢¯åº¦è®¡ç®—
- æµ‹è¯•GPUè®¾å¤‡åˆ†é…é€»è¾‘

### 2. é›†æˆæµ‹è¯•

- æµ‹è¯•å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ˆæ•°æ®åŠ è½½â†’è®­ç»ƒâ†’è¯„ä¼°ï¼‰
- æµ‹è¯•Stackingé›†æˆï¼ˆåŸºç¡€æ¨¡å‹â†’å…ƒç‰¹å¾â†’å…ƒå­¦ä¹ å™¨ï¼‰
- æµ‹è¯•Optunaä¼˜åŒ–æµç¨‹

### 3. æ€§èƒ½æµ‹è¯•

- å¯¹æ¯”ç®€åŒ–å‰åçš„è®­ç»ƒæ—¶é—´
- å¯¹æ¯”GPUåŠ é€Ÿå‰åçš„è®­ç»ƒæ—¶é—´
- å¯¹æ¯”ä¸åŒè¶…å‚æ•°é…ç½®çš„å‡†ç¡®ç‡

### 4. å›å½’æµ‹è¯•

- ç¡®ä¿ä¿®å¤åå‡†ç¡®ç‡ä¸ä½äºå½“å‰æ°´å¹³ï¼ˆ47%ï¼‰
- ç¡®ä¿ä¸‰æ¨¡å‹é›†æˆä»ç„¶æ­£å¸¸å·¥ä½œ
- ç¡®ä¿å…ƒå­¦ä¹ å™¨æ­£å¸¸ç»„åˆé¢„æµ‹

## æ€§èƒ½ä¼˜åŒ–

### 1. è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–

- âœ… ä½¿ç”¨GPUåŠ é€Ÿï¼ˆLightGBM/XGBoost/CatBoost/Informer-2ï¼‰
- âœ… ä½¿ç”¨æ‰¹å¤„ç†ï¼ˆbatch_size=256ï¼‰
- âœ… å‡å°‘Informer-2è®­ç»ƒè½®æ•°ï¼ˆ50è½®ï¼‰
- âœ… ä½¿ç”¨æ—©åœï¼ˆå¦‚æœéªŒè¯æŸå¤±ä¸å†ä¸‹é™ï¼‰

### 2. å†…å­˜ä¼˜åŒ–

- âœ… ä½¿ç”¨float32è€Œéfloat64
- âœ… åŠæ—¶é‡Šæ”¾ä¸­é—´å˜é‡
- âœ… ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆå¦‚æœGPUå†…å­˜ä¸è¶³ï¼‰

### 3. å‡†ç¡®ç‡ä¼˜åŒ–

- âœ… ä½¿ç”¨GMADLæŸå¤±å‡½æ•°ï¼ˆå…³æ³¨éš¾åˆ†æ ·æœ¬ï¼‰
- âœ… ä½¿ç”¨HOLDæƒ©ç½šï¼ˆå‡å°‘è¿‡åº¦é¢„æµ‹HOLDï¼‰
- âœ… ä½¿ç”¨Optunaè‡ªåŠ¨ä¼˜åŒ–è¶…å‚æ•°
- âœ… ä½¿ç”¨Stackingé›†æˆï¼ˆç»„åˆå¤šä¸ªæ¨¡å‹ï¼‰

## éƒ¨ç½²è€ƒè™‘

### 1. æ¨¡å‹ä¿å­˜

```python
# ä¿å­˜Informer-2æ¨¡å‹
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'hyperparameters': {...}
}, f'models/{symbol}_{timeframe}_informer2.pth')
```

### 2. æ¨¡å‹åŠ è½½

```python
# åŠ è½½Informer-2æ¨¡å‹
checkpoint = torch.load(model_path)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

### 3. æ¨ç†ä¼˜åŒ–

```python
# ä½¿ç”¨torch.no_grad()åŠ é€Ÿæ¨ç†
with torch.no_grad():
    logits = model(X_tensor)
    probs = F.softmax(logits, dim=-1)
```

## ç›‘æ§æŒ‡æ ‡

### 1. è®­ç»ƒæŒ‡æ ‡

- è®­ç»ƒæŸå¤±ï¼ˆGMADL Lossï¼‰
- éªŒè¯å‡†ç¡®ç‡
- å„ç±»åˆ«çš„ç²¾ç¡®ç‡/å¬å›ç‡/F1
- HOLDä¿¡å·å æ¯”

### 2. æ€§èƒ½æŒ‡æ ‡

- è®­ç»ƒæ—¶é—´ï¼ˆæ€»æ—¶é—´ã€æ¯è½®æ—¶é—´ï¼‰
- GPUåˆ©ç”¨ç‡
- å†…å­˜ä½¿ç”¨é‡

### 3. ä¸šåŠ¡æŒ‡æ ‡

- äº¤æ˜“ä¿¡å·å‡†ç¡®ç‡
- å¹´åŒ–æ”¶ç›Šç‡
- å¤æ™®æ¯”ç‡
- æœ€å¤§å›æ’¤
