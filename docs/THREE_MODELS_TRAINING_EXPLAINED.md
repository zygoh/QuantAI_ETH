# ğŸ“ ä¸‰ä¸ªæ¨¡å‹çš„è®­ç»ƒä½ç½®è¯¦è§£

**é¡¹ç›®**: QuantAI-ETH  
**æ¶æ„**: Stackingä¸‰æ¨¡å‹èåˆ  
**åˆ›å»ºæ—¶é—´**: 2025-10-17

---

## ğŸ¯ ä¸‰ä¸ªæ¨¡å‹çš„è®­ç»ƒä½ç½®

### æ¨¡å‹1: LightGBM

**è®­ç»ƒæ–¹æ³•ä½ç½®**: 
- **æ–‡ä»¶**: `backend/app/services/ml_service.py`
- **è¡Œæ•°**: ç¬¬806è¡Œ
- **æ–¹æ³•**: `_train_lightgbm()`

**è°ƒç”¨ä½ç½®**:
- **æ–‡ä»¶**: `backend/app/services/ensemble_ml_service.py`
- **è¡Œæ•°**: ç¬¬175è¡Œ
- **ä»£ç **: `lgb_model = self._train_lightgbm(X_train, y_train, timeframe)`

**è°ƒç”¨æ–¹å¼**: âœ… **ç»§æ‰¿è‡ªçˆ¶ç±»**ï¼ˆå¤ç”¨ä»£ç ï¼‰

---

### æ¨¡å‹2: XGBoost

**è®­ç»ƒæ–¹æ³•ä½ç½®**:
- **æ–‡ä»¶**: `backend/app/services/ensemble_ml_service.py`
- **è¡Œæ•°**: ç¬¬284è¡Œ
- **æ–¹æ³•**: `_train_xgboost()`

**è°ƒç”¨ä½ç½®**:
- **æ–‡ä»¶**: `backend/app/services/ensemble_ml_service.py`
- **è¡Œæ•°**: ç¬¬179è¡Œ
- **ä»£ç **: `xgb_model = self._train_xgboost(X_train, y_train, timeframe)`

**è°ƒç”¨æ–¹å¼**: âœ… **åœ¨å­ç±»ä¸­æ–°å¢**

---

### æ¨¡å‹3: CatBoost

**è®­ç»ƒæ–¹æ³•ä½ç½®**:
- **æ–‡ä»¶**: `backend/app/services/ensemble_ml_service.py`
- **è¡Œæ•°**: ç¬¬319è¡Œ
- **æ–¹æ³•**: `_train_catboost()`

**è°ƒç”¨ä½ç½®**:
- **æ–‡ä»¶**: `backend/app/services/ensemble_ml_service.py`
- **è¡Œæ•°**: ç¬¬183è¡Œ
- **ä»£ç **: `cat_model = self._train_catboost(X_train, y_train, timeframe)`

**è°ƒç”¨æ–¹å¼**: âœ… **åœ¨å­ç±»ä¸­æ–°å¢**

---

## ğŸ—ï¸ å®Œæ•´è°ƒç”¨é“¾

### Stackingè®­ç»ƒæµç¨‹

```
main.py
    â†“
scheduler.py (å®šæ—¶ä»»åŠ¡)
    â†“
ensemble_ml_service.train_model()
    â†“
ensemble_ml_service.train_all_timeframes()
    â†“
ensemble_ml_service._train_ensemble_single_timeframe(timeframe)
    â†“
ã€å‡†å¤‡æ•°æ®ã€‘ï¼ˆå¤ç”¨çˆ¶ç±»æ–¹æ³•ï¼‰
    self._prepare_training_data_for_timeframe()  â† ml_service.py
    self.feature_engineer.create_features()      â† ml_service.py
    self._create_labels()                        â† ml_service.py
    self._prepare_features_labels()              â† ml_service.py
    self._scale_features()                       â† ml_service.py
    â†“
ã€è®­ç»ƒä¸‰ä¸ªåŸºç¡€æ¨¡å‹ã€‘
    â”œâ”€ self._train_lightgbm()   â† ml_service.py:806 âœ…
    â”œâ”€ self._train_xgboost()    â† ensemble_ml_service.py:284 âœ…
    â””â”€ self._train_catboost()   â† ensemble_ml_service.py:319 âœ…
    â†“
ã€ç”Ÿæˆå…ƒç‰¹å¾ã€‘
    lgb_proba = lgb_model.predict_proba(X_train)
    xgb_proba = xgb_model.predict_proba(X_train)
    cat_proba = cat_model.predict_proba(X_train)
    meta_features = np.hstack([lgb_proba, xgb_proba, cat_proba])
    â†“
ã€è®­ç»ƒå…ƒå­¦ä¹ å™¨ã€‘
    meta_learner = LogisticRegression()
    meta_learner.fit(meta_features, y_train)
    â†“
ã€ä¿å­˜4ä¸ªæ¨¡å‹ã€‘
    self._save_ensemble_models()
```

---

## ğŸ“Š ä»£ç åˆ†å¸ƒ

### ml_service.pyï¼ˆåŸºç±»ï¼‰

**æä¾›çš„è®­ç»ƒç›¸å…³æ–¹æ³•**ï¼š

| æ–¹æ³• | è¡Œæ•° | ä½œç”¨ |
|------|------|------|
| `_prepare_training_data_for_timeframe()` | ~400è¡Œ | æ•°æ®å‡†å¤‡ |
| `_create_labels()` | ~80è¡Œ | æ ‡ç­¾åˆ›å»º |
| `_prepare_features_labels()` | ~100è¡Œ | ç‰¹å¾å‡†å¤‡ |
| `_select_features_intelligent()` | ~200è¡Œ | æ™ºèƒ½ç‰¹å¾é€‰æ‹© |
| `_scale_features()` | ~50è¡Œ | ç‰¹å¾ç¼©æ”¾ |
| **`_train_lightgbm()`** | **~150è¡Œ** | **LightGBMè®­ç»ƒ** â­ |
| `_evaluate_model()` | ~50è¡Œ | æ¨¡å‹è¯„ä¼° |
| `_save_model()` | ~30è¡Œ | æ¨¡å‹ä¿å­˜ |

**æ€»è®¡**: çº¦1060è¡Œï¼ˆæä¾›æ‰€æœ‰åŸºç¡€åŠŸèƒ½ï¼‰

---

### ensemble_ml_service.pyï¼ˆå­ç±»ï¼‰

**æ–°å¢çš„è®­ç»ƒæ–¹æ³•**ï¼š

| æ–¹æ³• | è¡Œæ•° | ä½œç”¨ |
|------|------|------|
| `train_all_timeframes()` | ~70è¡Œ | åè°ƒæ‰€æœ‰æ—¶é—´æ¡†æ¶è®­ç»ƒ |
| `_train_ensemble_single_timeframe()` | ~60è¡Œ | å•æ—¶é—´æ¡†æ¶Stacking |
| `_train_stacking_ensemble()` | ~150è¡Œ | Stackingæ ¸å¿ƒé€»è¾‘ |
| **`_train_xgboost()`** | **~70è¡Œ** | **XGBoostè®­ç»ƒ** â­ |
| **`_train_catboost()`** | **~70è¡Œ** | **CatBoostè®­ç»ƒ** â­ |
| `_save_ensemble_models()` | ~30è¡Œ | ä¿å­˜4ä¸ªæ¨¡å‹ |
| `_load_ensemble_models()` | ~30è¡Œ | åŠ è½½4ä¸ªæ¨¡å‹ |
| `predict()` | ~80è¡Œ | Stackingé¢„æµ‹ |
| `train_model()` | ~30è¡Œ | è¦†ç›–çˆ¶ç±»æ–¹æ³• |

**æ€»è®¡**: çº¦545è¡Œï¼ˆæ–°å¢é›†æˆé€»è¾‘ï¼‰

---

## ğŸ“‹ è®­ç»ƒæ–¹æ³•å¯¹æ¯”

### 1. _train_lightgbm()

**ä½ç½®**: `ml_service.py:806-955`

**å‚æ•°**:
```python
def _train_lightgbm(
    self, 
    X_train: pd.DataFrame, 
    y_train: pd.Series, 
    timeframe: str
) -> lgb.LGBMClassifier:
```

**æ ¸å¿ƒä»£ç **:
```python
# æ ·æœ¬åŠ æƒ
class_weights = compute_sample_weight('balanced', y_train)
time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
sample_weights = class_weights * time_decay

# å·®å¼‚åŒ–å‚æ•°
params = self.lgb_params_by_timeframe[timeframe]

# è®­ç»ƒ
model = lgb.LGBMClassifier(**params)
model.fit(X_train, y_train, sample_weight=sample_weights)

return model
```

**ç‰¹ç‚¹**:
- ä½¿ç”¨å·®å¼‚åŒ–å‚æ•°ï¼ˆ15m: 95å¶å­ï¼Œ2h: 63ï¼Œ4h: 47ï¼‰
- æ ·æœ¬åŠ æƒï¼ˆç±»åˆ«å¹³è¡¡ Ã— æ—¶é—´è¡°å‡ï¼‰
- GPUæ”¯æŒ

---

### 2. _train_xgboost()

**ä½ç½®**: `ensemble_ml_service.py:284-316`

**å‚æ•°**:
```python
def _train_xgboost(
    self, 
    X_train: pd.DataFrame, 
    y_train: pd.Series, 
    timeframe: str
):
```

**æ ¸å¿ƒä»£ç **:
```python
import xgboost as xgb

# æ ·æœ¬åŠ æƒï¼ˆä¸LightGBMä¸€è‡´ï¼‰
class_weights = compute_sample_weight('balanced', y_train)
time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
sample_weights = class_weights * time_decay

# å‚æ•°é…ç½®
params = {
    'max_depth': 6,
    'learning_rate': 0.05,
    'n_estimators': 300,
    'objective': 'multi:softprob',
    'num_class': 3,
    'eval_metric': 'mlogloss',
    'random_state': 42,
    'tree_method': 'hist',
    'reg_alpha': 0.3,
    'reg_lambda': 0.3,
    'subsample': 0.8,
    'colsample_bytree': 0.8
}

# è®­ç»ƒ
model = xgb.XGBClassifier(**params)
model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)

return model
```

**ç‰¹ç‚¹**:
- ç»Ÿä¸€å‚æ•°ï¼ˆæ‰€æœ‰æ—¶é—´æ¡†æ¶ç›¸åŒï¼‰
- æ ·æœ¬åŠ æƒï¼ˆä¸LightGBMä¸€è‡´ï¼‰
- å¼ºæ­£åˆ™åŒ–

---

### 3. _train_catboost()

**ä½ç½®**: `ensemble_ml_service.py:319-354`

**å‚æ•°**:
```python
def _train_catboost(
    self, 
    X_train: pd.DataFrame, 
    y_train: pd.Series, 
    timeframe: str
):
```

**æ ¸å¿ƒä»£ç **:
```python
from catboost import CatBoostClassifier

# æ ·æœ¬åŠ æƒï¼ˆä¸LightGBMä¸€è‡´ï¼‰
class_weights = compute_sample_weight('balanced', y_train)
time_decay = np.exp(-np.arange(len(X_train)) / (len(X_train) * 0.1))[::-1]
sample_weights = class_weights * time_decay

# å‚æ•°é…ç½®
params = {
    'iterations': 300,
    'learning_rate': 0.05,
    'depth': 6,
    'loss_function': 'MultiClass',
    'random_seed': 42,
    'verbose': False,
    'l2_leaf_reg': 3.0,
    'bootstrap_type': 'Bayesian',
    'bagging_temperature': 1.0,
    'subsample': 0.8
}

# è®­ç»ƒ
model = CatBoostClassifier(**params)
model.fit(X_train, y_train, sample_weight=sample_weights, verbose=False)

return model
```

**ç‰¹ç‚¹**:
- ç»Ÿä¸€å‚æ•°ï¼ˆæ‰€æœ‰æ—¶é—´æ¡†æ¶ç›¸åŒï¼‰
- æ ·æœ¬åŠ æƒï¼ˆä¸LightGBMä¸€è‡´ï¼‰
- è´å¶æ–¯è‡ªåŠ©æ³•

---

## ğŸ”„ è®­ç»ƒè°ƒç”¨æµç¨‹

### åœ¨_train_stacking_ensemble()ä¸­è°ƒç”¨

**ä½ç½®**: `ensemble_ml_service.py:136-228`

```python
def _train_stacking_ensemble(
    self, 
    X_train, y_train, X_val, y_val, 
    timeframe
):
    """è®­ç»ƒStackingé›†æˆ"""
    
    logger.info(f"ğŸ¯ Stage 1: è®­ç»ƒ3ä¸ªåŸºç¡€æ¨¡å‹...")
    
    # 1ï¸âƒ£ è®­ç»ƒLightGBM
    logger.info(f"  ğŸ“Š è®­ç»ƒLightGBM...")
    lgb_model = self._train_lightgbm(X_train, y_train, timeframe)
    # â†‘ è°ƒç”¨çˆ¶ç±»æ–¹æ³•ï¼ˆml_service.py:806ï¼‰
    
    # 2ï¸âƒ£ è®­ç»ƒXGBoost
    logger.info(f"  ğŸ“Š è®­ç»ƒXGBoost...")
    xgb_model = self._train_xgboost(X_train, y_train, timeframe)
    # â†‘ è°ƒç”¨æœ¬ç±»æ–¹æ³•ï¼ˆensemble_ml_service.py:284ï¼‰
    
    # 3ï¸âƒ£ è®­ç»ƒCatBoost
    logger.info(f"  ğŸ“Š è®­ç»ƒCatBoost...")
    cat_model = self._train_catboost(X_train, y_train, timeframe)
    # â†‘ è°ƒç”¨æœ¬ç±»æ–¹æ³•ï¼ˆensemble_ml_service.py:319ï¼‰
    
    logger.info(f"âœ… 3ä¸ªåŸºç¡€æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    # 4ï¸âƒ£ ç”Ÿæˆå…ƒç‰¹å¾
    logger.info(f"ğŸ¯ Stage 2: ç”Ÿæˆå…ƒç‰¹å¾...")
    lgb_pred_train = lgb_model.predict_proba(X_train)
    xgb_pred_train = xgb_model.predict_proba(X_train)
    cat_pred_train = cat_model.predict_proba(X_train)
    
    meta_features_train = np.hstack([
        lgb_pred_train,
        xgb_pred_train,
        cat_pred_train
    ])
    
    # 5ï¸âƒ£ è®­ç»ƒå…ƒå­¦ä¹ å™¨
    logger.info(f"ğŸ¯ Stage 3: è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆStackingï¼‰...")
    from sklearn.linear_model import LogisticRegression
    
    meta_learner = LogisticRegression(
        multi_class='multinomial',
        solver='lbfgs',
        max_iter=1000,
        random_state=42
    )
    meta_learner.fit(meta_features_train, y_train)
    
    # 6ï¸âƒ£ ä¿å­˜åˆ°å­—å…¸
    self.ensemble_models[timeframe] = {
        'lgb': lgb_model,
        'xgb': xgb_model,
        'cat': cat_model,
        'meta': meta_learner
    }
```

---

## ğŸ“Š æ–‡ä»¶ä¾èµ–å…³ç³»

### æ–‡ä»¶ç»“æ„

```
ml_service.pyï¼ˆåŸºç±»ï¼Œ1063è¡Œï¼‰
    â”œâ”€â”€ æä¾›é€šç”¨åŠŸèƒ½ï¼ˆ95%ï¼‰
    â”œâ”€â”€ _prepare_training_data()
    â”œâ”€â”€ _create_labels()
    â”œâ”€â”€ _prepare_features_labels()
    â”œâ”€â”€ _scale_features()
    â”œâ”€â”€ _train_lightgbm() â­
    â””â”€â”€ ... 30+ æ–¹æ³•
        â†‘ ç»§æ‰¿
ensemble_ml_service.pyï¼ˆå­ç±»ï¼Œ545è¡Œï¼‰
    â”œâ”€â”€ ç»§æ‰¿æ‰€æœ‰çˆ¶ç±»æ–¹æ³• âœ…
    â”œâ”€â”€ æ–°å¢ _train_xgboost() â­
    â”œâ”€â”€ æ–°å¢ _train_catboost() â­
    â”œâ”€â”€ æ–°å¢ _train_stacking_ensemble()
    â”‚   â”œâ”€ è°ƒç”¨ self._train_lightgbm() â† çˆ¶ç±»æ–¹æ³•
    â”‚   â”œâ”€ è°ƒç”¨ self._train_xgboost() â† æœ¬ç±»æ–¹æ³•
    â”‚   â””â”€ è°ƒç”¨ self._train_catboost() â† æœ¬ç±»æ–¹æ³•
    â””â”€â”€ è¦†ç›– train_model(), predict()
```

---

## ğŸ” ä»£ç ä½ç½®é€ŸæŸ¥è¡¨

| æ¨¡å‹ | è®­ç»ƒæ–¹æ³•å®šä¹‰ | è°ƒç”¨ä½ç½® | æ¥æº |
|------|------------|---------|------|
| **LightGBM** | ml_service.py:806 | ensemble_ml_service.py:175 | çˆ¶ç±»ç»§æ‰¿ âœ… |
| **XGBoost** | ensemble_ml_service.py:284 | ensemble_ml_service.py:179 | å­ç±»æ–°å¢ âœ… |
| **CatBoost** | ensemble_ml_service.py:319 | ensemble_ml_service.py:183 | å­ç±»æ–°å¢ âœ… |
| **å…ƒå­¦ä¹ å™¨** | ensemble_ml_service.py:195 | ensemble_ml_service.py:195 | å­ç±»æ–°å¢ âœ… |

---

## ğŸ’» è®­ç»ƒå‚æ•°å¯¹æ¯”

### LightGBMå‚æ•°

**æ–‡ä»¶**: `ml_service.py`ï¼ˆé€šè¿‡self.lgb_params_by_timeframeï¼‰

```python
# 15mæ—¶é—´æ¡†æ¶
{
    'num_leaves': 95,
    'learning_rate': 0.03,
    'n_estimators': 300,
    'max_depth': 7,
    'min_child_samples': 50,
    'reg_alpha': 0.5,
    'reg_lambda': 0.5,
    'subsample': 0.8,
    'colsample_bytree': 0.8
}
```

**ç‰¹ç‚¹**: å·®å¼‚åŒ–é…ç½®ï¼ˆæ¯ä¸ªtimeframeä¸åŒï¼‰

---

### XGBoostå‚æ•°

**æ–‡ä»¶**: `ensemble_ml_service.py:291-303`

```python
{
    'max_depth': 6,
    'learning_rate': 0.05,
    'n_estimators': 300,
    'objective': 'multi:softprob',
    'num_class': 3,
    'eval_metric': 'mlogloss',
    'random_state': 42,
    'tree_method': 'hist',
    'reg_alpha': 0.3,
    'reg_lambda': 0.3,
    'subsample': 0.8,
    'colsample_bytree': 0.8
}
```

**ç‰¹ç‚¹**: ç»Ÿä¸€é…ç½®ï¼ˆæ‰€æœ‰timeframeç›¸åŒï¼‰

---

### CatBoostå‚æ•°

**æ–‡ä»¶**: `ensemble_ml_service.py:329-340`

```python
{
    'iterations': 300,
    'learning_rate': 0.05,
    'depth': 6,
    'loss_function': 'MultiClass',
    'random_seed': 42,
    'verbose': False,
    'l2_leaf_reg': 3.0,
    'bootstrap_type': 'Bayesian',
    'bagging_temperature': 1.0,
    'subsample': 0.8
}
```

**ç‰¹ç‚¹**: ç»Ÿä¸€é…ç½®ï¼ˆæ‰€æœ‰timeframeç›¸åŒï¼‰

---

## ğŸ¯ ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

### è®¾è®¡åŸåˆ™

1. **ç»§æ‰¿å¤ç”¨**ï¼š
   - LightGBMè®­ç»ƒé€»è¾‘å·²ç»å¾ˆæˆç†Ÿï¼ˆml_service.pyï¼‰
   - ç›´æ¥ç»§æ‰¿ä½¿ç”¨ï¼Œé¿å…é‡å¤ä»£ç 
   - ä¿æŒä¸€è‡´æ€§

2. **æ–°å¢æ‰©å±•**ï¼š
   - XGBoostå’ŒCatBoostæ˜¯æ–°å¢åŠŸèƒ½
   - åœ¨å­ç±»ä¸­å®ç°ï¼Œä¸å½±å“åŸºç±»
   - ä¾¿äºç»´æŠ¤å’Œæµ‹è¯•

3. **æ¨¡å—åˆ†ç¦»**ï¼š
   - åŸºç¡€åŠŸèƒ½åœ¨ml_service.py
   - é›†æˆåŠŸèƒ½åœ¨ensemble_ml_service.py
   - èŒè´£æ¸…æ™°

---

## ğŸš€ è®­ç»ƒæ‰§è¡Œé¡ºåº

### å•ä¸ªæ—¶é—´æ¡†æ¶ï¼ˆä¾‹å¦‚15mï¼‰

```
Step 1: å‡†å¤‡æ•°æ®
    â†“ self._prepare_training_data_for_timeframe('15m')
    34560æ¡Kçº¿

Step 2: ç‰¹å¾å·¥ç¨‹
    â†“ self.feature_engineer.create_features(data)
    186ä¸ªç‰¹å¾

Step 3: åˆ›å»ºæ ‡ç­¾
    â†“ self._create_labels(data, '15m')
    é˜ˆå€¼Â±0.15%

Step 4: ç‰¹å¾é€‰æ‹©
    â†“ self._prepare_features_labels(data, '15m')
    æ™ºèƒ½é€‰æ‹©141ä¸ªç‰¹å¾

Step 5: ç‰¹å¾ç¼©æ”¾
    â†“ self._scale_features(X, '15m', fit=True)
    StandardScaler

Step 6: æ•°æ®åˆ†å‰²
    â†“ split_idx = int(len(X) * 0.8)
    è®­ç»ƒ27488ï¼ŒéªŒè¯6872

Step 7: è®­ç»ƒåŸºç¡€æ¨¡å‹1
    â†“ self._train_lightgbm(X_train, y_train, '15m')
    ã€ml_service.py:806ã€‘
    LightGBMæ¨¡å‹

Step 8: è®­ç»ƒåŸºç¡€æ¨¡å‹2
    â†“ self._train_xgboost(X_train, y_train, '15m')
    ã€ensemble_ml_service.py:284ã€‘
    XGBoostæ¨¡å‹

Step 9: è®­ç»ƒåŸºç¡€æ¨¡å‹3
    â†“ self._train_catboost(X_train, y_train, '15m')
    ã€ensemble_ml_service.py:319ã€‘
    CatBoostæ¨¡å‹

Step 10: ç”Ÿæˆå…ƒç‰¹å¾
    â†“ meta_features = [lgbæ¦‚ç‡, xgbæ¦‚ç‡, catæ¦‚ç‡]
    9ç»´å…ƒç‰¹å¾

Step 11: è®­ç»ƒå…ƒå­¦ä¹ å™¨
    â†“ meta_learner = LogisticRegression()
    â†“ meta_learner.fit(meta_features, y_train)
    å…ƒå­¦ä¹ å™¨ï¼ˆStackingæ ¸å¿ƒï¼‰

Step 12: ä¿å­˜4ä¸ªæ¨¡å‹
    â†“ ETHUSDT_15m_lgb_model.pkl
    â†“ ETHUSDT_15m_xgb_model.pkl
    â†“ ETHUSDT_15m_cat_model.pkl
    â†“ ETHUSDT_15m_meta_model.pkl
```

---

## âœ… æ€»ç»“

### ä¸‰ä¸ªæ¨¡å‹è®­ç»ƒä½ç½®

| æ¨¡å‹ | è®­ç»ƒæ–¹æ³•ä½ç½® | è°ƒç”¨ä½ç½® |
|------|------------|---------|
| **LightGBM** | `ml_service.py:806` | `ensemble_ml_service.py:175` |
| **XGBoost** | `ensemble_ml_service.py:284` | `ensemble_ml_service.py:179` |
| **CatBoost** | `ensemble_ml_service.py:319` | `ensemble_ml_service.py:183` |

### ä¸ºä»€ä¹ˆml_service.pyä¸èƒ½åˆ é™¤ï¼Ÿ

1. âœ… æä¾›LightGBMè®­ç»ƒæ–¹æ³•
2. âœ… æä¾›æ‰€æœ‰æ•°æ®å‡†å¤‡æ–¹æ³•
3. âœ… æ˜¯ensemble_ml_serviceçš„çˆ¶ç±»
4. âœ… å¤ç”¨ç‡68%
5. âœ… **åˆ é™¤ä¼šå¯¼è‡´ç³»ç»Ÿå´©æºƒ**

### æ–‡ä»¶èŒè´£

- **ml_service.py**: åŸºç±»ï¼Œæä¾›é€šç”¨MLåŠŸèƒ½
- **ensemble_ml_service.py**: å­ç±»ï¼Œå®ç°Stackingé›†æˆ

---

**LightGBMè®­ç»ƒ**: ml_service.py:806 âœ…  
**XGBoostè®­ç»ƒ**: ensemble_ml_service.py:284 âœ…  
**CatBoostè®­ç»ƒ**: ensemble_ml_service.py:319 âœ…  
**å…ƒå­¦ä¹ å™¨**: ensemble_ml_service.py:195 âœ…

